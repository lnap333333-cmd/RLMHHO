import pandas as pd
import numpy as np
import json
import os
from pathlib import Path
import glob

def load_experiment_data(result_dir):
    """ä»JSONæ–‡ä»¶åŠ è½½å®Œæ•´çš„å®éªŒæ•°æ®"""
    
    print("ğŸ“‚ æ­£åœ¨åŠ è½½å®éªŒæ•°æ®...")
    
    # åŠ è½½ç”°å£åˆ†æç»“æœ
    analysis_file = result_dir / "taguchi_analysis.json"
    with open(analysis_file, 'r', encoding='utf-8') as f:
        analysis_data = json.load(f)
    
    # åŠ è½½final_results.json
    final_file = result_dir / "final_results.json"
    with open(final_file, 'r', encoding='utf-8') as f:
        final_data = json.load(f)
    
    print(f"âœ… ç”°å£åˆ†ææ•°æ®åŠ è½½å®Œæˆ")
    print(f"âœ… æœ€ç»ˆç»“æœæ•°æ®åŠ è½½å®Œæˆ")
    
    # ä»final_resultsæ„å»ºDataFrame
    experiments = []
    
    for exp_id, exp_data in final_data.items():
        if exp_id.startswith('exp_') and exp_data is not None:
            exp_num = int(exp_id.split('_')[1])
            
            # æå–å‚æ•°é…ç½®
            if 'config' in exp_data:
                config = exp_data['config']
                learning_rate = config.get('learning_rate', 0)
                epsilon_decay = config.get('epsilon_decay', 0)
                gamma = config.get('gamma', 0)
                group_ratios = config.get('group_ratios', [])
            else:
                # å¦‚æœconfigä¸å­˜åœ¨ï¼Œä»ç¬¬ä¸€ä¸ªè¿è¡Œçš„configè·å–
                for run_key in exp_data.keys():
                    if run_key.startswith('run_') and exp_data[run_key] is not None:
                        if 'config' in exp_data[run_key]:
                            config = exp_data[run_key]['config']
                            learning_rate = config.get('learning_rate', 0)
                            epsilon_decay = config.get('epsilon_decay', 0) 
                            gamma = config.get('gamma', 0)
                            group_ratios = config.get('group_ratios', [])
                            break
                else:
                    learning_rate = epsilon_decay = gamma = 0
                    group_ratios = []
            
            # æ”¶é›†æ‰€æœ‰è¿è¡Œçš„æ€§èƒ½æ•°æ®
            hv_values = []
            igd_values = []
            gd_values = []
            comp_scores = []
            successful_runs = 0
            
            for run_key in exp_data.keys():
                if run_key.startswith('run_') and exp_data[run_key] is not None:
                    run_data = exp_data[run_key]
                    if 'performance' in run_data:
                        perf = run_data['performance']
                        if all(key in perf for key in ['hypervolume', 'igd', 'gd']):
                            hv_values.append(perf['hypervolume'])
                            igd_values.append(perf['igd'])
                            gd_values.append(perf['gd'])
                            
                            # è®¡ç®—5:3:2åŠ æƒç»¼åˆå¾—åˆ†
                            # å½’ä¸€åŒ–å¤„ç†ï¼ˆç®€å•çš„min-maxå½’ä¸€åŒ–ï¼Œå®é™…åº”è¯¥ç”¨å…¨å±€èŒƒå›´ï¼‰
                            hv_norm = perf['hypervolume']  # HVè¶Šå¤§è¶Šå¥½
                            igd_norm = 1.0 / (1.0 + perf['igd'])  # IGDè¶Šå°è¶Šå¥½
                            gd_norm = 1.0 / (1.0 + perf['gd'])    # GDè¶Šå°è¶Šå¥½
                            
                            comp_score = 0.5 * hv_norm + 0.3 * igd_norm + 0.2 * gd_norm
                            comp_scores.append(comp_score)
                            successful_runs += 1
            
            if comp_scores:  # ç¡®ä¿æœ‰æœ‰æ•ˆæ•°æ®
                # è®¡ç®—ç»Ÿè®¡é‡
                hv_mean = np.mean(hv_values)
                hv_std = np.std(hv_values) if len(hv_values) > 1 else 0
                igd_mean = np.mean(igd_values)
                igd_std = np.std(igd_values) if len(igd_values) > 1 else 0
                gd_mean = np.mean(gd_values)
                gd_std = np.std(gd_values) if len(gd_values) > 1 else 0
                comp_mean = np.mean(comp_scores)
                comp_std = np.std(comp_scores) if len(comp_scores) > 1 else 0
                
                # è®¡ç®—SNR (ä¿¡å™ªæ¯”ï¼Œæœ›å¤§ç‰¹æ€§)
                if comp_std > 0:
                    snr = -10 * np.log10(np.mean([1/(score**2) for score in comp_scores if score > 0]))
                else:
                    snr = -10 * np.log10(1/(comp_mean**2)) if comp_mean > 0 else -100
                
                experiments.append({
                    'Exp_ID': exp_num,
                    'A_LearningRate': learning_rate,
                    'B_EpsilonDecay': epsilon_decay,
                    'D_Gamma': gamma,
                    'C_GroupRatios': str(group_ratios),
                    'HV_Mean': hv_mean,
                    'HV_Std': hv_std,
                    'IGD_Mean': igd_mean,
                    'IGD_Std': igd_std,
                    'GD_Mean': gd_mean,
                    'GD_Std': gd_std,
                    'Comprehensive_Mean': comp_mean,
                    'Comprehensive_Std': comp_std,
                    'SNR_Value': snr,
                    'Successful_Runs': successful_runs
                })
    
    df = pd.DataFrame(experiments)
    df = df.sort_values('Exp_ID').reset_index(drop=True)
    
    print(f"âœ… æ•°æ®æ„å»ºå®Œæˆ: {len(df)}ä¸ªå®éªŒç»„")
    
    return df, analysis_data

def analyze_taguchi_results():
    """å®Œæ•´åˆ†æç”°å£å®éªŒç»“æœ"""
    
    print("=" * 80)
    print("ğŸ” ç”°å£L49å®éªŒæ·±åº¦æ•°æ®åˆ†æ - 2025-06-24")
    print("=" * 80)
    
    # 1. æ•°æ®æ–‡ä»¶æ£€æŸ¥
    print("\nğŸ“‚ 1. æ•°æ®æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥:")
    result_dir = Path("taguchi_results_20250624_172744")
    
    if not result_dir.exists():
        print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {result_dir}")
        return
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    analysis_file = result_dir / "taguchi_analysis.json"
    final_file = result_dir / "final_results.json"
    
    print(f"ğŸ“ˆ ç”°å£åˆ†ææ–‡ä»¶: {'âœ…' if analysis_file.exists() else 'âŒ'} {analysis_file}")
    print(f"ğŸ“Š æœ€ç»ˆç»“æœæ–‡ä»¶: {'âœ…' if final_file.exists() else 'âŒ'} {final_file}")
    
    if not analysis_file.exists() or not final_file.exists():
        print("âŒ å…³é”®æ•°æ®æ–‡ä»¶ç¼ºå¤±ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
        return
    
    # æ£€æŸ¥å®éªŒæ–‡ä»¶æ•°é‡
    exp_files = list(result_dir.glob("exp_*_summary.json"))
    run_files = list(result_dir.glob("exp_*_run_*.json"))
    print(f"ğŸ“„ å®éªŒæ±‡æ€»æ–‡ä»¶: {len(exp_files)}ä¸ª")
    print(f"ğŸ“„ è¿è¡Œè¯¦ç»†æ–‡ä»¶: {len(run_files)}ä¸ª")
    
    # 2. åŠ è½½æ•°æ®
    print("\nğŸ“‹ 2. æ•°æ®åŠ è½½å’Œå¤„ç†:")
    try:
        df, analysis_data = load_experiment_data(result_dir)
        print(f"âœ… æ•°æ®å¤„ç†å®Œæˆ: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 3. åŸºç¡€ç»Ÿè®¡åˆ†æ
    print("\nğŸ“ˆ 3. åŸºç¡€ç»Ÿè®¡åˆ†æ:")
    print(f"å®éªŒç»„æ€»æ•°: {len(df)}")
    print(f"æ¯ç»„é‡å¤æ¬¡æ•°: 10æ¬¡")
    print(f"æ€»å®éªŒæ¬¡æ•°: {len(df) * 10}")
    print(f"å®éªŒæˆåŠŸç‡: {(df['Successful_Runs'].sum() / (len(df) * 10) * 100):.1f}%")
    
    # SNRç»Ÿè®¡
    snr_stats = df['SNR_Value'].describe()
    print(f"\nSNRç»Ÿè®¡åˆ†æ:")
    print(f"  æœ€å¤§å€¼: {snr_stats['max']:.3f} dB")
    print(f"  æœ€å°å€¼: {snr_stats['min']:.3f} dB")
    print(f"  å¹³å‡å€¼: {snr_stats['mean']:.3f} dB")
    print(f"  æ ‡å‡†å·®: {snr_stats['std']:.3f} dB")
    print(f"  ä¸­ä½æ•°: {snr_stats['50%']:.3f} dB")
    print(f"  æ€§èƒ½è·¨åº¦: {snr_stats['max'] - snr_stats['min']:.3f} dB")
    
    # 4. å¯»æ‰¾æœ€ä¼˜ç»“æœ
    print("\nğŸ¯ 4. æœ€ä¼˜ç»“æœè¯†åˆ«:")
    
    # æŒ‰SNRæ’åº
    best_snr_idx = df['SNR_Value'].idxmax()
    best_snr_row = df.loc[best_snr_idx]
    print(f"\næŒ‰SNRæœ€å¤§å€¼æ‰¾åˆ°çš„æœ€ä¼˜è§£:")
    print(f"  å®éªŒç»„: {best_snr_row['Exp_ID']}")
    print(f"  SNRå€¼: {best_snr_row['SNR_Value']:.3f} dB")
    print(f"  å­¦ä¹ ç‡: {best_snr_row['A_LearningRate']}")
    print(f"  æ¢ç´¢ç‡è¡°å‡: {best_snr_row['B_EpsilonDecay']}")
    print(f"  æŠ˜æ‰£å› å­: {best_snr_row['D_Gamma']}")
    print(f"  é¹°ç¾¤åˆ†ç»„: {best_snr_row['C_GroupRatios']}")
    print(f"  ç»¼åˆå¾—åˆ†: {best_snr_row['Comprehensive_Mean']:.4f} Â± {best_snr_row['Comprehensive_Std']:.4f}")
    print(f"  æˆåŠŸè¿è¡Œ: {best_snr_row['Successful_Runs']}/10æ¬¡")
    
    # ç”°å£é¢„æµ‹æœ€ä¼˜
    optimal_combo = analysis_data['optimal_combination']
    predicted_snr = analysis_data['predicted_snr']
    print(f"\nç”°å£æ–¹æ³•é¢„æµ‹æœ€ä¼˜:")
    print(f"  é¢„æµ‹ç»„åˆ: A={optimal_combo['A']}, B={optimal_combo['B']}, C={optimal_combo['C']}, D={optimal_combo['D']}")
    print(f"  é¢„æµ‹SNR: {predicted_snr:.3f} dB")
    
    # 5. ç”°å£æ–¹æ³•å› å­æ•ˆåº”åˆ†æ
    print("\nğŸ”¬ 5. ç”°å£æ–¹æ³•å› å­æ•ˆåº”åˆ†æ:")
    factors = ['A', 'B', 'C', 'D']
    factor_names = ['å­¦ä¹ ç‡', 'æ¢ç´¢ç‡è¡°å‡', 'é¹°ç¾¤åˆ†ç»„æ¯”ä¾‹', 'æŠ˜æ‰£å› å­']
    
    factor_effects = analysis_data['factor_effects']
    factor_importance = []
    
    for factor, name in zip(factors, factor_names):
        effects = factor_effects[factor]
        rank = effects['rank']
        range_val = effects['range']
        # è·å–ANOVAç»“æœ
        anova = analysis_data['anova_results'][factor]
        contribution = anova['contribution']
        f_value = anova['f_value']
        
        factor_importance.append((rank, name, factor, range_val, contribution, f_value))
    
    # æŒ‰é‡è¦æ€§æ’åº
    factor_importance.sort(key=lambda x: x[0])
    
    print("å› å­é‡è¦æ€§æ’åº:")
    for rank, name, factor, range_val, contribution, f_value in factor_importance:
        print(f"{rank}. {name}({factor}): æå·®={range_val:.3f}, è´¡çŒ®åº¦={contribution:.2f}%, Få€¼={f_value:.3f}")
    
    # 6. æ’åºåˆ†æ
    print("\nğŸ“Š 6. å®éªŒç»„æ€§èƒ½æ’åºåˆ†æ:")
    df_sorted = df.sort_values('SNR_Value', ascending=False).reset_index(drop=True)
    
    print("\nå‰10åå®éªŒç»„:")
    print("æ’å  å®éªŒç»„  å­¦ä¹ ç‡      è¡°å‡ç‡    æŠ˜æ‰£å› å­  ç»¼åˆå¾—åˆ†    SNR(dB)   æˆåŠŸç‡")
    print("-" * 85)
    for i in range(min(10, len(df_sorted))):
        row = df_sorted.iloc[i]
        success_rate = row['Successful_Runs'] / 10 * 100
        print(f"{i+1:2d}    {row['Exp_ID']:2d}      {row['A_LearningRate']:.6f}  {row['B_EpsilonDecay']:.4f}   {row['D_Gamma']:.2f}     {row['Comprehensive_Mean']:.4f}     {row['SNR_Value']:6.2f}   {success_rate:3.0f}%")
    
    print("\nå10åå®éªŒç»„:")
    print("æ’å  å®éªŒç»„  å­¦ä¹ ç‡      è¡°å‡ç‡    æŠ˜æ‰£å› å­  ç»¼åˆå¾—åˆ†    SNR(dB)   æˆåŠŸç‡")
    print("-" * 85)
    for i in range(max(0, len(df_sorted)-10), len(df_sorted)):
        row = df_sorted.iloc[i]
        success_rate = row['Successful_Runs'] / 10 * 100
        print(f"{i+1:2d}    {row['Exp_ID']:2d}      {row['A_LearningRate']:.6f}  {row['B_EpsilonDecay']:.4f}   {row['D_Gamma']:.2f}     {row['Comprehensive_Mean']:.4f}     {row['SNR_Value']:6.2f}   {success_rate:3.0f}%")
    
    # 7. å‚æ•°åˆ†å¸ƒåˆ†æ
    print("\nğŸ“ˆ 7. å‚æ•°åˆ†å¸ƒåˆ†æ:")
    
    # å­¦ä¹ ç‡åˆ†å¸ƒ
    lr_groups = df.groupby('A_LearningRate')['SNR_Value'].agg(['mean', 'std', 'count'])
    print(f"\nå­¦ä¹ ç‡æ€§èƒ½åˆ†æ:")
    print("å­¦ä¹ ç‡        å¹³å‡SNR    æ ‡å‡†å·®    æ ·æœ¬æ•°")
    print("-" * 45)
    for lr, stats in lr_groups.iterrows():
        std_val = stats['std'] if not pd.isna(stats['std']) else 0.0
        print(f"{lr:.6f}     {stats['mean']:7.3f}   {std_val:6.3f}    {stats['count']:3.0f}")
    
    # æ¢ç´¢ç‡è¡°å‡åˆ†å¸ƒ
    decay_groups = df.groupby('B_EpsilonDecay')['SNR_Value'].agg(['mean', 'std', 'count'])
    print(f"\næ¢ç´¢ç‡è¡°å‡æ€§èƒ½åˆ†æ:")
    print("è¡°å‡ç‡     å¹³å‡SNR    æ ‡å‡†å·®    æ ·æœ¬æ•°")
    print("-" * 40)
    for decay, stats in decay_groups.iterrows():
        std_val = stats['std'] if not pd.isna(stats['std']) else 0.0
        print(f"{decay:.4f}    {stats['mean']:7.3f}   {std_val:6.3f}    {stats['count']:3.0f}")
    
    # æŠ˜æ‰£å› å­åˆ†å¸ƒ
    gamma_groups = df.groupby('D_Gamma')['SNR_Value'].agg(['mean', 'std', 'count'])
    print(f"\næŠ˜æ‰£å› å­æ€§èƒ½åˆ†æ:")
    print("æŠ˜æ‰£å› å­   å¹³å‡SNR    æ ‡å‡†å·®    æ ·æœ¬æ•°")
    print("-" * 37)
    for gamma, stats in gamma_groups.iterrows():
        std_val = stats['std'] if not pd.isna(stats['std']) else 0.0
        print(f"{gamma:.2f}      {stats['mean']:7.3f}   {std_val:6.3f}    {stats['count']:3.0f}")
    
    # 8. ä¿å­˜å®Œæ•´åˆ†æç»“æœ
    print("\nğŸ’¾ 8. ä¿å­˜åˆ†æç»“æœ:")
    
    # ä¿å­˜æ’åºåçš„å®Œæ•´æ•°æ®
    output_file = "ç”°å£L49å®éªŒæ·±åº¦åˆ†æç»“æœ_20250624.csv"
    df_sorted_with_rank = df_sorted.copy()
    df_sorted_with_rank['æ€§èƒ½æ’å'] = range(1, len(df_sorted_with_rank) + 1)
    df_sorted_with_rank.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"âœ… å®Œæ•´æ’åºæ•°æ®å·²ä¿å­˜: {output_file}")
    
    # ä¿å­˜å› å­æ•ˆåº”è¡¨
    factor_effects_data = []
    for factor, name in zip(factors, factor_names):
        effects = factor_effects[factor]
        for level in range(1, 8):
            if str(level) in effects:
                factor_effects_data.append({
                    'å› å­åç§°': name,
                    'å› å­ä»£ç ': factor,
                    'æ°´å¹³': level,
                    'SNRå‡å€¼': effects[str(level)],
                    'é‡è¦æ€§æ’å': effects['rank']
                })
    
    effects_df = pd.DataFrame(factor_effects_data)
    effects_file = "ç”°å£å› å­æ°´å¹³æ•ˆåº”è¡¨_20250624.csv"
    effects_df.to_csv(effects_file, index=False, encoding='utf-8-sig')
    print(f"âœ… å› å­æ•ˆåº”è¡¨å·²ä¿å­˜: {effects_file}")
    
    # 9. æœ€ç»ˆç»“è®º
    print("\nğŸ¯ 9. æœ€ç»ˆåˆ†æç»“è®º:")
    print(f"âœ… æœ€ä¼˜å®éªŒç»„: {best_snr_row['Exp_ID']}")
    print(f"âœ… æœ€ä¼˜SNR: {best_snr_row['SNR_Value']:.3f} dB") 
    print(f"âœ… æœ€ä¼˜å‚æ•°ç»„åˆ:")
    print(f"   å­¦ä¹ ç‡: {best_snr_row['A_LearningRate']}")
    print(f"   æ¢ç´¢ç‡è¡°å‡: {best_snr_row['B_EpsilonDecay']}")
    print(f"   æŠ˜æ‰£å› å­: {best_snr_row['D_Gamma']}")
    print(f"   é¹°ç¾¤åˆ†ç»„: {best_snr_row['C_GroupRatios']}")
    print(f"âœ… æ€§èƒ½æ”¹è¿›: ç›¸æ¯”æœ€å·®é…ç½®æå‡ {snr_stats['max'] - snr_stats['min']:.3f} dB")
    print(f"âœ… å…³é”®å½±å“å› å­: {factor_importance[0][1]} (æ’åç¬¬1)")
    
    print("\n" + "=" * 80)
    print("ğŸ” æ·±åº¦åˆ†æå®Œæˆï¼æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ç”Ÿæˆã€‚")
    print("=" * 80)

if __name__ == "__main__":
    analyze_taguchi_results() 