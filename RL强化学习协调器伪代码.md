# RLå¼ºåŒ–å­¦ä¹ åè°ƒå™¨ä¼ªä»£ç 

## 1. ç®—æ³•æ¦‚è¿°

åŸºäºæ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰çš„å¼ºåŒ–å­¦ä¹ åè°ƒå™¨ï¼Œç”¨äºRL-Chaotic-HHOç®—æ³•çš„æ™ºèƒ½ç­–ç•¥é€‰æ‹©å’Œè‡ªé€‚åº”è°ƒåº¦ã€‚åè°ƒå™¨å°†å¤šç›®æ ‡ä¼˜åŒ–è¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œé€šè¿‡æ„ŸçŸ¥å½“å‰æœç´¢çŠ¶æ€ï¼Œæ™ºèƒ½é€‰æ‹©æœ€ä¼˜çš„ç­–ç•¥ç»„åˆã€‚

## 2. æ ¸å¿ƒæ•°æ®ç»“æ„

### 2.1 ç»éªŒå…ƒç»„å®šä¹‰
```
Experience = (state, action, reward, next_state, done)
```

### 2.2 DQNç½‘ç»œç»“æ„
```
Network Architecture:
  Input Layer: 14 neurons (çŠ¶æ€ç»´åº¦)
  Hidden Layer 1: 128 neurons + ReLU
  Hidden Layer 2: 64 neurons + ReLU  
  Output Layer: 7 neurons (åŠ¨ä½œQå€¼)
```

### 2.3 åŠ¨ä½œç©ºé—´å®šä¹‰
```
ActionSpace = {
  0: "å¼ºåŒ–å…¨å±€æ¢ç´¢",
  1: "å¼ºåŒ–å±€éƒ¨å¼€å‘", 
  2: "å¹³è¡¡æœç´¢",
  3: "å¤šæ ·æ€§æ•‘æ´",
  4: "ç²¾è‹±å¼ºåŒ–",
  5: "å…¨å±€é‡å¯",
  6: "èµ„æºé‡åˆ†é…"
}
```

## 3. ä¸»è¦ç®—æ³•æµç¨‹

### ç®—æ³•1: RLåè°ƒå™¨åˆå§‹åŒ–
```
Algorithm 1: RLCoordinator_Initialize
Input: problem P, state_dim=14, action_dim=7, learning_rate=0.001
Output: åˆå§‹åŒ–çš„RLåè°ƒå™¨

Begin
  // ç½‘ç»œåˆå§‹åŒ–
  q_network â† DQNNetwork(state_dim, action_dim)
  target_network â† DQNNetwork(state_dim, action_dim)
  target_network.copy_weights_from(q_network)
  
  // ç»éªŒå›æ”¾ç¼“å†²åŒºåˆå§‹åŒ–
  memory â† PrioritizedReplayBuffer(capacity=10000, alpha=0.6)
  
  // è¶…å‚æ•°è®¾ç½®
  epsilon â† 0.9                    // åˆå§‹æ¢ç´¢ç‡
  epsilon_decay â† 0.995            // æ¢ç´¢ç‡è¡°å‡
  epsilon_min â† 0.01               // æœ€å°æ¢ç´¢ç‡
  gamma â† 0.98                     // æŠ˜æ‰£å› å­
  batch_size â† 32                  // æ‰¹æ¬¡å¤§å°
  target_update_freq â† 100         // ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
  
  // ç»Ÿè®¡å˜é‡åˆå§‹åŒ–
  training_step â† 0
  action_counts â† zeros(action_dim)
  action_rewards â† zeros(action_dim)
  action_success_rates â† zeros(action_dim)
  
  Return RLåè°ƒå™¨å®ä¾‹
End
```

### ç®—æ³•2: çŠ¶æ€æ„å»º
```
Algorithm 2: ConstructState
Input: population Pop, pareto_solutions P, current_iteration t, max_iterations T_max
Output: 14ç»´çŠ¶æ€å‘é‡ state

Begin
  state â† vector(14)
  
  // 1. æœç´¢è¿›å±•ç‰¹å¾ (ç»´åº¦0-3)
  progress â† t / T_max
  improvement_rate â† CalculateImprovementRate()
  stagnation_ratio â† min(no_improvement_count / 50, 1.0)
  pareto_size_ratio â† |P| / max(20, |P|)
  
  // 2. è§£é›†è´¨é‡ç‰¹å¾ (ç»´åº¦4-5)
  If |P| > 0 Then
    best_makespan â† min{sol.makespan : sol âˆˆ P}
    best_tardiness â† min{sol.total_tardiness : sol âˆˆ P}
    quality_score â† 1.0 / (1.0 + best_makespan / theoretical_lower_bound)
  Else
    quality_score â† 0.0
  EndIf
  
  factory_balance â† CalculateFactoryBalance()
  
  // 3. å„ç»„æ€§èƒ½ç‰¹å¾ (ç»´åº¦6-13)
  group_performance â† GetGroupPerformanceMetrics()
  
  // ç»„åˆçŠ¶æ€å‘é‡
  state[0] â† progress
  state[1] â† improvement_rate  
  state[2] â† stagnation_ratio
  state[3] â† pareto_size_ratio
  state[4] â† quality_score
  state[5] â† factory_balance
  state[6:14] â† group_performance[0:8]
  
  Return state
End
```

### ç®—æ³•3: çŠ¶æ€æ„å»ºï¼ˆå›¾ä¸­å…¬å¼ç‰ˆæœ¬ï¼‰
```
Algorithm 3: ConstructStateVector (åŸºäºå›¾ä¸­å…¬å¼23)
Input: å‰æ²¿è§£é›† P_t, ç§ç¾¤ Pop_t, ç²¾è‹±è§£é›† Elite_t
Output: çŠ¶æ€å‘é‡ S_t = [D_t, Î”HV_t, S_t, Î“_t, Ï_t]

Begin
  // D_t: å‰éæ”¯é…è§£é›†çš„å¹³å‡æ‹¥æŒ¤åº¦
  D_t â† CalculateAverageCrowdingDistance(P_t)
  
  // Î”HV_t: è¶…ä½“ç§¯å¢é‡ï¼ˆåæ˜ è§£é›†æ•´ä½“æ‰©å±•èƒ½åŠ›ï¼‰
  HV_current â† CalculateHypervolume(P_t)
  HV_previous â† GetPreviousHypervolume()
  Î”HV_t â† (HV_current - HV_previous) / max(HV_previous, 1e-10)
  
  // S_t: SpacingæŒ‡æ ‡ï¼ˆè¡¡é‡è§£é›†åˆ†å¸ƒçš„å‡åŒ€æ€§ï¼‰
  S_t â† CalculateSpacingMetric(P_t)
  
  // Î“_t: ç²¾è‹±è§£ä¸­æ–°å¼•å…¥è§£æ‰€å æ¯”ä¾‹
  new_solutions_count â† CountNewSolutionsInElite(Elite_t)
  Î“_t â† new_solutions_count / |Elite_t| if |Elite_t| > 0 else 0
  
  // Ï_t: æˆåŠŸç‡ï¼ˆå¯åŠ¨è·³åŠ¨æ‰€å¸¦æ¥çš„æœ‰æ•ˆè§£æ¯”ä¾‹ï¼‰
  Ï_t â† CalculateSuccessRate()
  
  S_t â† [D_t, Î”HV_t, S_t, Î“_t, Ï_t]
  Return S_t
End
```

### ç®—æ³•4: åŠ¨ä½œé€‰æ‹©
```
Algorithm 4: SelectAction
Input: state s, trainingæ¨¡å¼æ ‡å¿—
Output: é€‰æ‹©çš„åŠ¨ä½œ action

Begin
  // ç¡®ä¿çŠ¶æ€ç»´åº¦æ­£ç¡®
  If len(state) â‰  state_dim Then
    If len(state) > state_dim Then
      state â† state[0:state_dim]
    Else
      state â† pad(state, (0, state_dim - len(state)), 'constant')
    EndIf
  EndIf
  
  // Îµ-è´ªå©ªç­–ç•¥
  If training AND random() < epsilon Then
    action â† randint(0, action_dim - 1)  // æ¢ç´¢
  Else
    q_values â† q_network.forward(state)  // åˆ©ç”¨
    action â† argmax(q_values)
    
    // è®°å½•Qå€¼å†å²
    q_value_history.append(q_values)
  EndIf
  
  // æ›´æ–°åŠ¨ä½œç»Ÿè®¡
  action_counts[action] â† action_counts[action] + 1
  
  Return action
End
```

### ç®—æ³•5: å¥–åŠ±è®¡ç®—
```
Algorithm 5: CalculateReward (åŸºäºå›¾ä¸­å…¬å¼26)
Input: çŠ¶æ€ s_t, åŠ¨ä½œ a_t, ä¸‹ä¸€çŠ¶æ€ s_{t+1}
Output: å¥–åŠ±å€¼ r_t

Begin
  // æå–çŠ¶æ€ç‰¹å¾
  D_t â† s_t[0]         // æ‹¥æŒ¤åº¦
  Î”HV_t â† s_{t+1}[1]   // è¶…ä½“ç§¯æ”¹è¿›
  S_t â† s_{t+1}[2]     // SpacingæŒ‡æ ‡
  Î“_t â† s_{t+1}[3]     // ç²¾è‹±è§£æ–°å¼•å…¥æ¯”ä¾‹
  
  // æƒé‡ç³»æ•°
  Î± â† 0.4              // è¶…ä½“ç§¯æ”¹è¿›æƒé‡
  Î² â† 0.3              // åˆ†å¸ƒå‡åŒ€æ€§æƒé‡
  Î³ â† 0.2              // ç²¾è‹±æ›´æ–°æƒé‡
  Î» â† 0.1              // æ‹¥æŒ¤åº¦æƒ©ç½šæƒé‡
  
  // ç»¼åˆå¥–åŠ±è®¡ç®—ï¼ˆå…¬å¼26ï¼‰
  r_t â† Î± Ã— Î”HV_t + Î² Ã— (1 - S_t) + Î³ Ã— Î“_t - Î» Ã— D_t
  
  Return r_t
End
```

### ç®—æ³•6: ç»éªŒå­˜å‚¨
```
Algorithm 6: StoreExperience
Input: state, action, reward, next_state, done=False
Output: å­˜å‚¨ç»éªŒåˆ°ç¼“å†²åŒº

Begin
  // ç¡®ä¿çŠ¶æ€ç»´åº¦ä¸€è‡´
  state â† EnsureStateDimension(state, state_dim)
  next_state â† EnsureStateDimension(next_state, state_dim)
  
  experience â† Experience(state, action, reward, next_state, done)
  
  // è®¡ç®—TDè¯¯å·®ä½œä¸ºä¼˜å…ˆçº§
  current_q â† q_network.forward(state)[action]
  If done Then
    target_q â† reward
  Else
    next_q_values â† target_network.forward(next_state)
    target_q â† reward + gamma Ã— max(next_q_values)
  EndIf
  
  td_error â† |target_q - current_q|
  
  // å­˜å‚¨åˆ°ä¼˜å…ˆçº§ç»éªŒå›æ”¾ç¼“å†²åŒº
  memory.push(experience, td_error)
End
```

### ç®—æ³•7: ç½‘ç»œè®­ç»ƒ
```
Algorithm 7: TrainNetwork
Input: æ— 
Output: æ›´æ–°ç½‘ç»œæƒé‡

Begin
  If len(memory) < batch_size Then
    Return  // ç»éªŒä¸è¶³ï¼Œè·³è¿‡è®­ç»ƒ
  EndIf
  
  // é‡‡æ ·ç»éªŒæ‰¹æ¬¡
  experiences, indices, weights â† memory.sample(batch_size)
  
  // æå–æ‰¹æ¬¡æ•°æ®
  states â† [exp.state for exp in experiences]
  actions â† [exp.action for exp in experiences]
  rewards â† [exp.reward for exp in experiences]
  next_states â† [exp.next_state for exp in experiences]
  dones â† [exp.done for exp in experiences]
  
  // è®¡ç®—ç›®æ ‡Qå€¼
  current_q_values â† [q_network.forward(state) for state in states]
  next_q_values â† [target_network.forward(state) for state in next_states]
  
  target_q_values â† copy(current_q_values)
  
  For i = 0 to len(experiences) - 1 Do
    If dones[i] Then
      target_q_values[i][actions[i]] â† rewards[i]
    Else
      target_q_values[i][actions[i]] â† rewards[i] + gamma Ã— max(next_q_values[i])
    EndIf
  EndFor
  
  // è®¡ç®—æŸå¤±å’Œæ›´æ–°ç½‘ç»œ
  td_errors â† []
  total_loss â† 0
  
  For i = 0 to len(experiences) - 1 Do
    state â† states[i]
    action â† actions[i]
    target â† target_q_values[i][action]
    current â† current_q_values[i][action]
    
    td_error â† target - current
    td_errors.append(|td_error|)
    total_loss â† total_loss + weights[i] Ã— (td_error)Â²
    
    // æ¢¯åº¦æ›´æ–°
    SimpleGradientUpdate(state, action, td_error, weights[i])
  EndFor
  
  // æ›´æ–°ä¼˜å…ˆçº§
  memory.update_priorities(indices, td_errors)
  
  // è®°å½•æŸå¤±
  loss_history.append(total_loss / len(experiences))
End
```

### ç®—æ³•8: ä¸»æ›´æ–°æµç¨‹
```
Algorithm 8: Update
Input: state, action, reward, next_state
Output: æ›´æ–°åè°ƒå™¨

Begin
  // å­˜å‚¨ç»éªŒ
  StoreExperience(state, action, reward, next_state)
  
  // æ›´æ–°åŠ¨ä½œå¥–åŠ±ç»Ÿè®¡
  action_rewards[action] â† action_rewards[action] + reward
  If reward > 0 Then
    action_success_rates[action] â† action_success_rates[action] + 1
  EndIf
  
  // è®­ç»ƒç½‘ç»œ
  If len(memory) â‰¥ batch_size Then
    TrainNetwork()
  EndIf
  
  // æ›´æ–°ç›®æ ‡ç½‘ç»œ
  If training_step mod target_update_freq = 0 Then
    target_network.copy_weights_from(q_network)
    Print("ğŸ¯ æ›´æ–°ç›®æ ‡ç½‘ç»œ (æ­¥éª¤: " + training_step + ")")
  EndIf
  
  // è¡°å‡æ¢ç´¢ç‡
  If epsilon > epsilon_min Then
    epsilon â† epsilon Ã— epsilon_decay
  EndIf
  
  training_step â† training_step + 1
End
```

### ç®—æ³•9: ç­–ç•¥ç»Ÿè®¡
```
Algorithm 9: GetStrategyStatistics
Input: æ— 
Output: ç­–ç•¥ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯

Begin
  total_actions â† sum(action_counts)
  If total_actions = 0 Then
    Return {}
  EndIf
  
  stats â† {}
  For action_id = 0 to action_dim - 1 Do
    action_name â† action_space[action_id]
    count â† action_counts[action_id]
    
    stats[action_name] â† {
      'usage_count': count,
      'usage_rate': count / total_actions,
      'average_reward': action_rewards[action_id] / max(count, 1),
      'success_rate': action_success_rates[action_id] / max(count, 1)
    }
  EndFor
  
  Return stats
End
```

## 4. å…³é”®ç‰¹æ€§è¯´æ˜

### 4.1 çŠ¶æ€ç©ºé—´è®¾è®¡
- **14ç»´çŠ¶æ€å‘é‡**ï¼šå…¨é¢åæ˜ æœç´¢è¿‡ç¨‹çš„å…³é”®ä¿¡æ¯
- **å¤šå±‚æ¬¡ç‰¹å¾**ï¼šåŒ…å«æœç´¢è¿›å±•ã€è§£é›†è´¨é‡ã€ç§ç¾¤ç‰¹å¾ã€ç»„æ€§èƒ½ç­‰
- **å½’ä¸€åŒ–å¤„ç†**ï¼šæ‰€æœ‰ç‰¹å¾æ˜ å°„åˆ°[0,1]åŒºé—´ï¼Œæé«˜å­¦ä¹ æ•ˆç‡

### 4.2 åŠ¨ä½œç©ºé—´è®¾è®¡  
- **7ç§ç­–ç•¥**ï¼šæ¶µç›–æ¢ç´¢ã€å¼€å‘ã€å¹³è¡¡ã€å¤šæ ·æ€§ã€ç²¾è‹±åŒ–ç­‰æ ¸å¿ƒéœ€æ±‚
- **ç¦»æ•£åŠ¨ä½œ**ï¼šä¾¿äºQå­¦ä¹ çš„ä»·å€¼è¯„ä¼°å’Œç­–ç•¥é€‰æ‹©
- **ç­–ç•¥äº’è¡¥**ï¼šå„åŠ¨ä½œé’ˆå¯¹ä¸åŒæœç´¢é˜¶æ®µå’Œé—®é¢˜çŠ¶æ€

### 4.3 å¥–åŠ±å‡½æ•°è®¾è®¡
- **å¤šç›®æ ‡ç»¼åˆ**ï¼šç»“åˆè¶…ä½“ç§¯æ”¹è¿›ã€åˆ†å¸ƒå‡åŒ€æ€§ã€ç²¾è‹±æ›´æ–°ç­‰æŒ‡æ ‡
- **è‡ªé€‚åº”æƒé‡**ï¼šæ ¹æ®ä¼˜åŒ–é˜¶æ®µåŠ¨æ€è°ƒæ•´å„é¡¹æƒé‡
- **å³æ—¶åé¦ˆ**ï¼šæä¾›åŠæ—¶çš„ç­–ç•¥é€‰æ‹©æŒ‡å¯¼ä¿¡å·

### 4.4 å­¦ä¹ æœºåˆ¶
- **DQNæ¶æ„**ï¼šæ·±åº¦Qç½‘ç»œå¤„ç†é«˜ç»´è¿ç»­çŠ¶æ€ç©ºé—´
- **ç»éªŒå›æ”¾**ï¼šä¼˜å…ˆçº§ç»éªŒå›æ”¾æé«˜å­¦ä¹ æ•ˆç‡
- **ç›®æ ‡ç½‘ç»œ**ï¼šåŒç½‘ç»œç»“æ„å¢å¼ºè®­ç»ƒç¨³å®šæ€§
- **Îµ-è´ªå©ªç­–ç•¥**ï¼šå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨

## 5. ç®—æ³•å¤æ‚åº¦åˆ†æ

- **æ—¶é—´å¤æ‚åº¦**: O(batch_size Ã— network_forward_time)
- **ç©ºé—´å¤æ‚åº¦**: O(memory_size + network_parameters)
- **æ”¶æ•›æ€§**: åœ¨æ»¡è¶³ä¸€å®šæ¡ä»¶ä¸‹ï¼ŒDQNç®—æ³•å…·æœ‰æ”¶æ•›ä¿è¯

## 6. å®é™…åº”ç”¨è¦ç‚¹

1. **çŠ¶æ€ç‰¹å¾é€‰æ‹©**: ç¡®ä¿çŠ¶æ€ç‰¹å¾å…·æœ‰é©¬å°”å¯å¤«æ€§
2. **å¥–åŠ±å‡½æ•°è°ƒä¼˜**: æ ¹æ®å…·ä½“é—®é¢˜è°ƒæ•´æƒé‡ç³»æ•°
3. **è¶…å‚æ•°è®¾ç½®**: å­¦ä¹ ç‡ã€æ¢ç´¢ç‡ç­‰éœ€è¦ç»†è‡´è°ƒå‚
4. **ç½‘ç»œæ¶æ„**: æ ¹æ®é—®é¢˜å¤æ‚åº¦è°ƒæ•´éšè—å±‚å¤§å°
5. **è®­ç»ƒç­–ç•¥**: é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ æˆ–é¢„è®­ç»ƒæé«˜æ•ˆæœ 