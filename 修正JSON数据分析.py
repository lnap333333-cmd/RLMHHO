import pandas as pd
import numpy as np
import json
import os
from pathlib import Path
import glob

def load_experiment_data(result_dir):
    """ä»JSONæ–‡ä»¶åŠ è½½å®Œæ•´çš„å®éªŒæ•°æ®"""
    
    print("ğŸ“‚ æ­£åœ¨åŠ è½½å®éªŒæ•°æ®...")
    
    # åŠ è½½ç”°å£åˆ†æç»“æœ
    analysis_file = result_dir / "taguchi_analysis.json"
    with open(analysis_file, 'r', encoding='utf-8') as f:
        analysis_data = json.load(f)
    
    print(f"âœ… ç”°å£åˆ†ææ•°æ®åŠ è½½å®Œæˆ")
    
    # ä»å„ä¸ªå®éªŒæ±‡æ€»æ–‡ä»¶æ„å»ºDataFrame
    experiments = []
    
    # æ‰¾åˆ°æ‰€æœ‰å®éªŒæ±‡æ€»æ–‡ä»¶
    summary_files = list(result_dir.glob("exp_*_summary.json"))
    print(f"ğŸ“„ æ‰¾åˆ°{len(summary_files)}ä¸ªå®éªŒæ±‡æ€»æ–‡ä»¶")
    
    for summary_file in summary_files:
        try:
            with open(summary_file, 'r', encoding='utf-8') as f:
                exp_data = json.load(f)
            
            exp_id = exp_data['exp_id']
            exp_config = exp_data['exp_config']
            
            # ä»ç¬¬ä¸€ä¸ªè¿è¡Œè·å–å‚æ•°é…ç½®
            if exp_data['individual_results']:
                first_result = exp_data['individual_results'][0]
                params = first_result['parameters']
                
                learning_rate = params['learning_rate']
                epsilon_decay = params['epsilon_decay']
                gamma = params['gamma']
                group_ratios = params['group_ratios']
                
                # æ”¶é›†æ‰€æœ‰è¿è¡Œçš„æ€§èƒ½æ•°æ®
                hv_values = []
                igd_values = []
                gd_values = []
                comp_scores = []
                successful_runs = 0
                
                for result in exp_data['individual_results']:
                    if 'metrics' in result:
                        metrics = result['metrics']
                        if all(key in metrics for key in ['hypervolume', 'igd', 'gd']):
                            hv_values.append(metrics['hypervolume'])
                            igd_values.append(metrics['igd'])
                            gd_values.append(metrics['gd'])
                            
                            # è®¡ç®—5:3:2åŠ æƒç»¼åˆå¾—åˆ†
                            # å½’ä¸€åŒ–å¤„ç† - è¿™é‡Œéœ€è¦æ³¨æ„IGDå’ŒGDè¶Šå°è¶Šå¥½
                            hv_norm = metrics['hypervolume']  # HVè¶Šå¤§è¶Šå¥½
                            igd_norm = 1.0 / (1.0 + metrics['igd'])  # IGDè¶Šå°è¶Šå¥½ï¼Œè½¬æ¢ä¸ºè¶Šå¤§è¶Šå¥½
                            gd_norm = 1.0 / (1.0 + metrics['gd'])    # GDè¶Šå°è¶Šå¥½ï¼Œè½¬æ¢ä¸ºè¶Šå¤§è¶Šå¥½
                            
                            comp_score = 0.5 * hv_norm + 0.3 * igd_norm + 0.2 * gd_norm
                            comp_scores.append(comp_score)
                            successful_runs += 1
                
                if comp_scores:  # ç¡®ä¿æœ‰æœ‰æ•ˆæ•°æ®
                    # è®¡ç®—ç»Ÿè®¡é‡
                    hv_mean = np.mean(hv_values)
                    hv_std = np.std(hv_values) if len(hv_values) > 1 else 0
                    igd_mean = np.mean(igd_values)
                    igd_std = np.std(igd_values) if len(igd_values) > 1 else 0
                    gd_mean = np.mean(gd_values)
                    gd_std = np.std(gd_values) if len(gd_values) > 1 else 0
                    comp_mean = np.mean(comp_scores)
                    comp_std = np.std(comp_scores) if len(comp_scores) > 1 else 0
                    
                    # è®¡ç®—SNR (ä¿¡å™ªæ¯”ï¼Œæœ›å¤§ç‰¹æ€§)
                    if len(comp_scores) > 0:
                        # ä½¿ç”¨ç”°å£æ–¹æ³•çš„ä¿¡å™ªæ¯”è®¡ç®—å…¬å¼ï¼šSNR = -10 * log10(mean(1/yi^2))
                        snr = -10 * np.log10(np.mean([1/(score**2) for score in comp_scores if score > 0]))
                    else:
                        snr = -100  # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®
                    
                    experiments.append({
                        'Exp_ID': exp_id,
                        'A_LearningRate': learning_rate,
                        'B_EpsilonDecay': epsilon_decay,
                        'D_Gamma': gamma,
                        'C_GroupRatios': str(group_ratios),
                        'HV_Mean': hv_mean,
                        'HV_Std': hv_std,
                        'IGD_Mean': igd_mean,
                        'IGD_Std': igd_std,
                        'GD_Mean': gd_mean,
                        'GD_Std': gd_std,
                        'Comprehensive_Mean': comp_mean,
                        'Comprehensive_Std': comp_std,
                        'SNR_Value': snr,
                        'Successful_Runs': successful_runs
                    })
        
        except Exception as e:
            print(f"âš ï¸ å®éªŒ{summary_file.stem}åŠ è½½å¤±è´¥: {e}")
            continue
    
    df = pd.DataFrame(experiments)
    df = df.sort_values('Exp_ID').reset_index(drop=True)
    
    print(f"âœ… æ•°æ®æ„å»ºå®Œæˆ: {len(df)}ä¸ªå®éªŒç»„")
    
    return df, analysis_data

def analyze_taguchi_results():
    """å®Œæ•´åˆ†æç”°å£å®éªŒç»“æœ"""
    
    print("=" * 80)
    print("ğŸ” ç”°å£L49å®éªŒæ·±åº¦æ•°æ®åˆ†æ - 2025-06-24")
    print("=" * 80)
    
    # 1. æ•°æ®æ–‡ä»¶æ£€æŸ¥
    print("\nğŸ“‚ 1. æ•°æ®æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥:")
    result_dir = Path("taguchi_results_20250624_172744")
    
    if not result_dir.exists():
        print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {result_dir}")
        return
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    analysis_file = result_dir / "taguchi_analysis.json"
    
    print(f"ğŸ“ˆ ç”°å£åˆ†ææ–‡ä»¶: {'âœ…' if analysis_file.exists() else 'âŒ'} {analysis_file}")
    
    if not analysis_file.exists():
        print("âŒ å…³é”®æ•°æ®æ–‡ä»¶ç¼ºå¤±ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
        return
    
    # æ£€æŸ¥å®éªŒæ–‡ä»¶æ•°é‡
    exp_files = list(result_dir.glob("exp_*_summary.json"))
    run_files = list(result_dir.glob("exp_*_run_*.json"))
    print(f"ğŸ“„ å®éªŒæ±‡æ€»æ–‡ä»¶: {len(exp_files)}ä¸ª")
    print(f"ğŸ“„ è¿è¡Œè¯¦ç»†æ–‡ä»¶: {len(run_files)}ä¸ª")
    
    # 2. åŠ è½½æ•°æ®
    print("\nğŸ“‹ 2. æ•°æ®åŠ è½½å’Œå¤„ç†:")
    try:
        df, analysis_data = load_experiment_data(result_dir)
        print(f"âœ… æ•°æ®å¤„ç†å®Œæˆ: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
        print(f"ğŸ“Š åˆ—å: {list(df.columns)}")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 3. åŸºç¡€ç»Ÿè®¡åˆ†æ
    print("\nğŸ“ˆ 3. åŸºç¡€ç»Ÿè®¡åˆ†æ:")
    print(f"å®éªŒç»„æ€»æ•°: {len(df)}")
    print(f"æ¯ç»„é‡å¤æ¬¡æ•°: 10æ¬¡")
    print(f"æ€»å®éªŒæ¬¡æ•°: {len(df) * 10}")
    print(f"å®éªŒæˆåŠŸç‡: {(df['Successful_Runs'].sum() / (len(df) * 10) * 100):.1f}%")
    
    # SNRç»Ÿè®¡
    valid_snr = df[df['SNR_Value'] > -100]['SNR_Value']  # è¿‡æ»¤æ‰æ— æ•ˆæ•°æ®
    if len(valid_snr) > 0:
        snr_stats = valid_snr.describe()
        print(f"\nSNRç»Ÿè®¡åˆ†æ (æœ‰æ•ˆæ•°æ®{len(valid_snr)}ç»„):")
        print(f"  æœ€å¤§å€¼: {snr_stats['max']:.3f} dB")
        print(f"  æœ€å°å€¼: {snr_stats['min']:.3f} dB")
        print(f"  å¹³å‡å€¼: {snr_stats['mean']:.3f} dB")
        print(f"  æ ‡å‡†å·®: {snr_stats['std']:.3f} dB")
        print(f"  ä¸­ä½æ•°: {snr_stats['50%']:.3f} dB")
        print(f"  æ€§èƒ½è·¨åº¦: {snr_stats['max'] - snr_stats['min']:.3f} dB")
    else:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„SNRæ•°æ®")
        return
    
    # 4. å¯»æ‰¾æœ€ä¼˜ç»“æœ
    print("\nğŸ¯ 4. æœ€ä¼˜ç»“æœè¯†åˆ«:")
    
    # æŒ‰SNRæ’åºï¼ˆæ’é™¤æ— æ•ˆæ•°æ®ï¼‰
    valid_df = df[df['SNR_Value'] > -100]
    if len(valid_df) == 0:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å®éªŒæ•°æ®")
        return
        
    best_snr_idx = valid_df['SNR_Value'].idxmax()
    best_snr_row = valid_df.loc[best_snr_idx]
    
    print(f"\næŒ‰SNRæœ€å¤§å€¼æ‰¾åˆ°çš„æœ€ä¼˜è§£:")
    print(f"  å®éªŒç»„: {best_snr_row['Exp_ID']}")
    print(f"  SNRå€¼: {best_snr_row['SNR_Value']:.3f} dB")
    print(f"  å­¦ä¹ ç‡: {best_snr_row['A_LearningRate']}")
    print(f"  æ¢ç´¢ç‡è¡°å‡: {best_snr_row['B_EpsilonDecay']}")
    print(f"  æŠ˜æ‰£å› å­: {best_snr_row['D_Gamma']}")
    print(f"  é¹°ç¾¤åˆ†ç»„: {best_snr_row['C_GroupRatios']}")
    print(f"  ç»¼åˆå¾—åˆ†: {best_snr_row['Comprehensive_Mean']:.4f} Â± {best_snr_row['Comprehensive_Std']:.4f}")
    print(f"  HVå‡å€¼: {best_snr_row['HV_Mean']:.4f} Â± {best_snr_row['HV_Std']:.4f}")
    print(f"  IGDå‡å€¼: {best_snr_row['IGD_Mean']:.4f} Â± {best_snr_row['IGD_Std']:.4f}")
    print(f"  GDå‡å€¼: {best_snr_row['GD_Mean']:.4f} Â± {best_snr_row['GD_Std']:.4f}")
    print(f"  æˆåŠŸè¿è¡Œ: {best_snr_row['Successful_Runs']}/10æ¬¡")
    
    # ç”°å£é¢„æµ‹æœ€ä¼˜
    optimal_combo = analysis_data['optimal_combination']
    predicted_snr = analysis_data['predicted_snr']
    print(f"\nç”°å£æ–¹æ³•é¢„æµ‹æœ€ä¼˜:")
    print(f"  é¢„æµ‹ç»„åˆ: A={optimal_combo['A']}, B={optimal_combo['B']}, C={optimal_combo['C']}, D={optimal_combo['D']}")
    print(f"  é¢„æµ‹SNR: {predicted_snr:.3f} dB")
    
    # 5. ç”°å£æ–¹æ³•å› å­æ•ˆåº”åˆ†æ
    print("\nğŸ”¬ 5. ç”°å£æ–¹æ³•å› å­æ•ˆåº”åˆ†æ:")
    factors = ['A', 'B', 'C', 'D']
    factor_names = ['å­¦ä¹ ç‡', 'æ¢ç´¢ç‡è¡°å‡', 'é¹°ç¾¤åˆ†ç»„æ¯”ä¾‹', 'æŠ˜æ‰£å› å­']
    
    factor_effects = analysis_data['factor_effects']
    factor_importance = []
    
    for factor, name in zip(factors, factor_names):
        effects = factor_effects[factor]
        rank = effects['rank']
        range_val = effects['range']
        # è·å–ANOVAç»“æœ
        anova = analysis_data['anova_results'][factor]
        contribution = anova['contribution']
        f_value = anova['f_value']
        
        factor_importance.append((rank, name, factor, range_val, contribution, f_value))
    
    # æŒ‰é‡è¦æ€§æ’åº
    factor_importance.sort(key=lambda x: x[0])
    
    print("å› å­é‡è¦æ€§æ’åº:")
    for rank, name, factor, range_val, contribution, f_value in factor_importance:
        print(f"{rank}. {name}({factor}): æå·®={range_val:.3f}, è´¡çŒ®åº¦={contribution:.2f}%, Få€¼={f_value:.3f}")
    
    # 6. æ’åºåˆ†æ
    print("\nğŸ“Š 6. å®éªŒç»„æ€§èƒ½æ’åºåˆ†æ:")
    df_sorted = valid_df.sort_values('SNR_Value', ascending=False).reset_index(drop=True)
    
    print("\nå‰15åå®éªŒç»„:")
    print("æ’å  å®éªŒç»„  å­¦ä¹ ç‡      è¡°å‡ç‡    æŠ˜æ‰£å› å­  ç»¼åˆå¾—åˆ†    SNR(dB)   HVå‡å€¼   IGDå‡å€¼  æˆåŠŸç‡")
    print("-" * 105)
    for i in range(min(15, len(df_sorted))):
        row = df_sorted.iloc[i]
        success_rate = row['Successful_Runs'] / 10 * 100
        print(f"{i+1:2d}    {row['Exp_ID']:2d}      {row['A_LearningRate']:.6f}  {row['B_EpsilonDecay']:.4f}   {row['D_Gamma']:.2f}     {row['Comprehensive_Mean']:.4f}     {row['SNR_Value']:6.2f}   {row['HV_Mean']:.4f}  {row['IGD_Mean']:5.1f}   {success_rate:3.0f}%")
    
    print("\nå10åå®éªŒç»„:")
    print("æ’å  å®éªŒç»„  å­¦ä¹ ç‡      è¡°å‡ç‡    æŠ˜æ‰£å› å­  ç»¼åˆå¾—åˆ†    SNR(dB)   HVå‡å€¼   IGDå‡å€¼  æˆåŠŸç‡")
    print("-" * 105)
    for i in range(max(0, len(df_sorted)-10), len(df_sorted)):
        row = df_sorted.iloc[i]
        success_rate = row['Successful_Runs'] / 10 * 100
        print(f"{i+1:2d}    {row['Exp_ID']:2d}      {row['A_LearningRate']:.6f}  {row['B_EpsilonDecay']:.4f}   {row['D_Gamma']:.2f}     {row['Comprehensive_Mean']:.4f}     {row['SNR_Value']:6.2f}   {row['HV_Mean']:.4f}  {row['IGD_Mean']:5.1f}   {success_rate:3.0f}%")
    
    # 7. å‚æ•°åˆ†å¸ƒåˆ†æ
    print("\nğŸ“ˆ 7. å‚æ•°åˆ†å¸ƒåˆ†æ:")
    
    # å­¦ä¹ ç‡åˆ†å¸ƒ
    lr_groups = valid_df.groupby('A_LearningRate')['SNR_Value'].agg(['mean', 'std', 'count'])
    print(f"\nå­¦ä¹ ç‡æ€§èƒ½åˆ†æ:")
    print("å­¦ä¹ ç‡        å¹³å‡SNR    æ ‡å‡†å·®    æ ·æœ¬æ•°")
    print("-" * 45)
    for lr, stats in lr_groups.iterrows():
        std_val = stats['std'] if not pd.isna(stats['std']) else 0.0
        print(f"{lr:.6f}     {stats['mean']:7.3f}   {std_val:6.3f}    {stats['count']:3.0f}")
    
    # æ¢ç´¢ç‡è¡°å‡åˆ†å¸ƒ
    decay_groups = valid_df.groupby('B_EpsilonDecay')['SNR_Value'].agg(['mean', 'std', 'count'])
    print(f"\næ¢ç´¢ç‡è¡°å‡æ€§èƒ½åˆ†æ:")
    print("è¡°å‡ç‡     å¹³å‡SNR    æ ‡å‡†å·®    æ ·æœ¬æ•°")
    print("-" * 40)
    for decay, stats in decay_groups.iterrows():
        std_val = stats['std'] if not pd.isna(stats['std']) else 0.0
        print(f"{decay:.4f}    {stats['mean']:7.3f}   {std_val:6.3f}    {stats['count']:3.0f}")
    
    # æŠ˜æ‰£å› å­åˆ†å¸ƒ
    gamma_groups = valid_df.groupby('D_Gamma')['SNR_Value'].agg(['mean', 'std', 'count'])
    print(f"\næŠ˜æ‰£å› å­æ€§èƒ½åˆ†æ:")
    print("æŠ˜æ‰£å› å­   å¹³å‡SNR    æ ‡å‡†å·®    æ ·æœ¬æ•°")
    print("-" * 37)
    for gamma, stats in gamma_groups.iterrows():
        std_val = stats['std'] if not pd.isna(stats['std']) else 0.0
        print(f"{gamma:.2f}      {stats['mean']:7.3f}   {std_val:6.3f}    {stats['count']:3.0f}")
    
    # 8. æ•°æ®è´¨é‡æ£€æŸ¥
    print("\nğŸ” 8. æ•°æ®è´¨é‡æ£€æŸ¥:")
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    missing_data = df.isnull().sum()
    if missing_data.sum() > 0:
        print("âš ï¸ å‘ç°ç¼ºå¤±æ•°æ®:")
        for col, count in missing_data[missing_data > 0].items():
            print(f"  {col}: {count}ä¸ªç¼ºå¤±å€¼")
    else:
        print("âœ… æ— ç¼ºå¤±æ•°æ®")
    
    # æ£€æŸ¥æ— æ•ˆSNR
    invalid_snr = len(df[df['SNR_Value'] <= -100])
    if invalid_snr > 0:
        print(f"âš ï¸ å‘ç°{invalid_snr}ä¸ªæ— æ•ˆSNRå€¼ï¼ˆâ‰¤-100 dBï¼‰")
        invalid_exps = df[df['SNR_Value'] <= -100]['Exp_ID'].tolist()
        print(f"   æ— æ•ˆå®éªŒç»„: {invalid_exps}")
    else:
        print("âœ… æ‰€æœ‰SNRå€¼éƒ½æœ‰æ•ˆ")
    
    # 9. ä¿å­˜å®Œæ•´åˆ†æç»“æœ
    print("\nğŸ’¾ 9. ä¿å­˜åˆ†æç»“æœ:")
    
    # ä¿å­˜æ’åºåçš„å®Œæ•´æ•°æ®
    output_file = "ç”°å£L49å®éªŒæ·±åº¦åˆ†æç»“æœ_20250624.csv"
    df_sorted_with_rank = df_sorted.copy()
    df_sorted_with_rank['æ€§èƒ½æ’å'] = range(1, len(df_sorted_with_rank) + 1)
    df_sorted_with_rank.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"âœ… å®Œæ•´æ’åºæ•°æ®å·²ä¿å­˜: {output_file}")
    
    # ä¿å­˜å› å­æ•ˆåº”è¡¨
    factor_effects_data = []
    for factor, name in zip(factors, factor_names):
        effects = factor_effects[factor]
        for level in range(1, 8):
            if str(level) in effects:
                factor_effects_data.append({
                    'å› å­åç§°': name,
                    'å› å­ä»£ç ': factor,
                    'æ°´å¹³': level,
                    'SNRå‡å€¼': effects[str(level)],
                    'é‡è¦æ€§æ’å': effects['rank']
                })
    
    effects_df = pd.DataFrame(factor_effects_data)
    effects_file = "ç”°å£å› å­æ°´å¹³æ•ˆåº”è¡¨_20250624.csv"
    effects_df.to_csv(effects_file, index=False, encoding='utf-8-sig')
    print(f"âœ… å› å­æ•ˆåº”è¡¨å·²ä¿å­˜: {effects_file}")
    
    # 10. æœ€ç»ˆç»“è®º
    print("\nğŸ¯ 10. æœ€ç»ˆåˆ†æç»“è®º:")
    print(f"âœ… æœ€ä¼˜å®éªŒç»„: {best_snr_row['Exp_ID']}")
    print(f"âœ… æœ€ä¼˜SNR: {best_snr_row['SNR_Value']:.3f} dB") 
    print(f"âœ… æœ€ä¼˜å‚æ•°ç»„åˆ:")
    print(f"   å­¦ä¹ ç‡: {best_snr_row['A_LearningRate']}")
    print(f"   æ¢ç´¢ç‡è¡°å‡: {best_snr_row['B_EpsilonDecay']}")
    print(f"   æŠ˜æ‰£å› å­: {best_snr_row['D_Gamma']}")
    print(f"   é¹°ç¾¤åˆ†ç»„: {best_snr_row['C_GroupRatios']}")
    print(f"âœ… æ€§èƒ½è¡¨ç°:")
    print(f"   ç»¼åˆå¾—åˆ†: {best_snr_row['Comprehensive_Mean']:.4f} Â± {best_snr_row['Comprehensive_Std']:.4f}")
    print(f"   è¶…ä½“ç§¯(HV): {best_snr_row['HV_Mean']:.4f} Â± {best_snr_row['HV_Std']:.4f}")
    print(f"   åå‘ä¸–ä»£è·ç¦»(IGD): {best_snr_row['IGD_Mean']:.2f} Â± {best_snr_row['IGD_Std']:.2f}")
    print(f"   ä¸–ä»£è·ç¦»(GD): {best_snr_row['GD_Mean']:.2f} Â± {best_snr_row['GD_Std']:.2f}")
    print(f"âœ… æ€§èƒ½æ”¹è¿›: ç›¸æ¯”æœ€å·®é…ç½®æå‡ {snr_stats['max'] - snr_stats['min']:.3f} dB")
    print(f"âœ… å…³é”®å½±å“å› å­: {factor_importance[0][1]} (æ’åç¬¬1ï¼Œè´¡çŒ®åº¦{factor_importance[0][4]:.2f}%)")
    
    print("\n" + "=" * 80)
    print("ğŸ” æ·±åº¦åˆ†æå®Œæˆï¼æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ç”Ÿæˆã€‚")
    print("=" * 80)

if __name__ == "__main__":
    analyze_taguchi_results() 