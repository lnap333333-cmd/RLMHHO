# 5 RL 调度器决策机制与优化框架

> 本章深入解析 RL-Chaotic-HHO 体系中的 **DQN 调度器**（以下简称 RL 调度器），说明其在多目标分布式混合流水车间调度问题（MO-DHFSP）中的建模要素、学习算法、奖励设计及与四层鹰群协同的机理。全文采用严格的数学符号、公式编号与理论推导，确保与第 3 章“鹰群分组位置与能量更新策略”及第 4 章“哈里斯鹰能量建模”形成体系化闭环。

---

## 5.1 引言

在 RL-Chaotic-HHO 框架中，哈里斯鹰优化器承担 **连续空间** 的全局—局部搜索，而 **RL 调度器** 负责 **离散调度决策**。整体流程遵循“状态感知 → 策略输出 → 连续微调 → 反馈更新”的闭环：首先，RL 调度器依据系统状态 \(s_t\) 产生动作 \(a_t\) 并生成初始排程；随后，该排程被编码成向量交由 HHO 在连续空间精炼；HHO 的最优解再被解码回调度方案并刷新环境状态，为下一轮 RL 决策提供更优先验。通过这一离散—连续—离散循环，算法同时获得快速全局搜索与局部细化的优势。

---

## 5.2 MDP 建模

调度过程抽象为马尔可夫决策过程 \(\mathcal{M}=\langle \mathcal{S},\,\mathcal{A},\,P,\,R,\,\gamma \rangle\)。

### 5.2.1 状态空间

\[
\mathbf{s}_t = \big[\underbrace{\text{WIP}_{1:F}}_{\text{各工厂在制品}},\;\underbrace{\text{Load}_{1:M}}_{\text{机器负载}},\;\underbrace{\text{Due}\_{1:J}}_{\text{工件拖期}},\;\underbrace{\text{EG}\_{1:4}}_{\text{鹰群分组比例}},\;t/\text{T}_{\max}\big] \in \mathbb{R}^{14}.\tag{5.1}
\]

共 14 个连续特征，经线性归一化映射至 \([0,1]\)。

### 5.2.2 动作空间

根据前文修订，动作集合为

\[
\mathcal{A}=\{a^{(1)},\ldots,a^{(7)}\},\tag{5.2}
\]

其中 \(a^{(k)}\) 表示选择第 \(k\) 种 **调度启发式**（例如最短加工时间 SPT、最迟交期 LDD 等）或动态调整鹰群比例。动作 7 专门用于“自适应平衡”——触发第 3 章中平衡组 50 % 概率替换机制。

---

## 5.3 深度 Q 网络结构

采用三层前馈网络

\[
14 \;\xrightarrow{\;w_1\;} 128 \;\xrightarrow{\;w_2\;} 64 \;\xrightarrow{\;w_3\;} 7,\tag{5.3}
\]

激活函数为 **ReLU**，输出层为线性。网络参数 \(\theta=\{w_1,w_2,w_3\}\)。

### 5.3.1 经验回放与目标网络

引入经验池 \(\mathcal{D}\)（容量 \(10^5\)），并采用参数冻结的目标网络 \(Q_{\theta^-}\)，每 \(C=200\) 步同步一次：

\[
\theta^- \leftarrow \theta.\tag{5.4}
\]

---

## 5.4 奖励函数设计

双目标：完工时间 \(C_{\max}\) 与总拖期 \(T_{\text{tot}}\)。即刻奖励

\[
R_t = -\Big[\lambda_1\,\frac{C_{\max}^{(t)}-C_{\min}}{C_{\max}^{\max}-C_{\min}} + \lambda_2\,\frac{T_{\text{tot}}^{(t)}-T_{\min}}{T_{\max}-T_{\min}}\Big],\tag{5.5}
\]

其中 \(\lambda_1=\lambda_2=0.5\) 默认同权，亦可通过 Taguchi 设计优化。

---

## 5.5 学习规则与 ε-贪心策略

### 5.5.1 Q 值迭代

每步采用 **Huber 损失**

\[
\mathcal{L}(\theta)=\mathbb{E}_{(s,a,r,s')}\bigg[ \varphi\big( y - Q_{\theta}(s,a) \big) \bigg],\tag{5.6}
\]

其中目标值

\[
y = r + \gamma\,\max_{a'} Q_{\theta^-}(s',a'),\quad \gamma=0.99.\tag{5.7}
\]

### 5.5.2 ε 衰减

\[
\varepsilon_t = \max(\varepsilon_{\min},\; \varepsilon_0\,e^{-\kappa t}),\quad \varepsilon_0=1.0,\;\varepsilon_{\min}=0.05,\;\kappa=7\times10^{-4}.\tag{5.8}
\]

该指数衰减保证前期充分探索，后期偏重利用，与第 3 章“平衡组”机制保持一致的“**逐步收敛**”理念。

---

## 5.6 与四层鹰群的协同机制

合作逻辑包含三层耦合：其一，动作 \(a^{(7)}\) 直接触发第 3 章提出的“平衡组 50 % 替换”机制，其余六个动作则通过重新分布能量 \(E\) 以塑形“探索 ↔ 开发”倾向；其二，四层鹰群搜索得到的局部最优 \(\mathbf{x}_{\text{HHO}}\) 被即时解码为新排程，其性能指标作为当前回合奖励 \(R_t\) 回馈给 RL；其三，当精英组引入梯度跟随时，若对应调度策略在经验池中表现出高 TD-error，系统自动提高其采样权重，构成隐式的 *prioritized replay*。三层联动确保离散策略、连续搜索与数据驱动更新之间形成闭环强化。

---

## 5.7 复杂度分析

就增量成本而论，每回合 RL 更新的时间复杂度为 \(\mathcal{O}(B\,|\theta|)\)，批量大小 \(B=64\)。由于 HHO 已输出经过优化的排程向量，状态维度保持固定，故该开销相对整体搜索可忽略；空间上，经验池 \(\mathcal{D}\) 与网络参数合计约占 \(10^6\) 级浮点数，亦远小于记录 HHO 种群所需存储。

---

## 5.8 收敛性与可行性讨论

根据 **定理 5.1**：若 \(\gamma<1\)、学习率 \(\alpha_t\) 满足 Robbins–Monro 条件，且所有状态-动作对被无限次访问，则 Q-learning 收敛于最优 Q 值。  
本研究通过 ε-贪心保证遍历性、经验回放削弱样本相关性、目标网络降低估计偏移，因此满足定理先决条件。

---

## 5.9 实验设计建议

建议开展三类实验以验证所述机制：(1) **消融实验**：分别关闭 RL 或 HHO，以量化协同增益；(2) **超参敏感性**：围绕 \(\lambda_1/\lambda_2\) 及 ε 衰减率 \(\kappa\) 进行 Taguchi L-49 设计，分析双目标折衷曲线；(3) **可解释性分析**：统计不同时段动作分布与鹰群能量曲线的 Pearson 相关系数，以揭示策略—能量映射的因果结构。

---

## 5.10 结论

本章提出的 RL 调度器通过 **DQN-驱动的自适应启发式选择** 与 **HHO 连续搜索** 形成互补机制，实现了对 MO-DHFSP 双目标的高效优化。与此同时，引入的动作 7（平衡替换）与第 3 章的 50 % 触发规则保持一致，保证算法各子模块在理论与实践层面的耦合与一致性。 