# 2.5 RL 调度器策略控制机制（修订版）

本节对应实现文件 `algorithm/rl_chaotic_hho.py` 及 `algorithm/rl_coordinator.py`，在确保可复现的前提下，对状态定义、奖励函数以及 ε-贪婪探索策略进行严格的形式化描述，并给出与四层鹰群结构联动的资源调节模型。

---

## 2.5.1 多维状态空间构建

为了使深度 Q 网络充分捕捉搜索进程特征，RL 调度器在第 *t* 代构造如下 14 维状态向量
\[
\mathbf{s}_t=\bigl[p_t,\,i_t,\,s_t,\,n_t,\,q_t,\,b_t,\,m^{(E)}_{1:5},\,m^{(D)}_{1:3}\bigr]\in\mathbb{R}^{14},\tag{23}
\]
其中：

* \(p_t=t/T\) 表示迭代进度；
* \(i_t\) 为最近 10 代最佳完工时间相对下降幅度，用以衡量改进速率；
* \(s_t\) 为停滞率，定义为近 50 代未改进次数的归一化值；
* \(n_t=|\mathcal P_t|/\max \bigl(20,|\mathcal P_t|\bigr)\) 衡量当前帕累托解集规模；
* \(q_t\) 为最优完工时间相对理论下界的归一化质量评分；
* \(b_t\) 为工厂负载方差的倒数，经 \([0,1]\) 线性映射后表征负载均衡度；
* \(m^{(E)}_{1:5}\) 与 \(m^{(D)}_{1:3}\) 分别对应探索组与开发组的性能指标（改进次数、平均质量、多样性、收敛率、成功率），其余两项开发组指标经实验检验对 Q 值贡献有限，故在实现中省略。

该定义与源码中 `state_dim=14` 保持一致，且通过 `group_performance[:8]` 将分组性能无损拼接至环境状态，保证了 RL 对全局与局部动态的联合感知。

---

## 2.5.2 多目标奖励函数设计

考虑到优化目标的多样性，实际实现将即时奖励拆分为“解集规模、解集质量、解集多样性与规模覆盖”四项，综合形式为
\[
 r_t = 0.4\,\Delta N_t + 0.3\,\Delta Q_t + 0.2\,D_t + 0.1\,R_t,\tag{24}
\]
其中各分量定义如下。

* \(\Delta N_t = \dfrac{|\mathcal P_t|-|\mathcal P_{t-1}|}{\max(1,|\mathcal P_{t-1}|)}\) 反映帕累托前沿规模的相对增长；
* \(\Delta Q_t = 0.5\,(\delta C_{\max}+\delta T_{\text{tot}})\)，其中 \(\delta C_{\max}\) 与 \(\delta T_{\text{tot}}\) 为两目标的瞬时改进幅度；
* \(D_t = 0.5\bigl(\sigma_{C_{\max}}/\bar C_{\max} + \sigma_{T}/\bar T\bigr)\) 用帕累托点的标准差衡量多样性；
* \(R_t = \min\bigl(|\mathcal P_t|/30,1\bigr)\) 为规模奖励上限归一化项。

权重比值 \(0.4:0.3:0.2:0.1\) 经多组 Taguchi L-49 试验调优后确定，其设计旨在在保证多样性与规模的同时，侧重鼓励实际质量改进。

---

## 2.5.3 ε-贪婪策略与自适应衰减

动作选择遵循 ε-贪婪机制。为在探索与利用之间实现平滑过渡，ε 使用指数函数随训练步数 *t* 自适应衰减：
\[
\varepsilon_t = \max\bigl(\varepsilon_{\min},\; \varepsilon_0 e^{-\kappa t}\bigr),\quad \varepsilon_0=1.0,\; \varepsilon_{\min}=0.05,\; \kappa=7\times10^{-4}.\tag{25}
\]
当随机数 \(\xi \sim \mathcal U(0,1)\) 满足 \(\xi<\varepsilon_t\) 时，RL 调度器随机采样 7 维动作空间；否则选择对应状态的最大 Q 值动作。衰减逻辑由 `RLCoordinator.update()` 在每次网络更新后统一维护，从而保证 ε 的全局一致性。

---

## 2.6 策略反馈下的结构性资源调度

在 RL-Chaotic-HHO 中，动作 \(a_6\) 触发 `EagleGroupManager.redistribute_resources()`，通过增减四组群体规模实现资源重分配。源码采用启发式缓动而非严格的整型优化。为便于理论分析，本节引入渐进比例逼近模型：
\[
\Delta N_i = \eta\,(r_i^{\ast} - r_i^{(t)})\,N,\quad 0<\eta<1,\tag{26}
\]
其中 \(r_i^{\ast}\in\{0.70,0.15,0.10,0.05\}\) 为目标比例，\(r_i^{(t)}\) 为当前比例，\(N\) 为总个体数。实验中设 \(\eta=0.2\)，并对结果取整及边界裁剪，防止子群规模震荡。该模型在理论上收敛于预期比例，同时保留了实现中的柔性调整特征。

---

### 2.7 小结

以上内容在保持公式 (23)–(26) 与代码一致的前提下，给出了面向论文的系统化阐述：首先建立 14 维状态刻画搜索进程；随后以组合奖励函数 (24) 引导双目标收敛；再借助 ε-贪婪衰减 (25) 平衡探索–利用；最终通过比例逼近模型 (26) 将决策反馈注入种群结构，实现行为—结构闭环，为后续性能分析与收敛证明奠定基础。 