import pandas as pd
import numpy as np
import json
import os
from pathlib import Path

def analyze_taguchi_results():
    """å®Œæ•´åˆ†æç”°å£å®éªŒç»“æœï¼Œæ‰¾å‡ºæ•°æ®ä¸ä¸€è‡´çš„åŸå› """
    
    print("=" * 80)
    print("ğŸ” ç”°å£L49å®éªŒæ·±åº¦æ•°æ®åˆ†æ - 2025-06-24")
    print("=" * 80)
    
    # 1. æ•°æ®æ–‡ä»¶æ£€æŸ¥
    print("\nğŸ“‚ 1. æ•°æ®æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥:")
    result_dir = Path("taguchi_results_20250624_172744")
    
    if not result_dir.exists():
        print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {result_dir}")
        return
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    excel_file = result_dir / "l49_results_summary.xlsx"
    json_file = result_dir / "taguchi_analysis.json"
    
    print(f"ğŸ“Š Excelæ±‡æ€»æ–‡ä»¶: {'âœ…' if excel_file.exists() else 'âŒ'} {excel_file}")
    print(f"ğŸ“ˆ ç”°å£åˆ†ææ–‡ä»¶: {'âœ…' if json_file.exists() else 'âŒ'} {json_file}")
    
    if not excel_file.exists() or not json_file.exists():
        print("âŒ å…³é”®æ•°æ®æ–‡ä»¶ç¼ºå¤±ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
        return
    
    # 2. åŠ è½½å’ŒéªŒè¯æ•°æ®
    print("\nğŸ“‹ 2. æ•°æ®åŠ è½½å’ŒéªŒè¯:")
    try:
        # åŠ è½½Excelæ•°æ®
        df = pd.read_excel(excel_file)
        print(f"âœ… Excelæ•°æ®åŠ è½½æˆåŠŸ: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
        
        # åŠ è½½ç”°å£åˆ†ææ•°æ®
        with open(json_file, 'r', encoding='utf-8') as f:
            analysis_data = json.load(f)
        print(f"âœ… ç”°å£åˆ†ææ•°æ®åŠ è½½æˆåŠŸ")
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        required_columns = [
            'Exp_ID', 'A_LearningRate', 'B_EpsilonDecay', 'C_GroupRatios', 'D_Gamma',
            'HV_Mean', 'IGD_Mean', 'GD_Mean', 'Comprehensive_Mean', 'SNR_Value', 'Successful_Runs'
        ]
        
        missing_cols = [col for col in required_columns if col not in df.columns]
        if missing_cols:
            print(f"âš ï¸ ç¼ºå¤±å…³é”®åˆ—: {missing_cols}")
        else:
            print("âœ… æ‰€æœ‰å…³é”®åˆ—éƒ½å­˜åœ¨")
            
        print(f"ğŸ“Š åˆ—åè¯¦æƒ…: {list(df.columns)}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # 3. åŸºç¡€ç»Ÿè®¡åˆ†æ
    print("\nğŸ“ˆ 3. åŸºç¡€ç»Ÿè®¡åˆ†æ:")
    print(f"å®éªŒç»„æ€»æ•°: {len(df)}")
    print(f"æ¯ç»„é‡å¤æ¬¡æ•°: 10æ¬¡")
    print(f"æ€»å®éªŒæ¬¡æ•°: {len(df) * 10}")
    print(f"å®éªŒæˆåŠŸç‡: {(df['Successful_Runs'].sum() / (len(df) * 10) * 100):.1f}%")
    
    # SNRç»Ÿè®¡
    snr_stats = df['SNR_Value'].describe()
    print(f"\nSNRç»Ÿè®¡åˆ†æ:")
    print(f"  æœ€å¤§å€¼: {snr_stats['max']:.3f} dB")
    print(f"  æœ€å°å€¼: {snr_stats['min']:.3f} dB")
    print(f"  å¹³å‡å€¼: {snr_stats['mean']:.3f} dB")
    print(f"  æ ‡å‡†å·®: {snr_stats['std']:.3f} dB")
    print(f"  ä¸­ä½æ•°: {snr_stats['50%']:.3f} dB")
    print(f"  æ€§èƒ½è·¨åº¦: {snr_stats['max'] - snr_stats['min']:.3f} dB")
    
    # 4. å¯»æ‰¾æœ€ä¼˜ç»“æœï¼ˆå¤šç§æ–¹æ³•å¯¹æ¯”ï¼‰
    print("\nğŸ¯ 4. æœ€ä¼˜ç»“æœè¯†åˆ«ï¼ˆå¤šæ–¹æ³•å¯¹æ¯”ï¼‰:")
    
    # æ–¹æ³•1ï¼šæŒ‰SNRæ’åº
    best_snr_idx = df['SNR_Value'].idxmax()
    best_snr_row = df.loc[best_snr_idx]
    print(f"\næ–¹æ³•1 - æŒ‰SNRæœ€å¤§å€¼:")
    print(f"  å®éªŒç»„: {best_snr_row['Exp_ID']}")
    print(f"  SNRå€¼: {best_snr_row['SNR_Value']:.3f} dB")
    print(f"  å­¦ä¹ ç‡: {best_snr_row['A_LearningRate']}")
    print(f"  æ¢ç´¢ç‡è¡°å‡: {best_snr_row['B_EpsilonDecay']}")
    print(f"  æŠ˜æ‰£å› å­: {best_snr_row['D_Gamma']}")
    print(f"  ç»¼åˆå¾—åˆ†: {best_snr_row['Comprehensive_Mean']:.4f}")
    
    # æ–¹æ³•2ï¼šæŒ‰ç»¼åˆå¾—åˆ†æ’åº
    best_comp_idx = df['Comprehensive_Mean'].idxmax()
    best_comp_row = df.loc[best_comp_idx]
    print(f"\næ–¹æ³•2 - æŒ‰ç»¼åˆå¾—åˆ†æœ€å¤§å€¼:")
    print(f"  å®éªŒç»„: {best_comp_row['Exp_ID']}")
    print(f"  SNRå€¼: {best_comp_row['SNR_Value']:.3f} dB")
    print(f"  ç»¼åˆå¾—åˆ†: {best_comp_row['Comprehensive_Mean']:.4f}")
    
    # æ–¹æ³•3ï¼šç”°å£é¢„æµ‹æœ€ä¼˜
    optimal_combo = analysis_data['optimal_combination']
    predicted_snr = analysis_data['predicted_snr']
    print(f"\næ–¹æ³•3 - ç”°å£æ–¹æ³•é¢„æµ‹æœ€ä¼˜:")
    print(f"  é¢„æµ‹ç»„åˆ: A={optimal_combo['A']}, B={optimal_combo['B']}, C={optimal_combo['C']}, D={optimal_combo['D']}")
    print(f"  é¢„æµ‹SNR: {predicted_snr:.3f} dB")
    
    # æ£€æŸ¥ä¸€è‡´æ€§
    if best_snr_idx == best_comp_idx:
        print(f"\nâœ… SNRå’Œç»¼åˆå¾—åˆ†æŒ‡å‘åŒä¸€æœ€ä¼˜è§£: å®éªŒç»„{best_snr_row['Exp_ID']}")
    else:
        print(f"\nâš ï¸ SNRå’Œç»¼åˆå¾—åˆ†æŒ‡å‘ä¸åŒæœ€ä¼˜è§£:")
        print(f"   SNRæœ€ä¼˜: å®éªŒç»„{best_snr_row['Exp_ID']} (SNR={best_snr_row['SNR_Value']:.3f})")
        print(f"   ç»¼åˆå¾—åˆ†æœ€ä¼˜: å®éªŒç»„{best_comp_row['Exp_ID']} (å¾—åˆ†={best_comp_row['Comprehensive_Mean']:.4f})")
    
    # 5. ç”°å£æ–¹æ³•å› å­æ•ˆåº”åˆ†æ
    print("\nğŸ”¬ 5. ç”°å£æ–¹æ³•å› å­æ•ˆåº”åˆ†æ:")
    factors = ['A', 'B', 'C', 'D']
    factor_names = ['å­¦ä¹ ç‡', 'æ¢ç´¢ç‡è¡°å‡', 'é¹°ç¾¤åˆ†ç»„æ¯”ä¾‹', 'æŠ˜æ‰£å› å­']
    
    print("å› å­é‡è¦æ€§æ’åº:")
    factor_effects = analysis_data['factor_effects']
    factor_importance = []
    
    for factor, name in zip(factors, factor_names):
        effects = factor_effects[factor]
        rank = effects['rank']
        range_val = effects['range']
        # è·å–ANOVAç»“æœ
        anova = analysis_data['anova_results'][factor]
        contribution = anova['contribution']
        f_value = anova['f_value']
        
        factor_importance.append((rank, name, factor, range_val, contribution, f_value))
        
        print(f"\n{name} (å› å­{factor}):")
        print(f"  é‡è¦æ€§æ’å: {rank}")
        print(f"  æ•ˆåº”æå·®: {range_val:.3f}")
        print(f"  è´¡çŒ®åº¦: {contribution:.2f}%")
        print(f"  Få€¼: {f_value:.3f}")
        
        # æ˜¾ç¤ºå„æ°´å¹³æ•ˆåº”
        print(f"  å„æ°´å¹³SNRæ•ˆåº”:")
        for level in range(1, 8):
            if str(level) in effects:
                print(f"    æ°´å¹³{level}: {effects[str(level)]:.3f} dB")
    
    # 6. æ’åºåˆ†æ
    print("\nğŸ“Š 6. å®éªŒç»„æ€§èƒ½æ’åºåˆ†æ:")
    df_sorted = df.sort_values('SNR_Value', ascending=False).reset_index(drop=True)
    
    print("å‰10åå®éªŒç»„:")
    print("æ’å  å®éªŒç»„  å­¦ä¹ ç‡      è¡°å‡ç‡    æŠ˜æ‰£å› å­  ç»¼åˆå¾—åˆ†    SNR(dB)   æˆåŠŸç‡")
    print("-" * 85)
    for i in range(min(10, len(df_sorted))):
        row = df_sorted.iloc[i]
        success_rate = row['Successful_Runs'] / 10 * 100
        print(f"{i+1:2d}    {row['Exp_ID']:2d}      {row['A_LearningRate']:.6f}  {row['B_EpsilonDecay']:.4f}   {row['D_Gamma']:.2f}     {row['Comprehensive_Mean']:.4f}     {row['SNR_Value']:6.2f}   {success_rate:3.0f}%")
    
    print("\nå10åå®éªŒç»„:")
    print("æ’å  å®éªŒç»„  å­¦ä¹ ç‡      è¡°å‡ç‡    æŠ˜æ‰£å› å­  ç»¼åˆå¾—åˆ†    SNR(dB)   æˆåŠŸç‡")
    print("-" * 85)
    for i in range(max(0, len(df_sorted)-10), len(df_sorted)):
        row = df_sorted.iloc[i]
        success_rate = row['Successful_Runs'] / 10 * 100
        print(f"{i+1:2d}    {row['Exp_ID']:2d}      {row['A_LearningRate']:.6f}  {row['B_EpsilonDecay']:.4f}   {row['D_Gamma']:.2f}     {row['Comprehensive_Mean']:.4f}     {row['SNR_Value']:6.2f}   {success_rate:3.0f}%")
    
    # 7. å‚æ•°åˆ†å¸ƒåˆ†æ
    print("\nğŸ“ˆ 7. å‚æ•°åˆ†å¸ƒåˆ†æ:")
    
    # å­¦ä¹ ç‡åˆ†å¸ƒ
    lr_groups = df.groupby('A_LearningRate')['SNR_Value'].agg(['mean', 'std', 'count'])
    print(f"\nå­¦ä¹ ç‡æ€§èƒ½åˆ†æ:")
    print("å­¦ä¹ ç‡        å¹³å‡SNR    æ ‡å‡†å·®    æ ·æœ¬æ•°")
    print("-" * 45)
    for lr, stats in lr_groups.iterrows():
        print(f"{lr:.6f}     {stats['mean']:7.3f}   {stats['std']:6.3f}    {stats['count']:3.0f}")
    
    # æ¢ç´¢ç‡è¡°å‡åˆ†å¸ƒ
    decay_groups = df.groupby('B_EpsilonDecay')['SNR_Value'].agg(['mean', 'std', 'count'])
    print(f"\næ¢ç´¢ç‡è¡°å‡æ€§èƒ½åˆ†æ:")
    print("è¡°å‡ç‡     å¹³å‡SNR    æ ‡å‡†å·®    æ ·æœ¬æ•°")
    print("-" * 40)
    for decay, stats in decay_groups.iterrows():
        print(f"{decay:.4f}    {stats['mean']:7.3f}   {stats['std']:6.3f}    {stats['count']:3.0f}")
    
    # æŠ˜æ‰£å› å­åˆ†å¸ƒ
    gamma_groups = df.groupby('D_Gamma')['SNR_Value'].agg(['mean', 'std', 'count'])
    print(f"\næŠ˜æ‰£å› å­æ€§èƒ½åˆ†æ:")
    print("æŠ˜æ‰£å› å­   å¹³å‡SNR    æ ‡å‡†å·®    æ ·æœ¬æ•°")
    print("-" * 37)
    for gamma, stats in gamma_groups.iterrows():
        print(f"{gamma:.2f}      {stats['mean']:7.3f}   {stats['std']:6.3f}    {stats['count']:3.0f}")
    
    # 8. æ•°æ®è´¨é‡æ£€æŸ¥
    print("\nğŸ” 8. æ•°æ®è´¨é‡æ£€æŸ¥:")
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    missing_data = df.isnull().sum()
    if missing_data.sum() > 0:
        print("âš ï¸ å‘ç°ç¼ºå¤±æ•°æ®:")
        for col, count in missing_data[missing_data > 0].items():
            print(f"  {col}: {count}ä¸ªç¼ºå¤±å€¼")
    else:
        print("âœ… æ— ç¼ºå¤±æ•°æ®")
    
    # æ£€æŸ¥å¼‚å¸¸å€¼
    print("\nå¼‚å¸¸å€¼æ£€æŸ¥:")
    for col in ['Comprehensive_Mean', 'SNR_Value', 'HV_Mean', 'IGD_Mean', 'GD_Mean']:
        if col in df.columns:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
            if len(outliers) > 0:
                print(f"  {col}: {len(outliers)}ä¸ªå¼‚å¸¸å€¼")
                for idx in outliers.index:
                    print(f"    å®éªŒç»„{df.loc[idx, 'Exp_ID']}: {df.loc[idx, col]:.4f}")
            else:
                print(f"  {col}: æ— å¼‚å¸¸å€¼")
    
    # 9. ä¿å­˜å®Œæ•´åˆ†æç»“æœ
    print("\nğŸ’¾ 9. ä¿å­˜åˆ†æç»“æœ:")
    
    # ä¿å­˜æ’åºåçš„å®Œæ•´æ•°æ®
    output_file = "ç”°å£L49å®éªŒæ·±åº¦åˆ†æç»“æœ_20250624.csv"
    df_sorted_with_rank = df_sorted.copy()
    df_sorted_with_rank['æ€§èƒ½æ’å'] = range(1, len(df_sorted_with_rank) + 1)
    df_sorted_with_rank.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"âœ… å®Œæ•´æ’åºæ•°æ®å·²ä¿å­˜: {output_file}")
    
    # ä¿å­˜å› å­æ•ˆåº”è¡¨
    factor_effects_data = []
    for factor, name in zip(factors, factor_names):
        effects = factor_effects[factor]
        for level in range(1, 8):
            if str(level) in effects:
                factor_effects_data.append({
                    'å› å­åç§°': name,
                    'å› å­ä»£ç ': factor,
                    'æ°´å¹³': level,
                    'SNRå‡å€¼': effects[str(level)],
                    'é‡è¦æ€§æ’å': effects['rank']
                })
    
    effects_df = pd.DataFrame(factor_effects_data)
    effects_file = "ç”°å£å› å­æ°´å¹³æ•ˆåº”è¡¨_20250624.csv"
    effects_df.to_csv(effects_file, index=False, encoding='utf-8-sig')
    print(f"âœ… å› å­æ•ˆåº”è¡¨å·²ä¿å­˜: {effects_file}")
    
    # 10. æœ€ç»ˆç»“è®º
    print("\nğŸ¯ 10. æœ€ç»ˆåˆ†æç»“è®º:")
    print(f"âœ… æœ€ä¼˜å®éªŒç»„: {best_snr_row['Exp_ID']}")
    print(f"âœ… æœ€ä¼˜SNR: {best_snr_row['SNR_Value']:.3f} dB")
    print(f"âœ… æœ€ä¼˜å‚æ•°ç»„åˆ:")
    print(f"   å­¦ä¹ ç‡: {best_snr_row['A_LearningRate']}")
    print(f"   æ¢ç´¢ç‡è¡°å‡: {best_snr_row['B_EpsilonDecay']}")
    print(f"   æŠ˜æ‰£å› å­: {best_snr_row['D_Gamma']}")
    print(f"   é¹°ç¾¤åˆ†ç»„: {best_snr_row['C_GroupRatios']}")
    print(f"âœ… æ€§èƒ½æ”¹è¿›: ç›¸æ¯”æœ€å·®é…ç½®æå‡ {snr_stats['max'] - snr_stats['min']:.3f} dB")
    print(f"âœ… å…³é”®å½±å“å› å­: {factor_names[0]} (è´¡çŒ®åº¦æœ€é«˜)")
    
    print("\n" + "=" * 80)
    print("ğŸ” æ·±åº¦åˆ†æå®Œæˆï¼æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ç”Ÿæˆã€‚")
    print("=" * 80)

if __name__ == "__main__":
    analyze_taguchi_results() 