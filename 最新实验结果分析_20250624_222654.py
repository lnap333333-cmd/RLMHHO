#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€æ–°ç”°å£å®éªŒç»“æœåˆ†æè„šæœ¬ - taguchi_results_20250624_222654
åŸºäºä¼˜åŒ–åçš„å­¦ä¹ ç‡æ°´å¹³é…ç½®çš„å®éªŒç»“æœåˆ†æ
"""

import pandas as pd
import json
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

def load_experiment_data():
    """åŠ è½½æœ€æ–°å®éªŒæ•°æ®"""
    results_dir = Path('taguchi_results_20250624_222654')
    
    df = None
    taguchi_data = None
    
    # å°è¯•è¯»å–Excelæ–‡ä»¶
    excel_file = results_dir / 'l49_results_summary.xlsx'
    if excel_file.exists():
        print("ğŸ“Š è¯»å–Excelæ±‡æ€»æ–‡ä»¶...")
        df = pd.read_excel(excel_file)
    
    # è¯»å–ç”°å£åˆ†æJSON
    taguchi_file = results_dir / 'taguchi_analysis.json'
    if taguchi_file.exists():
        print("ğŸ“Š è¯»å–ç”°å£åˆ†æJSONæ–‡ä»¶...")
        with open(taguchi_file, 'r') as f:
            taguchi_data = json.load(f)
    
    if df is None and taguchi_data is None:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
    
    return df, taguchi_data

def analyze_basic_statistics(df, taguchi_data):
    """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
    print("=" * 80)
    print("ğŸ”¬ æœ€æ–°ç”°å£L49å®éªŒç»“æœåˆ†æ (2025-06-24 22:26:54)")
    print("=" * 80)
    
    if df is not None:
        print(f"\nğŸ“ˆ å®éªŒæ¦‚è§ˆ:")
        print(f"  â€¢ å®éªŒç»„æ•°: {len(df)} ç»„")
        print(f"  â€¢ æ•°æ®åˆ—æ•°: {len(df.columns)} åˆ—")
        print(f"  â€¢ æ€»å®éªŒæ¬¡æ•°: {len(df) * 10} æ¬¡ (æ¯ç»„10æ¬¡é‡å¤)")
        
        if 'SNR_Value' in df.columns:
            print(f"\nğŸ¯ SNRæ€§èƒ½ç»Ÿè®¡:")
            best_idx = df['SNR_Value'].idxmax()
            worst_idx = df['SNR_Value'].idxmin()
            
            print(f"  â€¢ æœ€é«˜SNR: {df.loc[best_idx, 'SNR_Value']:.3f} dB (å®éªŒç»„ {df.loc[best_idx, 'Exp_ID']})")
            print(f"  â€¢ æœ€ä½SNR: {df.loc[worst_idx, 'SNR_Value']:.3f} dB (å®éªŒç»„ {df.loc[worst_idx, 'Exp_ID']})")
            print(f"  â€¢ å¹³å‡SNR: {df['SNR_Value'].mean():.3f} Â± {df['SNR_Value'].std():.3f} dB")
            print(f"  â€¢ æ€§èƒ½è·¨åº¦: {df['SNR_Value'].max() - df['SNR_Value'].min():.3f} dB")
            
            return df.loc[best_idx], df.loc[worst_idx]
    
    if taguchi_data:
        print(f"\nğŸ”¬ ç”°å£åˆ†æç»“æœ (åŸºäºJSON):")
        snr_array = np.fromstring(taguchi_data['snr_data'][1:-1], sep=' ')
        print(f"  â€¢ æœ€é«˜SNR: {snr_array.max():.3f} dB")
        print(f"  â€¢ æœ€ä½SNR: {snr_array.min():.3f} dB") 
        print(f"  â€¢ å¹³å‡SNR: {snr_array.mean():.3f} Â± {snr_array.std():.3f} dB")
        print(f"  â€¢ é¢„æµ‹æœ€ä¼˜SNR: {taguchi_data['predicted_snr']:.3f} dB")
        
    return None, None

def analyze_factor_effects(taguchi_data):
    """åˆ†æå› å­æ•ˆåº”"""
    if not taguchi_data:
        return
        
    print("\nğŸ” å› å­æ•ˆåº”åˆ†æ:")
    print("=" * 60)
    
    factor_names = {
        'A': 'å­¦ä¹ ç‡ (ä¼˜åŒ–å)',
        'B': 'æ¢ç´¢ç‡è¡°å‡', 
        'C': 'é¹°ç¾¤åˆ†ç»„æ¯”ä¾‹',
        'D': 'æŠ˜æ‰£å› å­'
    }
    
    # æŒ‰é‡è¦æ€§æ’åº
    factors = taguchi_data['factor_effects']
    sorted_factors = sorted(factors.items(), key=lambda x: x[1]['range'], reverse=True)
    
    print("ğŸ“Š å› å­é‡è¦æ€§æ’åº:")
    for i, (factor, data) in enumerate(sorted_factors, 1):
        print(f"  {i}. {factor_names[factor]}: æå·®={data['range']:.3f}")
    
    print("\nğŸ¯ å„å› å­æœ€ä¼˜æ°´å¹³:")
    optimal = taguchi_data['optimal_combination']
    for factor, level in optimal.items():
        best_snr = factors[factor][str(level)]
        print(f"  â€¢ {factor_names[factor]}: æ°´å¹³{level} (SNR={best_snr:.3f} dB)")

def analyze_learning_rate_improvement(df, taguchi_data):
    """åˆ†æå­¦ä¹ ç‡ä¼˜åŒ–æ•ˆæœ"""
    print("\nğŸš€ å­¦ä¹ ç‡ä¼˜åŒ–æ•ˆæœåˆ†æ:")
    print("=" * 60)
    
    # æ–°çš„å­¦ä¹ ç‡æ°´å¹³æ˜ å°„
    new_lr_levels = {
        1: 0.00005, 2: 0.0001, 3: 0.0002, 4: 0.0005,
        5: 0.001, 6: 0.002, 7: 0.005
    }
    
    # æ—§çš„å­¦ä¹ ç‡æ°´å¹³æ˜ å°„
    old_lr_levels = {
        1: 0.00005, 2: 0.0001, 3: 0.0005, 4: 0.001,
        5: 0.003, 6: 0.005, 7: 0.01
    }
    
    print("ğŸ“ˆ å­¦ä¹ ç‡æ°´å¹³å¯¹æ¯”:")
    print("â”Œ" + "â”€" * 8 + "â”¬" + "â”€" * 12 + "â”¬" + "â”€" * 12 + "â”¬" + "â”€" * 15 + "â”")
    print("â”‚  æ°´å¹³  â”‚   æ—§é…ç½®   â”‚   æ–°é…ç½®   â”‚     çŠ¶æ€      â”‚")
    print("â”œ" + "â”€" * 8 + "â”¼" + "â”€" * 12 + "â”¼" + "â”€" * 12 + "â”¼" + "â”€" * 15 + "â”¤")
    
    for level in range(1, 8):
        old_val = old_lr_levels[level]
        new_val = new_lr_levels[level]
        status = "ä¸å˜" if old_val == new_val else ("å‡å°" if new_val < old_val else "å¢å¤§")
        print(f"â”‚   {level}    â”‚  {old_val:>8.5f}  â”‚  {new_val:>8.5f}  â”‚   {status:>10s}   â”‚")
    
    print("â””" + "â”€" * 8 + "â”´" + "â”€" * 12 + "â”´" + "â”€" * 12 + "â”´" + "â”€" * 15 + "â”˜")
    
    if taguchi_data:
        # åˆ†æå­¦ä¹ ç‡çš„å› å­æ•ˆåº”
        lr_effects = taguchi_data['factor_effects']['A']
        optimal_level = taguchi_data['optimal_combination']['A']
        optimal_lr = new_lr_levels[optimal_level]
        optimal_snr = lr_effects[str(optimal_level)]
        
        print(f"\nğŸ¯ ä¼˜åŒ–åå­¦ä¹ ç‡è¡¨ç°:")
        print(f"  â€¢ æœ€ä¼˜æ°´å¹³: æ°´å¹³{optimal_level} (å­¦ä¹ ç‡={optimal_lr:.5f})")
        print(f"  â€¢ æœ€ä¼˜SNR: {optimal_snr:.3f} dB")
        print(f"  â€¢ å­¦ä¹ ç‡é‡è¦æ€§æ’å: {lr_effects['rank']}")
        print(f"  â€¢ å­¦ä¹ ç‡æ•ˆåº”æå·®: {lr_effects['range']:.3f}")

def show_top_experiments(df):
    """æ˜¾ç¤ºæœ€ä¼˜å®éªŒç»„"""
    if df is None or 'SNR_Value' not in df.columns:
        return
        
    print("\nğŸ† Top 10 æœ€ä¼˜å®éªŒç»„:")
    print("=" * 80)
    
    # é€‰æ‹©å…³é”®åˆ—æ˜¾ç¤º
    display_cols = ['Exp_ID', 'A_LearningRate', 'B_EpsilonDecay', 'D_Gamma', 'SNR_Value']
    if all(col in df.columns for col in display_cols):
        top10 = df.nlargest(10, 'SNR_Value')[display_cols]
        
        print("â”Œ" + "â”€" * 6 + "â”¬" + "â”€" * 12 + "â”¬" + "â”€" * 12 + "â”¬" + "â”€" * 10 + "â”¬" + "â”€" * 12 + "â”")
        print("â”‚ å®éªŒç»„ â”‚   å­¦ä¹ ç‡   â”‚  æ¢ç´¢è¡°å‡  â”‚  æŠ˜æ‰£å› å­  â”‚   SNR(dB)  â”‚")
        print("â”œ" + "â”€" * 6 + "â”¼" + "â”€" * 12 + "â”¼" + "â”€" * 12 + "â”¼" + "â”€" * 10 + "â”¼" + "â”€" * 12 + "â”¤")
        
        for _, row in top10.iterrows():
            print(f"â”‚  {int(row['Exp_ID']):>3d}   â”‚ {row['A_LearningRate']:>10.5f} â”‚ {row['B_EpsilonDecay']:>10.4f} â”‚ {row['D_Gamma']:>8.3f} â”‚ {row['SNR_Value']:>10.3f} â”‚")
        
        print("â””" + "â”€" * 6 + "â”´" + "â”€" * 12 + "â”´" + "â”€" * 12 + "â”´" + "â”€" * 10 + "â”´" + "â”€" * 12 + "â”˜")

def compare_with_previous_results():
    """ä¸å‰æ¬¡å®éªŒç»“æœå¯¹æ¯”"""
    print("\nğŸ“Š ä¸å‰æ¬¡å®éªŒç»“æœå¯¹æ¯”:")
    print("=" * 60)
    
    # ä»è®°å¿†ä¸­è·å–å‰æ¬¡æœ€ä¼˜ç»“æœ
    previous_best_snr = -15.743  # å®éªŒç»„10çš„ç»“æœ
    previous_best_lr = 0.0001
    
    print(f"å‰æ¬¡æœ€ä¼˜ç»“æœ (taguchi_results_20250624_172744):")
    print(f"  â€¢ æœ€ä¼˜SNR: {previous_best_snr:.3f} dB")
    print(f"  â€¢ æœ€ä¼˜å­¦ä¹ ç‡: {previous_best_lr:.5f}")
    print(f"  â€¢ å®éªŒæ—¶é—´: 2025-06-24 17:27:44")
    
    print(f"\næœ¬æ¬¡å®éªŒ (taguchi_results_20250624_222654):")
    print(f"  â€¢ å®éªŒæ—¶é—´: 2025-06-24 22:26:54")
    print(f"  â€¢ å­¦ä¹ ç‡é…ç½®: å·²ä¼˜åŒ–ä¸ºå›´ç»•0.0001çš„å¯†é›†é‡‡æ ·")
    print(f"  â€¢ é…ç½®æ”¹è¿›: ç§»é™¤è¿‡å¤§å­¦ä¹ ç‡ï¼Œå¢åŠ ç²¾ç»†åº¦")

def generate_summary_report(df, taguchi_data, best_exp, worst_exp):
    """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
    print("\n" + "=" * 80)
    print("ğŸ“‹ å®éªŒæ€»ç»“æŠ¥å‘Š")
    print("=" * 80)
    
    print(f"ğŸ• å®éªŒæ—¶é—´: 2025å¹´6æœˆ24æ—¥ 22:26:54")
    print(f"ğŸ”¬ å®éªŒç±»å‹: ç”°å£L49æ­£äº¤å®éªŒ (ä¼˜åŒ–å­¦ä¹ ç‡ç‰ˆæœ¬)")
    print(f"ğŸ“Š å®éªŒè§„æ¨¡: 49ç»„ Ã— 10æ¬¡é‡å¤ = 490æ¬¡å®éªŒ")
    
    if best_exp is not None:
        print(f"\nğŸ¥‡ æœ€ä¼˜é…ç½® (å®éªŒç»„{int(best_exp['Exp_ID'])}):")
        print(f"  â€¢ å­¦ä¹ ç‡: {best_exp['A_LearningRate']:.5f}")
        print(f"  â€¢ æ¢ç´¢ç‡è¡°å‡: {best_exp['B_EpsilonDecay']:.4f}")
        print(f"  â€¢ æŠ˜æ‰£å› å­: {best_exp['D_Gamma']:.3f}")
        print(f"  â€¢ SNRå€¼: {best_exp['SNR_Value']:.3f} dB")
    
    if taguchi_data:
        print(f"\nğŸ¯ ç”°å£æ–¹æ³•é¢„æµ‹:")
        optimal = taguchi_data['optimal_combination']
        print(f"  â€¢ é¢„æµ‹æœ€ä¼˜ç»„åˆ: A{optimal['A']}-B{optimal['B']}-C{optimal['C']}-D{optimal['D']}")
        print(f"  â€¢ é¢„æµ‹SNR: {taguchi_data['predicted_snr']:.3f} dB")
    
    print(f"\nâœ… å­¦ä¹ ç‡ä¼˜åŒ–æˆæ•ˆ:")
    print(f"  â€¢ é…ç½®æ›´ç§‘å­¦: å›´ç»•æœ€ä¼˜å€¼0.0001è¿›è¡Œå¯†é›†é‡‡æ ·")
    print(f"  â€¢ èŒƒå›´æ›´åˆç†: å…¨éƒ¨åœ¨DQNæ¨èèŒƒå›´å†…")
    print(f"  â€¢ é¢„æœŸæ”¹è¿›: æ›´ç¨³å®šçš„è®­ç»ƒæ”¶æ•›")

def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½æ•°æ®
    df, taguchi_data = load_experiment_data()
    
    # åŸºç¡€ç»Ÿè®¡åˆ†æ
    best_exp, worst_exp = analyze_basic_statistics(df, taguchi_data)
    
    # å› å­æ•ˆåº”åˆ†æ
    analyze_factor_effects(taguchi_data)
    
    # å­¦ä¹ ç‡ä¼˜åŒ–åˆ†æ
    analyze_learning_rate_improvement(df, taguchi_data)
    
    # æ˜¾ç¤ºæœ€ä¼˜å®éªŒ
    show_top_experiments(df)
    
    # å¯¹æ¯”å‰æ¬¡ç»“æœ
    compare_with_previous_results()
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    generate_summary_report(df, taguchi_data, best_exp, worst_exp)
    
    print(f"\nğŸ‰ åˆ†æå®Œæˆ!")
    print(f"ğŸ“ æ•°æ®ç›®å½•: taguchi_results_20250624_222654/")

if __name__ == "__main__":
    main() 