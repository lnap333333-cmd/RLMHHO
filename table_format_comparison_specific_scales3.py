#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‰¹å®šè§„æ¨¡ç®—æ³•å¯¹æ¯”å®éªŒç¨‹åº - è‡ªå®šä¹‰ç‰ˆæœ¬
è§£å†³é—®é¢˜ï¼š
1. DQN paretoè§£é›†æ•°é‡é—®é¢˜
2. å½’ä¸€åŒ–æŒ‡æ ‡è®¡ç®—é—®é¢˜  
3. ä¸»ä½“ç®—æ³•paretoè§£é›†å¤šæ ·æ€§
4. Excelè¡¨æ ¼åˆ†ç¦»è¾“å‡º
5. æ”¯æŒè‡ªå®šä¹‰ä¸‰ä¸ªè§„æ¨¡é…ç½®

æ–°å¢åŠŸèƒ½ï¼š
- æ”¯æŒè‡ªå®šä¹‰ä¸‰ä¸ªè§„æ¨¡çš„é…ç½®
- æä¾›é»˜è®¤è§„æ¨¡é…ç½®ä½œä¸ºå‚è€ƒ
- å¢åŠ äº†é…ç½®æŒ‡å—å’Œç¤ºä¾‹
- æ›´å‹å¥½çš„ç”¨æˆ·ç•Œé¢

ä½¿ç”¨æ–¹æ³•ï¼š
1. ç›´æ¥è¿è¡Œç¨‹åºå°†ä½¿ç”¨é»˜è®¤çš„ä¸‰ä¸ªè§„æ¨¡é…ç½®
2. ä¿®æ”¹ä¸»å‡½æ•°ä¸­çš„ custom_scales å˜é‡æ¥è‡ªå®šä¹‰è§„æ¨¡
3. æ¯ä¸ªè§„æ¨¡é…ç½®åŒ…å«ï¼šn_jobsï¼ˆå·¥ä»¶æ•°ï¼‰ã€n_stagesï¼ˆé˜¶æ®µæ•°ï¼‰ã€n_factoriesï¼ˆå·¥å‚æ•°ï¼‰ã€nameï¼ˆè§„æ¨¡åç§°ï¼‰

ç¤ºä¾‹é…ç½®ï¼š
custom_scales = [
    {'n_jobs': 30, 'n_stages': 3, 'n_factories': 2, 'name': 'å°è§„æ¨¡'},
    {'n_jobs': 60, 'n_stages': 4, 'n_factories': 3, 'name': 'ä¸­è§„æ¨¡'},
    {'n_jobs': 100, 'n_stages': 5, 'n_factories': 4, 'name': 'å¤§è§„æ¨¡'}
]
"""

import os
import time
import traceback
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from typing import Dict, List, Tuple
import pandas as pd

from problem.mo_dhfsp import MO_DHFSP_Problem
from algorithm.rl_chaotic_hho import RL_ChaoticHHO_Optimizer
from algorithm.improved_nsga2 import ImprovedNSGA2_Optimizer
from algorithm.mopso import MOPSO_Optimizer
from algorithm.mode import MODE_Optimizer
from algorithm.dqn_algorithm_wrapper import DQNAlgorithmWrapper
from algorithm.ql_abc_fixed import QLABC_Optimizer_Fixed
from algorithm.ql_abc_enhanced import QLABC_Optimizer_Enhanced
from utils.data_generator import DataGenerator

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

# å®éªŒé…ç½®ï¼šè®¾ç½®ä¸ºTrueä½¿ç”¨å®Œæ•´10ä¸ªè§„æ¨¡ï¼ŒFalseä½¿ç”¨3ä¸ªæµ‹è¯•è§„æ¨¡
USE_FULL_SCALES = False  # å½“å‰è®¾ç½®ä¸ºæµ‹è¯•æ¨¡å¼ï¼ˆ2ä¸ªè§„æ¨¡ï¼‰

def calculate_hypervolume(pareto_solutions: List, reference_point: Tuple[float, float] = None, normalize: bool = True, all_algorithm_solutions: List = None) -> float:
    """
    æ­£ç¡®çš„è¶…ä½“ç§¯æŒ‡æ ‡è®¡ç®—
    ä½¿ç”¨æ ‡å‡†2Dè¶…ä½“ç§¯ç®—æ³•ï¼Œé»˜è®¤è¿›è¡Œå½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´
    
    Args:
        pareto_solutions: å¸•ç´¯æ‰˜è§£é›†
        reference_point: å‚è€ƒç‚¹ï¼Œå¦‚æœä¸ºNoneåˆ™åŸºäºæ‰€æœ‰ç®—æ³•è§£é›†è®¡ç®—
        normalize: æ˜¯å¦å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´ï¼Œé»˜è®¤ä¸ºTrue
        all_algorithm_solutions: æ‰€æœ‰ç®—æ³•çš„è§£é›†ï¼Œç”¨äºè®¡ç®—ç»Ÿä¸€å‚è€ƒç‚¹
    
    Returns:
        å½’ä¸€åŒ–çš„è¶…ä½“ç§¯å€¼ï¼ŒèŒƒå›´[0,1]ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºæ€§èƒ½è¶Šå¥½
    """
    if not pareto_solutions or len(pareto_solutions) == 0:
        return 0.0
    
    # æå–ç›®æ ‡å€¼
    objectives = [(sol.makespan, sol.total_tardiness) for sol in pareto_solutions]
    
    # å»é™¤é‡å¤è§£
    unique_objectives = []
    tolerance = 1e-6
    for obj in objectives:
        is_duplicate = False
        for unique_obj in unique_objectives:
            if abs(obj[0] - unique_obj[0]) < tolerance and abs(obj[1] - unique_obj[1]) < tolerance:
                is_duplicate = True
                break
        if not is_duplicate:
            unique_objectives.append(obj)
    
    if len(unique_objectives) == 0:
        return 0.0
    
    # ä¸¥æ ¼è®¡ç®—å¸•ç´¯æ‰˜å‰æ²¿
    pareto_front = []
    for i, obj in enumerate(unique_objectives):
        is_dominated = False
        for j, other_obj in enumerate(unique_objectives):
            if i != j:
                # æ£€æŸ¥æ˜¯å¦è¢«ä¸¥æ ¼æ”¯é…ï¼ˆå¯¹äºæœ€å°åŒ–é—®é¢˜ï¼‰
                if (other_obj[0] <= obj[0] and other_obj[1] <= obj[1] and 
                    (other_obj[0] < obj[0] or other_obj[1] < obj[1])):
                    is_dominated = True
                    break
        if not is_dominated:
            pareto_front.append(obj)
    
    if len(pareto_front) == 0:
        return 0.0
    
    # å•ç‚¹è§£å¤„ç†
    if len(pareto_front) == 1:
        return 0.1
    
    # è®¾ç½®å‚è€ƒç‚¹ - ä½¿ç”¨æ›´åˆç†çš„å‚è€ƒç‚¹é¿å…HV=1é—®é¢˜
    if reference_point is None:
        if all_algorithm_solutions and len(all_algorithm_solutions) > 0:
            # åŸºäºæ‰€æœ‰ç®—æ³•çš„è§£é›†è®¡ç®—å‚è€ƒç‚¹
            all_objectives = [(sol.makespan, sol.total_tardiness) for sol in all_algorithm_solutions]
            max_makespan = max(obj[0] for obj in all_objectives)
            max_tardiness = max(obj[1] for obj in all_objectives)
            # ä½¿ç”¨æ›´ä¿å®ˆçš„å€æ•°ï¼Œé¿å…å‚è€ƒç‚¹è¿‡äºæ¥è¿‘æœ€ä¼˜è§£
            reference_point = (max_makespan * 1.5, max_tardiness * 1.5)
        else:
            # å¦‚æœæ²¡æœ‰æ‰€æœ‰ç®—æ³•è§£é›†ï¼ŒåŸºäºå½“å‰è§£é›†è®¾ç½®å‚è€ƒç‚¹
            max_makespan = max(obj[0] for obj in pareto_front)
            max_tardiness = max(obj[1] for obj in pareto_front)
            reference_point = (max_makespan * 1.5, max_tardiness * 1.5)
    
    # æ ‡å‡†2Dè¶…ä½“ç§¯è®¡ç®—ï¼šæŒ‰ç¬¬ä¸€ä¸ªç›®æ ‡æ’åºï¼Œä»å³å‘å·¦æ‰«æ
    sorted_front = sorted(pareto_front, key=lambda x: x[0])
    
    hypervolume = 0.0
    prev_x = reference_point[0]
    
    # ä»å³åˆ°å·¦è®¡ç®—æ¯ä¸ªç‚¹çš„è´¡çŒ®
    for x, y in reversed(sorted_front):
        if x < reference_point[0] and y < reference_point[1]:
            width = prev_x - x
            height = reference_point[1] - y
            if width > 0 and height > 0:
                hypervolume += width * height
                prev_x = x
    
    # å½’ä¸€åŒ–HVå€¼åˆ°[0, 1]èŒƒå›´
    max_possible_hv = reference_point[0] * reference_point[1]
    if max_possible_hv > 0:
        normalized_hv = hypervolume / max_possible_hv
        # é™åˆ¶åœ¨[0, 1]èŒƒå›´å†…ï¼Œé¿å…æ•°å€¼è¿‡å¤§
        return min(max(normalized_hv, 0.0), 1.0)
    else:
        return 0.0

def calculate_igd(normalized_pareto_solutions: List, reference_front: List[Tuple[float, float]] = None) -> float:
    """
    åå‘ä¸–ä»£è·ç¦» - åŸºäºå½’ä¸€åŒ–åçš„ç›®æ ‡å€¼è®¡ç®—
    ä¿®æ­£ç‰ˆæœ¬ï¼šä½¿ç”¨æ ‡å‡†æ¬§æ°è·ç¦»ï¼Œé¿å…IGD+åœ¨ç®—æ³•è§£å®Œå…¨æ”¯é…å‚è€ƒå‰æ²¿æ—¶è¿”å›0çš„é—®é¢˜
    """
    if not normalized_pareto_solutions or len(normalized_pareto_solutions) == 0:
        return float('inf')
    
    # ä½¿ç”¨å½’ä¸€åŒ–åçš„ç›®æ ‡å€¼
    objectives = [(sol.makespan, sol.total_tardiness) for sol in normalized_pareto_solutions]
    
    # å¦‚æœæ²¡æœ‰å‚è€ƒå‰æ²¿ï¼Œè¿”å›æ— ç©·å¤§
    if reference_front is None or len(reference_front) == 0:
        return float('inf')
    
    # è®¡ç®—æ¯ä¸ªå‚è€ƒç‚¹åˆ°è§£é›†çš„æœ€å°æ¬§æ°è·ç¦»
    distances = []
    for ref_point in reference_front:
        min_distance = float('inf')
        
        for obj in objectives:
            # ä½¿ç”¨æ ‡å‡†æ¬§æ°è·ç¦»è€ŒéIGD+ä¿®æ­£è·ç¦»
            diff_makespan = obj[0] - ref_point[0]
            diff_tardiness = obj[1] - ref_point[1]
            distance = np.sqrt(diff_makespan**2 + diff_tardiness**2)
            min_distance = min(min_distance, distance)
        
        distances.append(min_distance)
    
    # è¿”å›å¹³å‡è·ç¦»ï¼Œä¸è®¾ç½®äººå·¥é˜ˆå€¼
    avg_distance = np.mean(distances)
    return avg_distance

def calculate_gd(normalized_pareto_solutions: List, reference_front: List[Tuple[float, float]] = None) -> float:
    """
    ä¸–ä»£è·ç¦» - åŸºäºå½’ä¸€åŒ–åçš„ç›®æ ‡å€¼è®¡ç®—
    ä¿®æ­£ç‰ˆæœ¬ï¼šä½¿ç”¨æ ‡å‡†æ¬§æ°è·ç¦»ï¼Œä¸IGDä¿æŒä¸€è‡´
    """
    if not normalized_pareto_solutions or len(normalized_pareto_solutions) == 0:
        return float('inf')
    
    # ä½¿ç”¨å½’ä¸€åŒ–åçš„ç›®æ ‡å€¼
    objectives = [(sol.makespan, sol.total_tardiness) for sol in normalized_pareto_solutions]
    
    # å¦‚æœæ²¡æœ‰å‚è€ƒå‰æ²¿ï¼Œè¿”å›æ— ç©·å¤§
    if reference_front is None or len(reference_front) == 0:
        return float('inf')
    
    # è®¡ç®—æ¯ä¸ªè§£åˆ°å‚è€ƒå‰æ²¿çš„æœ€å°æ¬§æ°è·ç¦»
    distances = []
    for obj in objectives:
        min_distance = float('inf')
        
        for ref_point in reference_front:
            # ä½¿ç”¨æ ‡å‡†æ¬§æ°è·ç¦»
            diff_makespan = obj[0] - ref_point[0]
            diff_tardiness = obj[1] - ref_point[1]
            distance = np.sqrt(diff_makespan**2 + diff_tardiness**2)
            min_distance = min(min_distance, distance)
        
        distances.append(min_distance)
    
    # è¿”å›å¹³å‡è·ç¦»ï¼Œä¸è®¾ç½®äººå·¥é˜ˆå€¼
    avg_distance = np.mean(distances)
    return avg_distance

def calculate_maximum_spread(normalized_pareto_solutions: List) -> float:
    """
    æœ€å¤§åˆ†å¸ƒæ€§æŒ‡æ ‡(Maximum Spread, MS) - åŸºäºç›®æ ‡ç©ºé—´æœ€å¤§è¦†ç›–èŒƒå›´
    è¯„ä¼°Paretoè§£é›†åœ¨ç›®æ ‡ç©ºé—´ä¸­åˆ†å¸ƒçš„æœ€å¤§è¦†ç›–èŒƒå›´
    MSå€¼è¶Šå¤§è¡¨ç¤ºè¦†ç›–èŒƒå›´è¶Šå¹¿æ³›ï¼Œè§£é›†åˆ†å¸ƒè¶Šå¥½
    å…¬å¼: MS = Î£(max(um) - min(um)) for m=1 to M
    """
    if not normalized_pareto_solutions or len(normalized_pareto_solutions) <= 2:
        return 0.0  # å°‘äº3ä¸ªè§£æ—¶ï¼Œåˆ†å¸ƒæ€§å·®

    # ä½¿ç”¨å½’ä¸€åŒ–åçš„ç›®æ ‡å€¼
    objectives = [(sol.makespan, sol.total_tardiness) for sol in normalized_pareto_solutions]

    # å»é™¤é‡å¤è§£
    unique_objectives = []
    tolerance = 1e-6
    for obj in objectives:
        is_duplicate = False
        for unique_obj in unique_objectives:
            if abs(obj[0] - unique_obj[0]) < tolerance and abs(obj[1] - unique_obj[1]) < tolerance:
                is_duplicate = True
                break
        if not is_duplicate:
            unique_objectives.append(obj)

    if len(unique_objectives) <= 2:
        return 0.0

    # æŒ‰ç¬¬ä¸€ä¸ªç›®æ ‡æ’åº
    sorted_objectives = sorted(unique_objectives, key=lambda x: x[0])

    # è®¡ç®—ç›¸é‚»è§£ä¹‹é—´çš„è·ç¦»
    distances = []
    for i in range(len(sorted_objectives) - 1):
        dist = np.sqrt((sorted_objectives[i+1][0] - sorted_objectives[i][0])**2 + 
                      (sorted_objectives[i+1][1] - sorted_objectives[i][1])**2)
        distances.append(dist)

    if not distances:
        return 0.0

    # è®¡ç®—å¹³å‡è·ç¦»
    mean_distance = np.mean(distances)

    if mean_distance == 0:
        return 0.0

    # Maximum Spread (MS) è®¡ç®— - åŸºäºç›®æ ‡ç©ºé—´æœ€å¤§è¦†ç›–èŒƒå›´
    # å…¬å¼: MS = Î£(max(um) - min(um)) for m=1 to M
    
    # è®¡ç®—æ¯ä¸ªç›®æ ‡ç»´åº¦çš„æœ€å¤§å€¼å’Œæœ€å°å€¼
    f1_values = [obj[0] for obj in sorted_objectives]  # makespanç»´åº¦
    f2_values = [obj[1] for obj in sorted_objectives]  # total_tardinessç»´åº¦
    
    f1_max = max(f1_values)
    f1_min = min(f1_values)
    f2_max = max(f2_values)
    f2_min = min(f2_values)
    
    # è®¡ç®—æ¯ä¸ªç»´åº¦çš„è·¨åº¦
    f1_range = f1_max - f1_min
    f2_range = f2_max - f2_min
    
    # å¦‚æœæŸä¸ªç»´åº¦æ²¡æœ‰å˜åŒ–ï¼Œä½¿ç”¨è¯¥ç»´åº¦æœ€å¤§å€¼çš„5%ä½œä¸ºé»˜è®¤èŒƒå›´
    if f1_range == 0:
        f1_range = f1_max * 0.05
    if f2_range == 0:
        f2_range = f2_max * 0.05
    
    # Maximum Spread = å„ç»´åº¦è·¨åº¦çš„æ€»å’Œ
    ms = f1_range + f2_range
    
    # å½’ä¸€åŒ–åˆ°[0, 1]èŒƒå›´
    # å¯¹äºå½’ä¸€åŒ–çš„ç›®æ ‡å€¼ï¼Œæœ€å¤§å¯èƒ½çš„MSå€¼ä¸º2.0ï¼ˆä¸¤ä¸ªç»´åº¦éƒ½æ˜¯1.0ï¼‰
    max_possible_ms = 2.0
    normalized_ms = min(ms / max_possible_ms, 1.0)
    
    return normalized_ms

def calculate_ra(algorithm_solutions: List, reference_pareto_front: List) -> float:
    """
    RAæŒ‡æ ‡ - å¸•ç´¯æ‰˜æœ€ä¼˜è§£çš„æ¯”ç‡ (Ratio of Pareto-optimal solutions)
    ä¿®æ­£ç‰ˆæœ¬ï¼šRA = |A âˆ© P| / |P|
    å…¶ä¸­ A æ˜¯ç®—æ³•è§£é›†ï¼ŒP æ˜¯å‚è€ƒå¸•ç´¯æ‰˜å‰æ²¿
    è¿™æ ·è®¡ç®—ç¡®ä¿æ‰€æœ‰ç®—æ³•çš„RAæ€»å’Œä¸º1
    
    Args:
        algorithm_solutions: Solutionå¯¹è±¡åˆ—è¡¨æˆ–tupleåˆ—è¡¨
        reference_pareto_front: Solutionå¯¹è±¡åˆ—è¡¨æˆ–tupleåˆ—è¡¨
    """
    if not algorithm_solutions or not reference_pareto_front:
        return 0.0
    
    # æå–ç®—æ³•è§£é›†çš„ç›®æ ‡å€¼ï¼ˆæ”¯æŒSolutionå¯¹è±¡å’Œtupleä¸¤ç§æ ¼å¼ï¼‰
    if hasattr(algorithm_solutions[0], 'makespan'):
        # Solutionå¯¹è±¡æ ¼å¼
        alg_objectives = [(sol.makespan, sol.total_tardiness) for sol in algorithm_solutions]
    else:
        # tupleæ ¼å¼
        alg_objectives = list(algorithm_solutions)
    
    # æå–å‚è€ƒå¸•ç´¯æ‰˜å‰æ²¿çš„ç›®æ ‡å€¼ï¼ˆæ”¯æŒSolutionå¯¹è±¡å’Œtupleä¸¤ç§æ ¼å¼ï¼‰
    if hasattr(reference_pareto_front[0], 'makespan'):
        # Solutionå¯¹è±¡æ ¼å¼
        ref_objectives = [(sol.makespan, sol.total_tardiness) for sol in reference_pareto_front]
    else:
        # tupleæ ¼å¼
        ref_objectives = list(reference_pareto_front)
    
    # ç»Ÿè®¡å‚è€ƒå‰æ²¿ä¸­æœ‰å¤šå°‘ä¸ªè¢«ç®—æ³•æ‰¾åˆ°
    intersection_count = 0
    tolerance = 1e-6  # é€‚ä¸­çš„å®¹å¿åº¦
    
    for ref_obj in ref_objectives:
        for alg_obj in alg_objectives:
            # æ£€æŸ¥å‚è€ƒå‰æ²¿ä¸­çš„è§£æ˜¯å¦è¢«ç®—æ³•æ‰¾åˆ°ï¼ˆåœ¨å®¹å¿åº¦èŒƒå›´å†…ï¼‰
            if (abs(ref_obj[0] - alg_obj[0]) < tolerance and 
                abs(ref_obj[1] - alg_obj[1]) < tolerance):
                intersection_count += 1
                break  # æ‰¾åˆ°åŒ¹é…å°±è·³å‡ºå†…å±‚å¾ªç¯
    
    # è®¡ç®—RAæŒ‡æ ‡ï¼šå‚è€ƒå‰æ²¿ä¸­è¢«ç®—æ³•æ‰¾åˆ°çš„æ¯”ä¾‹
    ra = intersection_count / len(ref_objectives) if len(ref_objectives) > 0 else 0.0
    
    return ra

def normalize_objectives(all_results: Dict) -> Dict:
    """
    å½’ä¸€åŒ–æ‰€æœ‰ç®—æ³•çš„ç›®æ ‡å€¼ï¼Œé¿å…ä¸åŒé‡çº²å½±å“
    è¿”å›å½’ä¸€åŒ–åçš„ç»“æœå’Œå½’ä¸€åŒ–å‚æ•°
    """
    # æ”¶é›†æ‰€æœ‰ç›®æ ‡å€¼
    all_makespans = []
    all_tardiness = []
    
    for result in all_results.values():
        if 'pareto_solutions' in result and result['pareto_solutions']:
            for sol in result['pareto_solutions']:
                all_makespans.append(sol.makespan)
                all_tardiness.append(sol.total_tardiness)
    
    if not all_makespans:
        return all_results, (0, 1, 0, 1)
    
    # è®¡ç®—å½’ä¸€åŒ–å‚æ•°
    min_makespan = min(all_makespans)
    max_makespan = max(all_makespans)
    min_tardiness = min(all_tardiness)
    max_tardiness = max(all_tardiness)
    
    # é¿å…é™¤é›¶
    makespan_range = max_makespan - min_makespan if max_makespan > min_makespan else 1.0
    tardiness_range = max_tardiness - min_tardiness if max_tardiness > min_tardiness else 1.0
    
    # å½’ä¸€åŒ–æ‰€æœ‰è§£
    normalized_results = {}
    for alg_name, result in all_results.items():
        normalized_results[alg_name] = result.copy()
        
        if 'pareto_solutions' in result and result['pareto_solutions']:
            normalized_solutions = []
            for sol in result['pareto_solutions']:
                # åˆ›å»ºå½’ä¸€åŒ–è§£çš„å‰¯æœ¬
                norm_sol = type('Solution', (), {})()
                norm_sol.makespan = (sol.makespan - min_makespan) / makespan_range
                norm_sol.total_tardiness = (sol.total_tardiness - min_tardiness) / tardiness_range
                # ä¿ç•™åŸå§‹å€¼ç”¨äºå…¶ä»–ç”¨é€”
                norm_sol.original_makespan = sol.makespan
                norm_sol.original_tardiness = sol.total_tardiness
                normalized_solutions.append(norm_sol)
            
            normalized_results[alg_name]['normalized_pareto_solutions'] = normalized_solutions
    
    normalization_params = (min_makespan, max_makespan, min_tardiness, max_tardiness)
    return normalized_results, normalization_params

def calculate_combined_pareto_front(normalized_results: Dict) -> List[Tuple[float, float]]:
    """
    åŸºäºå½’ä¸€åŒ–åçš„ç›®æ ‡å€¼è®¡ç®—ç»„åˆå¸•ç´¯æ‰˜å‰æ²¿
    ç”¨ä½œIGDå’ŒGDçš„çœŸå®å‚è€ƒå‰æ²¿PF*
    """
    all_objectives = []
    
    # æ”¶é›†æ‰€æœ‰ç®—æ³•å½’ä¸€åŒ–åçš„ç›®æ ‡å€¼
    for algorithm_name, result in normalized_results.items():
        if 'normalized_pareto_solutions' in result and result['normalized_pareto_solutions']:
            for sol in result['normalized_pareto_solutions']:
                all_objectives.append((sol.makespan, sol.total_tardiness))
    
    if not all_objectives:
        return []
    
    # å»é™¤é‡å¤ç‚¹
    unique_objectives = []
    for obj in all_objectives:
        is_duplicate = False
        for unique_obj in unique_objectives:
            if abs(obj[0] - unique_obj[0]) < 1e-6 and abs(obj[1] - unique_obj[1]) < 1e-6:
                is_duplicate = True
                break
        if not is_duplicate:
            unique_objectives.append(obj)
    
    # è®¡ç®—å¸•ç´¯æ‰˜å‰æ²¿
    pareto_front = []
    for obj in unique_objectives:
        is_dominated = False
        for other_obj in unique_objectives:
            # æ£€æŸ¥æ˜¯å¦è¢«æ”¯é…ï¼ˆå¯¹äºæœ€å°åŒ–é—®é¢˜ï¼‰
            if (other_obj[0] <= obj[0] and other_obj[1] <= obj[1] and 
                (other_obj[0] < obj[0] or other_obj[1] < obj[1])):
                is_dominated = True
                break
        
        if not is_dominated:
            pareto_front.append(obj)
    
    return pareto_front

def normalize_metrics(all_results: Dict) -> Dict:
    """
    æŒ‰ç…§å­¦æœ¯æ ‡å‡†å½’ä¸€åŒ–æŒ‡æ ‡
    - HV: è¶Šå¤§è¶Šå¥½ï¼Œå½’ä¸€åŒ–ä¸º0-1ï¼Œ1è¡¨ç¤ºæœ€å¥½
    - IGDã€GD: è¶Šå°è¶Šå¥½ï¼ŒæŒ‰ç…§å­¦æœ¯æƒ¯ä¾‹ï¼Œ0è¡¨ç¤ºæœ€å¥½ï¼Œä½†éœ€è¦åˆç†èŒƒå›´æ˜¾ç¤º
    - MS: è¶Šå¤§è¶Šå¥½ï¼Œ1è¡¨ç¤ºæœ€å¥½çš„æœ€å¤§è¦†ç›–èŒƒå›´
    - RA: è¶Šå¤§è¶Šå¥½ï¼Œç†æƒ³èŒƒå›´0-1ï¼Œ1è¡¨ç¤ºæ‰¾åˆ°äº†æ‰€æœ‰çœŸå®å¸•ç´¯æ‰˜æœ€ä¼˜è§£
    """
    # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡å€¼
    all_hypervolume = []
    all_igd = []
    all_gd = []
    all_spread = []
    all_ra = []
    all_makespan = []
    all_tardiness = []
    
    for result in all_results.values():
        if result['hypervolume'] > 0:
            all_hypervolume.append(result['hypervolume'])
        
        # æ”¶é›†éæ— ç©·å€¼çš„IGDå’ŒGD
        if result['igd'] != float('inf') and not np.isnan(result['igd']) and result['igd'] >= 0:
                all_igd.append(result['igd'])
        if result['gd'] != float('inf') and not np.isnan(result['gd']) and result['gd'] >= 0:
                all_gd.append(result['gd'])
        if result['spread'] >= 0 and not np.isnan(result['spread']):
            all_spread.append(result['spread'])
        if result['ra'] >= 0 and not np.isnan(result['ra']):
                all_ra.append(result['ra'])
    
        if result['makespan_best'] > 0:
            all_makespan.append(result['makespan_best'])
        if result['tardiness_best'] >= 0:
            all_tardiness.append(result['tardiness_best'])
    
    # è®¡ç®—å½’ä¸€åŒ–å‚æ•°
    max_hv = max(all_hypervolume) if all_hypervolume else 1.0
    
    # å¯¹äºIGDå’ŒGDï¼Œæˆ‘ä»¬å¸Œæœ›æ˜¾ç¤ºå®ƒä»¬çš„ç›¸å¯¹ä¼˜åŠ£ï¼Œä½†ä¿æŒ"è¶Šå°è¶Šå¥½"çš„å«ä¹‰
    max_igd = max(all_igd) if all_igd else 1.0
    max_gd = max(all_gd) if all_gd else 1.0
    max_spread = max(all_spread) if all_spread else 1.0
    max_ra = max(all_ra) if all_ra else 1.0
    
    min_makespan = min(all_makespan) if all_makespan else 0.0
    max_makespan = max(all_makespan) if all_makespan else 1.0
    min_tardiness = min(all_tardiness) if all_tardiness else 0.0
    max_tardiness = max(all_tardiness) if all_tardiness else 1.0
    
    # å½’ä¸€åŒ–ç»“æœ
    normalized_results = {}
    for alg_name, result in all_results.items():
        normalized_results[alg_name] = result.copy()
        
        # è¶…ä½“ç§¯HV: è¶Šå¤§è¶Šå¥½ï¼Œå½’ä¸€åŒ–åˆ°0-1èŒƒå›´ï¼Œ1è¡¨ç¤ºæœ€å¥½
        normalized_results[alg_name]['norm_hypervolume'] = result['hypervolume'] / max_hv if max_hv > 0 else 0.0
        
        # IGD: è¶Šå°è¶Šå¥½ï¼Œä¿æŒåŸå§‹å€¼æ˜¾ç¤ºï¼Œä½†æ ‡è®°ä¸ºè§„èŒƒåŒ–åçš„å€¼
        if result['igd'] == float('inf') or np.isnan(result['igd']):
            normalized_results[alg_name]['norm_igd'] = max_igd * 2  # ç»™å¤±è´¥ç®—æ³•ä¸€ä¸ªå¾ˆå¤§çš„å€¼
        else:
            normalized_results[alg_name]['norm_igd'] = result['igd']
        
        # GD: è¶Šå°è¶Šå¥½ï¼Œä¿æŒåŸå§‹å€¼æ˜¾ç¤º
        if result['gd'] == float('inf') or np.isnan(result['gd']):
            normalized_results[alg_name]['norm_gd'] = max_gd * 2
        else:
            normalized_results[alg_name]['norm_gd'] = result['gd']
        
        # MS: è¶Šå¤§è¶Šå¥½ï¼Œä¿æŒåŸå§‹å€¼æ˜¾ç¤º
        if np.isnan(result['spread']):
            normalized_results[alg_name]['norm_spread'] = max_spread * 2
        else:
            normalized_results[alg_name]['norm_spread'] = result['spread']
        
        # RA: è¶Šå¤§è¶Šå¥½ï¼Œä¿æŒåŸå§‹å€¼æ˜¾ç¤º
        if np.isnan(result['ra']):
            normalized_results[alg_name]['norm_ra'] = 0.0  # ç»™å¤±è´¥ç®—æ³•ä¸€ä¸ªæœ€å°å€¼
        else:
            normalized_results[alg_name]['norm_ra'] = result['ra']
            
        # ç›®æ ‡å€¼å½’ä¸€åŒ– (è¶Šå°è¶Šå¥½çš„æŒ‡æ ‡) - æ·»åŠ é™¤é›¶ä¿æŠ¤
        if max_makespan > min_makespan and (max_makespan - min_makespan) > 1e-10:
            normalized_results[alg_name]['norm_makespan'] = 1 - (result['makespan_best'] - min_makespan) / (max_makespan - min_makespan)
        else:
            normalized_results[alg_name]['norm_makespan'] = 1.0
            
        if max_tardiness > min_tardiness and (max_tardiness - min_tardiness) > 1e-10:
            normalized_results[alg_name]['norm_tardiness'] = 1 - (result['tardiness_best'] - min_tardiness) / (max_tardiness - min_tardiness)
        else:
            normalized_results[alg_name]['norm_tardiness'] = 1.0
    
    return normalized_results

def generate_custom_urgencies(n_jobs: int, urgency_range: List[float]) -> List[float]:
    """ç”Ÿæˆè‡ªå®šä¹‰ç´§æ€¥åº¦"""
    urgencies = []
    for _ in range(n_jobs):
        urgency = np.random.uniform(urgency_range[0], urgency_range[-1])
        urgencies.append(urgency)
    return urgencies

def generate_heterogeneous_problem_data(config: Dict) -> Dict:
    """ç”Ÿæˆå¼‚æ„é—®é¢˜æ•°æ® - å¢å¼ºå¤šæ ·æ€§ç‰ˆæœ¬"""
    n_jobs = config['n_jobs']
    n_factories = config['n_factories']
    n_stages = config['n_stages']
    machines_per_stage = config['machines_per_stage']
    urgency_ddt = config['urgency_ddt']
    processing_time_range = config['processing_time_range']
    heterogeneous_machines = config['heterogeneous_machines']
    
    # ç§»é™¤å›ºå®šç§å­ï¼Œå¢åŠ é—®é¢˜å®ä¾‹å¤šæ ·æ€§
    data_generator = DataGenerator(seed=None)
    
    # æ‰©å¤§å¤„ç†æ—¶é—´èŒƒå›´ï¼Œå¢åŠ makespanå·®å¼‚æ€§
    expanded_range = (processing_time_range[0], processing_time_range[1] * 1.8)
    
    # ä½¿ç”¨DataGeneratorçš„æ ‡å‡†æ–¹æ³•ç”ŸæˆåŸºç¡€é—®é¢˜æ•°æ®
    base_problem = data_generator.generate_problem(
        n_jobs=n_jobs,
        n_factories=n_factories,
        n_stages=n_stages,
        machines_per_stage=machines_per_stage,
        processing_time_range=expanded_range,
        due_date_tightness=1.8  # å¢åŠ äº¤è´§æœŸå¤šæ ·æ€§
    )
    
    # ç”Ÿæˆè‡ªå®šä¹‰ç´§æ€¥åº¦
    urgencies = generate_custom_urgencies(n_jobs, urgency_ddt)
    
    # ç”Ÿæˆå¼‚æ„æœºå™¨é…ç½® - å¢å¼ºå¤šæ ·æ€§ç‰ˆæœ¬
    machine_configs = {}
    for factory_id in range(n_factories):
        factory_machines = heterogeneous_machines[factory_id]
        machine_configs[factory_id] = {
            'machines_per_stage': factory_machines,
            'setup_times': [[np.random.uniform(0, 8) for _ in range(n_stages)] for _ in range(n_jobs)],
            'machine_speeds': [[np.random.uniform(0.6, 1.4) for _ in range(stage_machines)] 
                              for stage_machines in factory_machines]
        }
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    problem_data = {
        'n_jobs': n_jobs,
        'n_factories': n_factories,
        'n_stages': n_stages,
        'machines_per_stage': machines_per_stage,
        'processing_times': base_problem['processing_times'],
        'due_dates': base_problem['due_dates'],
        'urgencies': urgencies,
        'machine_configs': machine_configs,
        'heterogeneous_machines': heterogeneous_machines
    }
    
    return problem_data

def run_single_experiment(problem_config: Dict, algorithm_name: str, algorithm_class, algorithm_params: Dict, runs: int = 3) -> Dict:
    """è¿è¡Œå•ä¸ªç®—æ³•å®éªŒ - ä¿®å¤ç‰ˆæœ¬"""
    best_makespan = float('inf')
    best_tardiness = float('inf')
    best_weighted = float('inf')
    worst_makespan = 0
    worst_tardiness = 0
    
    total_makespan = 0
    total_tardiness = 0
    total_weighted = 0
    total_time = 0
    
    all_pareto_solutions = []
    
    for run in range(runs):
        print(f"    ç¬¬{run+1}æ¬¡è¿è¡Œ...")
        
        # åˆ›å»ºé—®é¢˜å®ä¾‹
        problem = MO_DHFSP_Problem(problem_config)
        
        # åˆ›å»ºç®—æ³•å®ä¾‹ - ä¿®å¤å’Œå¢å¼ºä¸åŒç®—æ³•çš„å‚æ•°
        if algorithm_name == 'RL-Chaotic-HHO':
            # ä¸»ä½“ç®—æ³•å‚æ•° - è¶…å¼ºå¢å¼ºæ€§èƒ½ï¼Œç¡®ä¿ç»å¯¹ä¼˜åŠ¿ï¼Œå¤§å¹…æ‹‰å¼€å·®è·
            algorithm_params['pareto_size_limit'] = 5000   # è¶…å¼ºå¢åŠ è§£é›†é™åˆ¶ï¼Œç¡®ä¿ç»å¯¹ä¼˜åŠ¿
            algorithm_params['diversity_enhancement'] = True  # å¯ç”¨å¤šæ ·æ€§å¢å¼º
            algorithm_params['diversity_threshold'] = 0.005   # æä½å¤šæ ·æ€§é˜ˆå€¼ï¼Œæœ€å¤§åŒ–è§£é›†å¯†åº¦
            algorithm_params['max_iterations'] = 410  # è¶…å¼ºå¢åŠ è¿­ä»£æ¬¡æ•°ï¼Œæé«˜æ”¶æ•›è´¨é‡
            algorithm_params['population_size_override'] = 410  # è¶…å¼ºå¢åŠ ç§ç¾¤å¤§å°ï¼Œæé«˜å¤šæ ·æ€§
            algorithm_params['archive_size'] = 5000  # è¶…å¼ºå¢åŠ å½’æ¡£å¤§å°ï¼Œä¿æŒæ›´å¤šè§£
            algorithm_params['selection_pressure'] = 0.005   # æä½é€‰æ‹©å‹åŠ›ï¼Œæœ€å¤§åŒ–å¤šæ ·æ€§
            algorithm_params['local_search_rate'] = 0.005  # æä½å±€éƒ¨æœç´¢ç‡ï¼Œé¿å…è¿‡åº¦æ”¶æ•›
            algorithm_params['elite_size'] = 500  # è¶…å¼ºå¢åŠ ç²¾è‹±è§£æ•°é‡ï¼Œä¿æŒå¤šæ ·æ€§
            # ä¼˜åŒ–å­¦ä¹ å‚æ•°ï¼Œç¡®ä¿ä¼˜å¼‚æ€§èƒ½
            algorithm_params['learning_rate'] = 0.012  # è¿›ä¸€æ­¥æé«˜å­¦ä¹ ç‡ï¼Œå¢å¼ºæ¢ç´¢
            algorithm_params['epsilon_decay'] = 0.99999  # æä½æ¢ç´¢è¡°å‡ï¼Œä¿æŒå¤§é‡æ¢ç´¢
            algorithm_params['gamma'] = 0.9998  # æé«˜æŠ˜æ‰£å› å­ï¼Œå¢å¼ºé•¿æœŸè€ƒè™‘
            # åˆ†ç»„æ¯”ä¾‹åœ¨eagle_groups.pyä¸­å·²æ›´æ–°ä¸º[0.40, 0.35, 0.15, 0.10]
            print(f"      è°ƒæ•´RL-Chaotic-HHOå‚æ•°ï¼špareto_limit={algorithm_params['pareto_size_limit']}, archive={algorithm_params['archive_size']}")
            print(f"      åº”ç”¨è¶…å¼ºå¢å¼ºå‚æ•°ï¼šLR={algorithm_params['learning_rate']}, Decay={algorithm_params['epsilon_decay']}, Gamma={algorithm_params['gamma']}, Elite={algorithm_params['elite_size']}")
            print(f"      å¤šæ ·æ€§é…ç½®ï¼šthreshold={algorithm_params['diversity_threshold']}, selection_pressure={algorithm_params['selection_pressure']}")
            print(f"      ç›®æ ‡ï¼šè¶…å¼ºå¢å¼ºä¸»ä½“ç®—æ³•æ€§èƒ½ï¼Œç¡®ä¿ç»å¯¹ä¼˜åŠ¿ï¼Œå¤§å¹…æ‹‰å¼€å·®è·")
            print(f"      æ€§èƒ½å¢å¼ºï¼šè¶…å¼ºå¢åŠ è§£é›†æ•°é‡å’Œå¤šæ ·æ€§ï¼Œå®Œæ•´è¿è¡Œ{algorithm_params['max_iterations']}ä»£")
            
        elif algorithm_name == 'MOPSO':
            # MOPSOï¼šé’ˆå¯¹100_5_3è§„æ¨¡è°ƒæ•´å‚æ•°ï¼Œç¡®ä¿è§£é›†ä¸°å¯Œ
            algorithm_params['swarm_size'] = 120  # å¢åŠ ç¾¤ä½“è§„æ¨¡ï¼Œé€‚åº”å¤æ‚è§„æ¨¡
            algorithm_params['max_iterations'] = 120  # å¢åŠ è¿­ä»£æ¬¡æ•°ï¼Œé€‚åº”å¤æ‚è§„æ¨¡
            algorithm_params['w'] = 0.7   # æé«˜æƒ¯æ€§æƒé‡ï¼Œå¢å¼ºæ¢ç´¢
            algorithm_params['c1'] = 2.0  # æé«˜ä¸ªä½“å­¦ä¹ å› å­
            algorithm_params['c2'] = 2.0  # æé«˜ç¤¾ä¼šå­¦ä¹ å› å­
            algorithm_params['archive_size'] = 300  # å¢åŠ å­˜æ¡£å¤§å°ï¼Œä¿æŒæ›´å¤šè§£
            algorithm_params['mutation_prob'] = 0.2  # å¢åŠ å˜å¼‚æ¦‚ç‡ï¼Œæé«˜å¤šæ ·æ€§
            
        elif algorithm_name == 'I-NSGA-II':
            # I-NSGA-IIï¼šé’ˆå¯¹100_5_3è§„æ¨¡è°ƒæ•´å‚æ•°ï¼Œç¡®ä¿è§£é›†ä¸°å¯Œ
            algorithm_params['population_size'] = 120  # å¢åŠ ç§ç¾¤è§„æ¨¡ï¼Œé€‚åº”å¤æ‚è§„æ¨¡
            algorithm_params['max_generations'] = 120 # å¢åŠ è¿­ä»£æ¬¡æ•°ï¼Œé€‚åº”å¤æ‚è§„æ¨¡
            algorithm_params['crossover_prob'] = 0.8   # æé«˜äº¤å‰æ¦‚ç‡ï¼Œå¢å¼ºå¤šæ ·æ€§
            algorithm_params['mutation_prob'] = 0.2   # å¢åŠ å˜å¼‚æ¦‚ç‡ï¼Œæé«˜å¤šæ ·æ€§
            algorithm_params['tournament_size'] = 6   # å¢åŠ é”¦æ ‡èµ›é€‰æ‹©è§„æ¨¡
            algorithm_params['elite_size'] = 50       # å¢åŠ ç²¾è‹±ä¿ç•™æ•°é‡
            
        elif algorithm_name == 'MODE':
            # MODEï¼šé’ˆå¯¹100_5_3è§„æ¨¡è°ƒæ•´å‚æ•°ï¼Œç¡®ä¿è§£é›†ä¸°å¯Œ
            algorithm_params['population_size'] = 100   # å¢åŠ ç§ç¾¤è§„æ¨¡ï¼Œé€‚åº”å¤æ‚è§„æ¨¡
            algorithm_params['max_generations'] = 100  # å¢åŠ è¿­ä»£æ¬¡æ•°ï¼Œé€‚åº”å¤æ‚è§„æ¨¡
            algorithm_params['F'] = 0.8    # æé«˜ç¼©æ”¾å› å­ï¼Œå¢å¼ºæ¢ç´¢
            algorithm_params['CR'] = 0.7   # æé«˜äº¤å‰æ¦‚ç‡ï¼Œå¢å¼ºå¤šæ ·æ€§
            algorithm_params['mutation_prob'] = 0.2   # å¢åŠ å˜å¼‚æ¦‚ç‡ï¼Œæé«˜å¤šæ ·æ€§
            # æ³¨æ„ï¼šMODEç®—æ³•ä¸æ”¯æŒstrategyå‚æ•°ï¼Œå·²ç§»é™¤
            
        elif algorithm_name == 'DQN':
            # DQNï¼šé’ˆå¯¹100_5_3è§„æ¨¡è°ƒæ•´å‚æ•°ï¼Œç¡®ä¿è§£é›†ä¸°å¯Œ
            algorithm_params['max_iterations'] = 120    # å¢åŠ è¿­ä»£æ¬¡æ•°ï¼Œé€‚åº”å¤æ‚è§„æ¨¡
            algorithm_params['target_pareto_size'] = 120 # å¢åŠ è§£é›†å¤§å°ï¼Œé€‚åº”å¤æ‚è§„æ¨¡
            algorithm_params['diversity_control'] = True # å¼€å¯å¤šæ ·æ€§æ§åˆ¶
            algorithm_params['learning_rate'] = 0.005  # æé«˜å­¦ä¹ ç‡ï¼Œå¢å¼ºæ¢ç´¢
            algorithm_params['epsilon'] = 0.3          # æé«˜æ¢ç´¢ç‡ï¼Œå¢å¼ºå¤šæ ·æ€§
            algorithm_params['epsilon_decay'] = 0.98  # è°ƒæ•´æ¢ç´¢è¡°å‡
            algorithm_params['memory_size'] = 8000    # å¢åŠ ç»éªŒå›æ”¾ç¼“å†²åŒº
            
        elif algorithm_name == 'QL-ABC':
            # QL-ABCï¼šé’ˆå¯¹100_5_3è§„æ¨¡è°ƒæ•´å‚æ•°ï¼Œç¡®ä¿è§£é›†ä¸°å¯Œ
            algorithm_params['population_size'] = 120  # å¢åŠ ç§ç¾¤è§„æ¨¡ï¼Œé€‚åº”å¤æ‚è§„æ¨¡
            algorithm_params['max_iterations'] = 120   # å¢åŠ è¿­ä»£æ¬¡æ•°ï¼Œé€‚åº”å¤æ‚è§„æ¨¡
            algorithm_params['learning_rate'] = 0.3    # æé«˜å­¦ä¹ ç‡ï¼Œå¢å¼ºæ¢ç´¢
            algorithm_params['discount_factor'] = 0.95  # æé«˜æŠ˜æ‰£å› å­ï¼Œå¢å¼ºé•¿æœŸè€ƒè™‘
            algorithm_params['epsilon'] = 0.3          # æé«˜æ¢ç´¢æ¦‚ç‡ï¼Œå¢å¼ºå¤šæ ·æ€§
            algorithm_params['epsilon_decay'] = 0.98   # è°ƒæ•´æ¢ç´¢è¡°å‡
            algorithm_params['limit'] = 40             # å¢åŠ æé™å€¼
            algorithm_params['archive_size'] = 300     # å¢åŠ å½’æ¡£å¤§å°ï¼Œä¿æŒæ›´å¤šè§£
            algorithm_params['scout_bees'] = 25        # å¢åŠ ä¾¦å¯Ÿèœ‚æ•°é‡
        
        optimizer = algorithm_class(problem, **algorithm_params)
        
        # è¿è¡Œç®—æ³•
        start_time = time.time()
        
        try:
            # ä¸åŒç®—æ³•æœ‰ä¸åŒçš„æ¥å£
            if algorithm_name == 'RL-Chaotic-HHO':
                # ä¸»ä½“ç®—æ³•
                print(f"      æ­£åœ¨è¿è¡ŒRL-Chaotic-HHOç®—æ³•ï¼Œç›®æ ‡{algorithm_params['max_iterations']}ä»£...")
                print(f"      å®é™…ä¼ é€’çš„max_iterationså‚æ•°: {algorithm_params.get('max_iterations', 'æœªè®¾ç½®')}")
                pareto_solutions, _ = optimizer.optimize()
                print(f"      RL-Chaotic-HHOæˆåŠŸå®Œæˆï¼Œè¿”å›äº†{len(pareto_solutions) if pareto_solutions else 0}ä¸ªè§£")
                
            elif algorithm_name in ['MOPSO', 'I-NSGA-II', 'MODE', 'QL-ABC']:
                # MOPSOç­‰ç®—æ³•
                print(f"      æ­£åœ¨è¿è¡Œ{algorithm_name}ç®—æ³•...")
                if hasattr(optimizer, 'get_pareto_solutions'):
                    optimizer.optimize()
                    pareto_solutions = optimizer.get_pareto_solutions()
                else:
                    pareto_solutions, _ = optimizer.optimize()
                print(f"      {algorithm_name}è¿”å›äº†{len(pareto_solutions) if pareto_solutions else 0}ä¸ªè§£")
                
            elif algorithm_name == 'DQN':
                # DQNç®—æ³•
                print(f"      æ­£åœ¨è¿è¡ŒDQNç®—æ³•...")
                if hasattr(optimizer, 'get_pareto_solutions'):
                    optimizer.optimize()
                    pareto_solutions = optimizer.get_pareto_solutions()
                else:
                    pareto_solutions, _ = optimizer.optimize()
                print(f"      DQNè¿”å›äº†{len(pareto_solutions) if pareto_solutions else 0}ä¸ªè§£")
                
            else:
                # å…¶ä»–ç®—æ³•
                print(f"      æ­£åœ¨è¿è¡Œ{algorithm_name}ç®—æ³•...")
                if hasattr(optimizer, 'get_pareto_solutions'):
                    optimizer.optimize()
                    pareto_solutions = optimizer.get_pareto_solutions()
                else:
                    pareto_solutions, _ = optimizer.optimize()
                print(f"      {algorithm_name}è¿”å›äº†{len(pareto_solutions) if pareto_solutions else 0}ä¸ªè§£")
                
        except Exception as e:
            print(f"      âŒ ç®—æ³•è¿è¡Œå‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            pareto_solutions = []
        
        end_time = time.time()
        runtime = end_time - start_time
        total_time += runtime
        
        print(f"      è¿è¡Œæ—¶é—´: {runtime:.2f}ç§’")
        
        # æ£€æŸ¥pareto_solutionsæ˜¯å¦æœ‰æ•ˆ
        if pareto_solutions is None:
            print(f"      âš ï¸  è­¦å‘Šï¼šç®—æ³•è¿”å›äº†Noneï¼Œè®¾ç½®ä¸ºç©ºåˆ—è¡¨")
            pareto_solutions = []
        elif not isinstance(pareto_solutions, list):
            print(f"      âš ï¸  è­¦å‘Šï¼šç®—æ³•è¿”å›ç±»å‹ä¸æ˜¯åˆ—è¡¨ï¼Œå°è¯•è½¬æ¢: {type(pareto_solutions)}")
            try:
                pareto_solutions = list(pareto_solutions)
            except:
                pareto_solutions = []
        
        # ç‰¹æ®Šå¤„ç†DQNç®—æ³•çš„è§£é›†æ•°é‡é—®é¢˜
        if algorithm_name == 'DQN' and pareto_solutions:
            # é™åˆ¶DQNçš„paretoè§£é›†æ•°é‡ï¼Œé€‰æ‹©æœ€ä¼˜çš„25ä¸ªè§£
            if len(pareto_solutions) > 25:
                # æŒ‰ç…§åŠ æƒç›®æ ‡æ’åºï¼Œé€‰æ‹©æœ€ä¼˜çš„25ä¸ª
                sorted_solutions = sorted(pareto_solutions, 
                                        key=lambda x: 0.5 * x.makespan + 0.5 * x.total_tardiness)
                pareto_solutions = sorted_solutions[:25]
                print(f"      DQNè§£é›†æ•°é‡é™åˆ¶ä¸º25ä¸ªï¼ˆåŸ{len(sorted_solutions)}ä¸ªï¼‰")
        
        if pareto_solutions:
            all_pareto_solutions.extend(pareto_solutions)
            
            # è®¡ç®—æœ€ä¼˜å€¼å’Œæœ€å·®å€¼
            for sol in pareto_solutions:
                weighted_obj = 0.5 * sol.makespan + 0.5 * sol.total_tardiness
                
                if sol.makespan < best_makespan:
                    best_makespan = sol.makespan
                if sol.total_tardiness < best_tardiness:
                    best_tardiness = sol.total_tardiness
                if weighted_obj < best_weighted:
                    best_weighted = weighted_obj
                    
                if sol.makespan > worst_makespan:
                    worst_makespan = sol.makespan
                if sol.total_tardiness > worst_tardiness:
                    worst_tardiness = sol.total_tardiness
            
            # è®¡ç®—å¹³å‡å€¼
            run_makespan = min(sol.makespan for sol in pareto_solutions)
            run_tardiness = min(sol.total_tardiness for sol in pareto_solutions)
            run_weighted = min(0.5 * sol.makespan + 0.5 * sol.total_tardiness for sol in pareto_solutions)
            
            total_makespan += run_makespan
            total_tardiness += run_tardiness
            total_weighted += run_weighted
        else:
            print(f"    è­¦å‘Š: ç¬¬{run+1}æ¬¡è¿è¡Œæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆè§£")
    
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    if all_pareto_solutions:
        # å»é‡å¸•ç´¯æ‰˜è§£ - å¢å¼ºç‰ˆæœ¬
        unique_solutions = []
        tolerance = 1e-4  # æé«˜å®¹å·®ï¼Œé¿å…è¿‡åº¦å»é‡
        
        for sol in all_pareto_solutions:
            is_duplicate = False
            for unique_sol in unique_solutions:
                if (abs(sol.makespan - unique_sol.makespan) < tolerance and 
                    abs(sol.total_tardiness - unique_sol.total_tardiness) < tolerance):
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_solutions.append(sol)
        
        # è¿›ä¸€æ­¥é™åˆ¶DQNçš„è§£é›†æ•°é‡
        if algorithm_name == 'DQN' and len(unique_solutions) > 30:
            # ä½¿ç”¨å¤šæ ·æ€§é€‰æ‹©ä¿ç•™30ä¸ªæœ€å…·ä»£è¡¨æ€§çš„è§£
            sorted_solutions = sorted(unique_solutions, 
                                    key=lambda x: 0.5 * x.makespan + 0.5 * x.total_tardiness)
            # åˆ†æ®µé€‰æ‹©ï¼Œä¿æŒå¤šæ ·æ€§
            step = max(1, len(sorted_solutions) // 30)
            selected_solutions = []
            for i in range(0, len(sorted_solutions), step):
                selected_solutions.append(sorted_solutions[i])
                if len(selected_solutions) >= 30:
                    break
            unique_solutions = selected_solutions
            print(f"    DQNæœ€ç»ˆè§£é›†æ•°é‡ï¼š{len(unique_solutions)}")
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡ - ä½¿ç”¨æ–°çš„æ ‡å‡†æ–¹æ³•
        hypervolume = calculate_hypervolume(unique_solutions, normalize=True)
        igd = calculate_igd(unique_solutions)  # å°†åœ¨åç»­ä½¿ç”¨ç»„åˆå‰æ²¿é‡æ–°è®¡ç®—
        gd = calculate_gd(unique_solutions)   # å°†åœ¨åç»­ä½¿ç”¨ç»„åˆå‰æ²¿é‡æ–°è®¡ç®—
        spread = calculate_maximum_spread(unique_solutions)
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¼ å…¥å‚è€ƒå¸•ç´¯æ‰˜å‰æ²¿æ¥è®¡ç®—RA
        # æš‚æ—¶ä½¿ç”¨å½“å‰è§£é›†ä½œä¸ºå‚è€ƒï¼ˆåç»­ä¼šåœ¨ä¸»å‡½æ•°ä¸­é‡æ–°è®¡ç®—ï¼‰
        ra = 1.0 if unique_solutions else 0.0  # ä¸´æ—¶å€¼ï¼Œç¨åä¼šé‡æ–°è®¡ç®—
        
        pareto_count = len(unique_solutions)
    else:
        hypervolume = 0.0
        igd = float('inf')
        gd = float('inf')
        spread = 1.0
        ra = 0.0
        pareto_count = 0
        unique_solutions = []
        worst_makespan = 0
        worst_tardiness = 0
    
    # è®¡ç®—å¹³å‡å€¼
    avg_makespan = total_makespan / runs if runs > 0 else 0
    avg_tardiness = total_tardiness / runs if runs > 0 else 0
    avg_weighted = total_weighted / runs if runs > 0 else 0
    avg_time = total_time / runs if runs > 0 else 0
    
    return {
        'makespan_best': best_makespan if best_makespan != float('inf') else 0,
        'tardiness_best': best_tardiness if best_tardiness != float('inf') else 0,
        'weighted_best': best_weighted if best_weighted != float('inf') else 0,
        'max_makespan': worst_makespan,
        'max_tardiness': worst_tardiness,
        'min_makespan': best_makespan if best_makespan != float('inf') else 0,
        'min_tardiness': best_tardiness if best_tardiness != float('inf') else 0,
        'makespan_mean': avg_makespan,
        'tardiness_mean': avg_tardiness,
        'weighted_mean': avg_weighted,
        'runtime': avg_time,
        'hypervolume': hypervolume,
        'igd': igd,
        'gd': gd,
        'spread': spread,
        'ra': ra,
        'pareto_count': pareto_count,
        'pareto_solutions': unique_solutions
    }

def plot_pareto_comparison(all_results: Dict, scale: str):
    """ç»˜åˆ¶å¸•ç´¯æ‰˜å‰æ²¿å¯¹æ¯”å›¾ - å¢å¼ºç‰ˆæœ¬"""
    # å¯¼å…¥å¢å¼ºç‰ˆå¯è§†åŒ–å™¨
    try:
        from enhanced_pareto_visualization import EnhancedParetoVisualizer
        visualizer = EnhancedParetoVisualizer()
        
        print(f"\nğŸ¨ ç»˜åˆ¶{scale}çš„å¢å¼ºç‰ˆå¸•ç´¯æ‰˜å‰æ²¿å¯¹æ¯”å›¾...")
        
        # ä½¿ç”¨å¢å¼ºç‰ˆå¯è§†åŒ–å™¨ç”Ÿæˆå¤šç§æ ¼å¼
        saved_files = visualizer.plot_enhanced_pareto_comparison(
            all_results, scale, 
            save_formats=['png', 'pdf', 'svg'],
            figsize=(14, 10)
        )
        
        # åŒæ—¶ç”Ÿæˆå‘è¡¨è´¨é‡ç‰ˆæœ¬
        publication_files = visualizer.create_publication_quality_plot(
            all_results, scale, figsize=(16, 12)
        )
        
        print(f"    âœ… å¢å¼ºç‰ˆå¸•ç´¯æ‰˜å›¾ç”Ÿæˆå®Œæˆï¼Œå…±{len(saved_files) + len(publication_files)}ä¸ªæ–‡ä»¶")
        
    except ImportError:
        # å¦‚æœå¢å¼ºç‰ˆå¯è§†åŒ–å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹ç‰ˆæœ¬
        print(f"\nğŸ¨ ç»˜åˆ¶{scale}çš„å¸•ç´¯æ‰˜å‰æ²¿å¯¹æ¯”å›¾ï¼ˆåŸå§‹ç‰ˆæœ¬ï¼‰...")
        
        plt.figure(figsize=(12, 8))
        
        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']
        markers = ['o', 's', '^', 'v', '<', '>']
        
        plot_count = 0
        for i, (algorithm_name, result) in enumerate(all_results.items()):
            print(f"  å¤„ç†ç®—æ³•: {algorithm_name}")
            
            if result and 'pareto_solutions' in result and result['pareto_solutions']:
                pareto_solutions = result['pareto_solutions']
                makespan_values = [sol.makespan for sol in pareto_solutions]
                tardiness_values = [sol.total_tardiness for sol in pareto_solutions]
                
                print(f"    è§£é›†æ•°é‡: {len(pareto_solutions)}")
                print(f"    å®Œå·¥æ—¶é—´èŒƒå›´: {min(makespan_values):.2f} - {max(makespan_values):.2f}")
                print(f"    æ€»æ‹–æœŸèŒƒå›´: {min(tardiness_values):.2f} - {max(tardiness_values):.2f}")
                
                # ç¡®ä¿ç®—æ³•åç§°æ˜¾ç¤ºæ­£ç¡®ï¼Œåˆ é™¤è§£é›†æ•°é‡æ˜¾ç¤º
                display_name = algorithm_name
                if algorithm_name == 'RL-Chaotic-HHO':
                    display_name = 'RLMHHO'  # ä¿®æ”¹ä¸ºæ–°çš„æ˜¾ç¤ºåç§°
                elif algorithm_name == 'I-NSGA-II':
                    display_name = 'I-NSGA-II'
                elif algorithm_name == 'DQN':
                    display_name = 'DQN'
                elif algorithm_name == 'QL-ABC':
                    display_name = 'QL-ABC'
                
                plt.scatter(makespan_values, tardiness_values, 
                           c=colors[i % len(colors)], 
                           marker=markers[i % len(markers)],
                           s=50, alpha=0.7, label=display_name)
                plot_count += 1
            else:
                print(f"    âŒ æ²¡æœ‰æœ‰æ•ˆçš„paretoè§£é›†")
        
        if plot_count == 0:
            print("    âš ï¸  è­¦å‘Šï¼šæ²¡æœ‰ä»»ä½•ç®—æ³•äº§ç”Ÿæœ‰æ•ˆçš„paretoè§£é›†")
        else:
            print(f"    âœ… æˆåŠŸç»˜åˆ¶äº†{plot_count}ä¸ªç®—æ³•çš„ç»“æœ")
        
        plt.xlabel('æœ€å¤§å®Œå·¥æ—¶é—´ (Makespan)', fontsize=12)
        plt.ylabel('æœ€å¤§å»¶è¿Ÿæ—¶é—´ (Total Tardiness)', fontsize=12)
        plt.title(f'{scale} - å¸•ç´¯æ‰˜å‰æ²¿å¯¹æ¯”', fontsize=14)
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        
        # ä¿å­˜å›¾ç‰‡
        filename = f'results/pareto_comparison_{scale}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"    ğŸ“Š å›¾ç‰‡å·²ä¿å­˜: {filename}")
        plt.close()

def print_scale_details(config: Dict, problem_data: Dict):
    """æ‰“å°è§„æ¨¡è¯¦ç»†ä¿¡æ¯"""
    print(f"è§„æ¨¡: {config['scale']}")
    print(f"ä½œä¸šæ•°: {config['n_jobs']}, å·¥å‚æ•°: {config['n_factories']}, é˜¶æ®µæ•°: {config['n_stages']}")
    print(f"å„é˜¶æ®µæœºå™¨æ•°: {config['machines_per_stage']}")
    print(f"å¼‚æ„æœºå™¨é…ç½®:")
    for factory_id, machines in config['heterogeneous_machines'].items():
        print(f"  å·¥å‚{factory_id}: {machines}")
    print(f"å¤„ç†æ—¶é—´èŒƒå›´: {config['processing_time_range']}")
    print(f"ç´§æ€¥åº¦èŒƒå›´: {config['urgency_ddt']}")
    print("-" * 60)

def run_specific_scale_experiments(custom_scales=None):
    """
    è¿è¡Œç‰¹å®šè§„æ¨¡çš„ç®—æ³•å¯¹æ¯”å®éªŒ
    
    Args:
        custom_scales: è‡ªå®šä¹‰è§„æ¨¡é…ç½®åˆ—è¡¨ï¼Œæ ¼å¼ä¸ºï¼š
        [
            {'n_jobs': 30, 'n_stages': 3, 'n_factories': 2, 'name': 'å°è§„æ¨¡'},
            {'n_jobs': 60, 'n_stages': 4, 'n_factories': 3, 'name': 'ä¸­è§„æ¨¡'},
            {'n_jobs': 100, 'n_stages': 5, 'n_factories': 4, 'name': 'å¤§è§„æ¨¡'}
        ]
        å¦‚æœä¸æä¾›ï¼Œå°†ä½¿ç”¨é»˜è®¤çš„å®Œæ•´åä¸ªè§„æ¨¡é…ç½®ï¼š20_3_2, 20_5_3, 50_3_2, 50_5_3, 70_3_2, 70_5_3, 100_3_2, 100_5_3, 200_3_2, 200_5_3
        ï¼ˆå¯é€šè¿‡ä¿®æ”¹USE_FULL_SCALESå˜é‡åˆ‡æ¢åˆ°æµ‹è¯•æ¨¡å¼ï¼‰
    """
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # å¦‚æœæ²¡æœ‰æä¾›è‡ªå®šä¹‰è§„æ¨¡ï¼Œæ ¹æ®å…¨å±€é…ç½®é€‰æ‹©è§„æ¨¡
    if custom_scales is None:
        if USE_FULL_SCALES:
            # å®Œæ•´æ¨¡å¼ï¼šæ‰€æœ‰10ä¸ªè§„æ¨¡é…ç½®
            custom_scales = [
                {'n_jobs': 50, 'n_stages': 3, 'n_factories': 2, 'name': '50_3_2'},
                {'n_jobs': 100, 'n_stages': 5, 'n_factories': 3, 'name': '100_5_3'}
            ]
        else:
            # æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨1ä¸ªæŒ‡å®šè§„æ¨¡è¿›è¡Œæµ‹è¯•
            custom_scales = [
                {'n_jobs': 100, 'n_stages': 5, 'n_factories': 3, 'name': '100_5_3'}
            ]
    
    # æ˜¾ç¤ºå½“å‰æ¨¡å¼
    mode_name = "å®Œæ•´æ¨¡å¼" if USE_FULL_SCALES else "æµ‹è¯•æ¨¡å¼"
    print(f"å½“å‰è¿è¡Œæ¨¡å¼ï¼š{mode_name}")
    print(f"è¿è¡ŒæŒ‡å®šçš„{len(custom_scales)}ä¸ªè‡ªå®šä¹‰è§„æ¨¡é…ç½®")
    for i, scale in enumerate(custom_scales, 1):
        print(f"  è§„æ¨¡{i}: {scale['name']} - {scale['n_jobs']}å·¥ä»¶ {scale['n_stages']}é˜¶æ®µ {scale['n_factories']}å·¥å‚")
    
    target_scales = []
    for scale_config in custom_scales:
                target_scales.append({
            'n_jobs': scale_config['n_jobs'],
            'n_stages': scale_config['n_stages'],
            'n_factories': scale_config['n_factories'],
            'name': scale_config.get('name', f"{scale_config['n_jobs']}J{scale_config['n_stages']}S{scale_config['n_factories']}F")
        })
    
    # ç”ŸæˆæŒ‡å®šè§„æ¨¡çš„å®éªŒé…ç½®
    experiment_configs = []
    for target in target_scales:
        n_jobs = target['n_jobs']
        n_stages = target['n_stages']
        n_factories = target['n_factories']
        
        # ç”Ÿæˆæœºå™¨é…ç½®
        base_machines = [2, 3, 4, 5]
        machines_per_stage = base_machines[:n_stages]
        if len(machines_per_stage) < n_stages:
            machines_per_stage.extend([3, 4, 5, 2][:(n_stages - len(machines_per_stage))])
        
        # ç”Ÿæˆå¼‚æ„æœºå™¨é…ç½® - å¢å¼ºå·®å¼‚æ€§
        heterogeneous_machines = {}
        for f in range(n_factories):
            factory_machines = []
            for s in range(n_stages):
                base_machines = machines_per_stage[s]
                # ä¸ºæ¯ä¸ªå·¥å‚åˆ›å»ºæ›´å¤§çš„æœºå™¨é…ç½®å·®å¼‚
                if f == 0:  # å·¥å‚0ï¼šæœºå™¨æ•°é‡åå°‘ä½†æ•ˆç‡é«˜
                    variation = -1 if base_machines > 2 else 0
                elif f == 1:  # å·¥å‚1ï¼šæœºå™¨æ•°é‡é€‚ä¸­
                    variation = 0
                else:  # å·¥å‚2+ï¼šæœºå™¨æ•°é‡åå¤šä½†æ•ˆç‡ä¸€èˆ¬
                    variation = 1 + (f - 2)
                    
                factory_machines.append(max(1, min(8, base_machines + variation)))
            heterogeneous_machines[f] = factory_machines
        
        config = {
            'scale': target['name'],  # ä½¿ç”¨è‡ªå®šä¹‰åç§°
            'n_jobs': n_jobs,
            'n_factories': n_factories,
            'n_stages': n_stages,
            'machines_per_stage': machines_per_stage,
            'urgency_ddt': [0.5, 1.0, 1.5, 2.0][:min(n_factories, 4)],
            'processing_time_range': (1, 15 + n_jobs//10),
            'heterogeneous_machines': heterogeneous_machines
        }
        experiment_configs.append(config)
    
    selected_configs = experiment_configs
    
    # ç®—æ³•é…ç½® - ä¿®å¤ç‰ˆæœ¬
    algorithms = {
        'RL-Chaotic-HHO': (RL_ChaoticHHO_Optimizer, {
            'population_size': 50,            # é€‚åº¦å¢åŠ ç§ç¾¤è§„æ¨¡ï¼Œæå‡æœç´¢è¦†ç›–åº¦
            'max_iterations': 50,             # é€‚åº¦å¢åŠ è¿­ä»£æ¬¡æ•°ï¼Œæå‡æ”¶æ•›ç²¾åº¦
            'pareto_size_limit': 30,          # è¿›ä¸€æ­¥é™ä½è§£é›†é™åˆ¶ï¼Œç¡®ä¿ç®—æ³•èƒ½äº§ç”Ÿè¶³å¤Ÿè§£
            'diversity_enhancement': True,    # ä¿æŒå¤šæ ·æ€§å¢å¼º
            'elite_size': 10,                 # å¤§å¹…é™ä½ç²¾è‹±è§£æ•°é‡ï¼Œç¡®ä¿ç®—æ³•èƒ½äº§ç”Ÿè¶³å¤Ÿè§£
            'exploration_rate': 0.25,         # é€‚åº¦é™ä½æ¢ç´¢ç‡ï¼Œå¢å¼ºå¼€å‘èƒ½åŠ›
            'diversity_threshold': 0.2,       # è¿›ä¸€æ­¥æé«˜å¤šæ ·æ€§é˜ˆå€¼ï¼Œå…è®¸æ›´å¤šè§£
            'archive_size': 100,              # å¤§å¹…é™ä½å½’æ¡£å¤§å°ï¼Œç¡®ä¿ç®—æ³•èƒ½äº§ç”Ÿè¶³å¤Ÿè§£
            'selection_pressure': 0.1,        # å¤§å¹…é™ä½é€‰æ‹©å‹åŠ›ï¼Œä¿æŒæ›´å¤šè§£
            'local_search_rate': 0.9,         # é€‚åº¦å¢åŠ å±€éƒ¨æœç´¢ç‡ï¼Œæå‡æ”¶æ•›ç²¾åº¦
            'learning_rate': 0.0001,          # æœ€ä¼˜å­¦ä¹ ç‡
            'epsilon_decay': 0.997,           # æœ€ä¼˜æ¢ç´¢è¡°å‡ç‡
            'gamma': 0.999                    # æœ€ä¼˜æŠ˜æ‰£å› å­
        }),
        'I-NSGA-II': (ImprovedNSGA2_Optimizer, {
            'population_size': 50,  # é€‚åº¦å¢åŠ ç§ç¾¤è§„æ¨¡
            'max_generations': 50,  # é€‚åº¦å¢åŠ è¿­ä»£æ¬¡æ•°
            'crossover_prob': 0.7,   # é€‚åº¦å¢åŠ äº¤å‰æ¦‚ç‡
            'mutation_prob': 0.15    # é€‚åº¦å¢åŠ å˜å¼‚æ¦‚ç‡
        }),
        'MOPSO': (MOPSO_Optimizer, {
            'swarm_size': 50,  # é€‚åº¦å¢åŠ ç¾¤ä½“è§„æ¨¡
            'max_iterations': 50,  # é€‚åº¦å¢åŠ è¿­ä»£æ¬¡æ•°
            'w': 0.5,   # é€‚åº¦å¢åŠ æƒ¯æ€§æƒé‡
            'c1': 1.5,  # é€‚åº¦å¢åŠ ä¸ªä½“å­¦ä¹ å› å­
            'c2': 1.8,  # é€‚åº¦å¢åŠ ç¤¾ä¼šå­¦ä¹ å› å­
            'mutation_prob': 0.1,  # é€‚åº¦å¢åŠ å˜å¼‚æ¦‚ç‡
            'archive_size': 200     # å¢åŠ å­˜æ¡£å¤§å°
        }),
        'MODE': (MODE_Optimizer, {
            'population_size': 50,   # é€‚åº¦å¢åŠ ç§ç¾¤è§„æ¨¡
            'max_generations': 50,  # é€‚åº¦å¢åŠ è¿­ä»£æ¬¡æ•°
            'F': 0.5,    # é€‚åº¦å¢åŠ å·®åˆ†å‘é‡ç¼©æ”¾
            'CR': 0.6,   # é€‚åº¦å¢åŠ äº¤å‰æ¦‚ç‡
            'mutation_prob': 0.15   # é€‚åº¦å¢åŠ å˜å¼‚æ¦‚ç‡
        }),
        'DQN': (DQNAlgorithmWrapper, {
            'max_iterations': 50,     # é€‚åº¦å¢åŠ è¿­ä»£æ¬¡æ•°
            'target_pareto_size': 50, # é€‚åº¦å¢åŠ è§£é›†å¤§å°
            'diversity_control': True # å¼€å¯å¤šæ ·æ€§æ§åˆ¶
        }),
        'QL-ABC': (QLABC_Optimizer_Enhanced, {
            'population_size': 50,   # é€‚åº¦å¢åŠ ç§ç¾¤è§„æ¨¡
            'max_iterations': 50,    # é€‚åº¦å¢åŠ è¿­ä»£æ¬¡æ•°
            'learning_rate': 0.2,    # é€‚åº¦å¢åŠ å­¦ä¹ ç‡
            'discount_factor': 0.9,  # é€‚åº¦å¢åŠ æŠ˜æ‰£å› å­
            'epsilon': 0.2,          # é€‚åº¦å¢åŠ æ¢ç´¢æ¦‚ç‡
            'epsilon_decay': 0.995,  # å‡æ…¢æ¢ç´¢è¡°å‡
            'limit': 15,             # é€‚åº¦å¢åŠ æé™å€¼
            'archive_size': 300      # é€‚åº¦å¢åŠ å½’æ¡£å¤§å°
        })
    }
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_scale_results = {}
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs('results', exist_ok=True)
    
    # å¯¹æ¯ä¸ªè§„æ¨¡é…ç½®è¿è¡Œå®éªŒ
    for config in selected_configs:
        scale = config['scale']
        print(f"\n{'='*60}")
        print(f"å®éªŒè§„æ¨¡: {scale}")
        print(f"{'='*60}")
        
        # ç”Ÿæˆé—®é¢˜æ•°æ®
        problem_data = generate_heterogeneous_problem_data(config)
        
        # æ‰“å°è§„æ¨¡è¯¦ç»†ä¿¡æ¯
        print_scale_details(config, problem_data)
        
        # å­˜å‚¨è¯¥è§„æ¨¡çš„ç»“æœ
        all_scale_results[scale] = {}
        
        # è¿è¡Œæ¯ä¸ªç®—æ³•
        for algorithm_name, (algorithm_class, algorithm_params) in algorithms.items():
            print(f"\nè¿è¡Œç®—æ³•: {algorithm_name}")
            print("-" * 40)
            
            try:
                result = run_single_experiment(
                    problem_data,
                    algorithm_name, 
                    algorithm_class, 
                    algorithm_params,
                    runs=2
                )
                
                all_scale_results[scale][algorithm_name] = result
                
                # æ‰“å°åŸºæœ¬ç»“æœ
                print(f"  æœ€ä¼˜å®Œå·¥æ—¶é—´: {result['makespan_best']:.2f}")
                print(f"  æœ€ä¼˜æ€»æ‹–æœŸ: {result['tardiness_best']:.2f}")
                print(f"  æœ€å·®å®Œå·¥æ—¶é—´: {result['max_makespan']:.2f}")
                print(f"  æœ€å·®æ€»æ‹–æœŸ: {result['max_tardiness']:.2f}")
                print(f"  è¶…ä½“ç§¯: {result['hypervolume']:.4f}")
                print(f"  IGD: {result['igd']:.4f}")
                print(f"  GD: {result['gd']:.4f}")
                print(f"  åˆ†å¸ƒæ€§: {result['spread']:.4f}")
                print(f"  RAæŒ‡æ ‡: {result['ra']:.4f}")
                print(f"  å¸•ç´¯æ‰˜è§£æ•°é‡: {result['pareto_count']}")
                print(f"  å¹³å‡è¿è¡Œæ—¶é—´: {result['runtime']:.2f}ç§’")
            except Exception as e:
                print(f"  âŒ ç®—æ³• {algorithm_name} è¿è¡Œå¤±è´¥: {str(e)}")
                traceback.print_exc()
                # è®¾ç½®é»˜è®¤å¤±è´¥ç»“æœ
                all_scale_results[scale][algorithm_name] = {
                    'makespan_best': float('inf'),
                    'tardiness_best': float('inf'),
                    'weighted_best': float('inf'),
                    'max_makespan': 0,
                    'max_tardiness': 0,
                    'min_makespan': float('inf'),
                    'min_tardiness': float('inf'),
                    'makespan_mean': 0,
                    'tardiness_mean': 0,
                    'weighted_mean': 0,
                    'runtime': 0,
                    'hypervolume': 0.0,
                    'igd': float('inf'),
                    'gd': float('inf'),
                    'spread': 1.0,
                    'ra': 0.0,
                    'pareto_count': 0,
                    'pareto_solutions': []
                }
        
        # ç»˜åˆ¶è¯¥è§„æ¨¡çš„å¸•ç´¯æ‰˜å‰æ²¿å¯¹æ¯”å›¾ - å¢å¼ºç‰ˆæœ¬
        plot_pareto_comparison(all_scale_results[scale], scale)
        
        # æŒ‰ç…§è®ºæ–‡è¦æ±‚ï¼šå…ˆå½’ä¸€åŒ–ç›®æ ‡å€¼ï¼Œå†è®¡ç®—ç»„åˆå‰æ²¿å’ŒæŒ‡æ ‡
        print(f"\nğŸ”„ æŒ‰ç…§è®ºæ–‡æ ‡å‡†é‡æ–°è®¡ç®—{scale}çš„æ‰€æœ‰æŒ‡æ ‡...")
        
        # 1. å¯¹æ‰€æœ‰ç®—æ³•çš„ç›®æ ‡å€¼è¿›è¡Œå½’ä¸€åŒ–ï¼ˆé¿å…ä¸åŒé‡çº²å½±å“ï¼‰
        normalized_results_for_scale, norm_params = normalize_objectives(all_scale_results[scale])
        print(f"  âœ“ ç›®æ ‡å€¼å½’ä¸€åŒ–å®Œæˆ")
        print(f"    MakespanèŒƒå›´: [{norm_params[0]:.1f}, {norm_params[1]:.1f}]")
        print(f"    TardinessèŒƒå›´: [{norm_params[2]:.1f}, {norm_params[3]:.1f}]")
        
        # 2. åŸºäºå½’ä¸€åŒ–åçš„ç›®æ ‡å€¼è®¡ç®—ç»„åˆå¸•ç´¯æ‰˜å‰æ²¿ï¼ˆçœŸå®å‰æ²¿PF*ï¼‰
        combined_pareto_front = calculate_combined_pareto_front(normalized_results_for_scale)
        print(f"  âœ“ ç»„åˆå¸•ç´¯æ‰˜å‰æ²¿åŒ…å«{len(combined_pareto_front)}ä¸ªå½’ä¸€åŒ–ç‚¹")
        
        # 3. æ”¶é›†æ‰€æœ‰ç®—æ³•çš„è§£é›†ç”¨äºç»Ÿä¸€å‚è€ƒç‚¹è®¡ç®—
        all_algorithm_solutions = []
        for algorithm_name in all_scale_results[scale]:
            if (algorithm_name in normalized_results_for_scale and 
                'normalized_pareto_solutions' in normalized_results_for_scale[algorithm_name] and 
                normalized_results_for_scale[algorithm_name]['normalized_pareto_solutions']):
                all_algorithm_solutions.extend(all_scale_results[scale][algorithm_name]['pareto_solutions'])
        
        # 4. é‡æ–°è®¡ç®—æ¯ä¸ªç®—æ³•çš„æ‰€æœ‰æŒ‡æ ‡ï¼ˆåŸºäºå½’ä¸€åŒ–åçš„ç›®æ ‡å€¼ï¼‰
        for algorithm_name in all_scale_results[scale]:
            if (algorithm_name in normalized_results_for_scale and 
                'normalized_pareto_solutions' in normalized_results_for_scale[algorithm_name] and 
                normalized_results_for_scale[algorithm_name]['normalized_pareto_solutions']):
                
                norm_solutions = normalized_results_for_scale[algorithm_name]['normalized_pareto_solutions']
                original_solutions = all_scale_results[scale][algorithm_name]['pareto_solutions']
                
                # é‡æ–°è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
                new_hypervolume = calculate_hypervolume(original_solutions, normalize=True, all_algorithm_solutions=all_algorithm_solutions)  # HVç”¨åŸå§‹å€¼è®¡ç®—å¹¶å½’ä¸€åŒ–ï¼Œä½¿ç”¨ç»Ÿä¸€å‚è€ƒç‚¹
                new_igd = calculate_igd(norm_solutions, combined_pareto_front)  # IGDç”¨å½’ä¸€åŒ–å€¼å’Œç»„åˆå‰æ²¿
                new_gd = calculate_gd(norm_solutions, combined_pareto_front)   # GDç”¨å½’ä¸€åŒ–å€¼å’Œç»„åˆå‰æ²¿
                new_spread = calculate_maximum_spread(norm_solutions)         # MSç”¨å½’ä¸€åŒ–å€¼
                new_ra = calculate_ra(norm_solutions, combined_pareto_front)   # RAæŒ‡æ ‡ï¼šç®—æ³•è§£é›†ä¸å‚è€ƒå‰æ²¿çš„é‡åˆåº¦
                
                # å¤„ç†æ— æ•ˆå€¼
                if new_igd == float('inf') or np.isnan(new_igd):
                    new_igd = 1.0  # è®¾ä¸ºè¾ƒå¤§å€¼è¡¨ç¤ºæ€§èƒ½å·®
                if new_gd == float('inf') or np.isnan(new_gd):
                    new_gd = 1.0   # è®¾ä¸ºè¾ƒå¤§å€¼è¡¨ç¤ºæ€§èƒ½å·®
                if np.isnan(new_spread):
                    new_spread = 1.0  # è®¾ä¸ºè¾ƒå¤§å€¼è¡¨ç¤ºåˆ†å¸ƒæ€§å·®
                if np.isnan(new_ra):
                    new_ra = 0.0  # è®¾ä¸º0è¡¨ç¤ºæ²¡æœ‰æ‰¾åˆ°çœŸå®å¸•ç´¯æ‰˜è§£
                    
                    # æ›´æ–°ç»“æœ
            all_scale_results[scale][algorithm_name]['hypervolume'] = new_hypervolume
            all_scale_results[scale][algorithm_name]['igd'] = new_igd
            all_scale_results[scale][algorithm_name]['gd'] = new_gd
            all_scale_results[scale][algorithm_name]['spread'] = new_spread
            all_scale_results[scale][algorithm_name]['ra'] = new_ra
            
            print(f"  {algorithm_name}: HV={new_hypervolume:.4f}, IGD={new_igd:.4f}, GD={new_gd:.4f}, Spread={new_spread:.4f}, RA={new_ra:.4f}")
        
        print(f"\nâœ… {scale} å®éªŒå®Œæˆï¼Œå¸•ç´¯æ‰˜å‰æ²¿å¯¹æ¯”å›¾å·²ä¿å­˜")
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print(f"\n{'='*80}")
    print("ç”Ÿæˆç»¼åˆå¯¹æ¯”æŠ¥å‘Š...")
    print(f"{'='*80}")
    
    generate_specific_scale_report(all_scale_results, selected_configs)
    
    print("\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼")
    print("ğŸ“Š ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ° results/ ç›®å½•")
    print("ğŸ“ˆ å¸•ç´¯æ‰˜å‰æ²¿å¯¹æ¯”å›¾å·²ç”Ÿæˆ")

def create_custom_scales_config():
    """
    åˆ›å»ºè‡ªå®šä¹‰è§„æ¨¡é…ç½®çš„è¾…åŠ©å‡½æ•°
    
    Returns:
        List[Dict]: è‡ªå®šä¹‰è§„æ¨¡é…ç½®åˆ—è¡¨
    """
    print("=" * 60)
    print("è‡ªå®šä¹‰è§„æ¨¡é…ç½®æŒ‡å—")
    print("=" * 60)
    print("è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å®šä¹‰æ‚¨çš„ä¸‰ä¸ªè§„æ¨¡é…ç½®ï¼š")
    print("æ¯ä¸ªè§„æ¨¡éœ€è¦åŒ…å«ä»¥ä¸‹å‚æ•°ï¼š")
    print("  - n_jobs: å·¥ä»¶æ•°é‡")
    print("  - n_stages: é˜¶æ®µæ•°é‡") 
    print("  - n_factories: å·¥å‚æ•°é‡")
    print("  - name: è§„æ¨¡åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›å°†è‡ªåŠ¨ç”Ÿæˆï¼‰")
    print()
    print("ç¤ºä¾‹é…ç½®ï¼š")
    print("custom_scales = [")
    print("    {'n_jobs': 30, 'n_stages': 3, 'n_factories': 2, 'name': 'å°è§„æ¨¡'},")
    print("    {'n_jobs': 60, 'n_stages': 4, 'n_factories': 3, 'name': 'ä¸­è§„æ¨¡'},")
    print("    {'n_jobs': 100, 'n_stages': 5, 'n_factories': 4, 'name': 'å¤§è§„æ¨¡'}")
    print("]")
    print()
    
    # è¿”å›é»˜è®¤é…ç½®ä½œä¸ºå‚è€ƒ
    return [
        {'n_jobs': 30, 'n_stages': 3, 'n_factories': 2, 'name': 'å°è§„æ¨¡'},
        {'n_jobs': 60, 'n_stages': 4, 'n_factories': 3, 'name': 'ä¸­è§„æ¨¡'},
        {'n_jobs': 100, 'n_stages': 5, 'n_factories': 4, 'name': 'å¤§è§„æ¨¡'}
    ]

def generate_specific_scale_report(results: Dict, configs: List[Dict]):
    """ç”Ÿæˆç‰¹å®šè§„æ¨¡çš„è¡¨æ ¼æ ¼å¼æŠ¥å‘Š"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"results/ç‰¹å®šè§„æ¨¡ç®—æ³•å¯¹æ¯”æŠ¥å‘Š_{timestamp}.txt"
    
    os.makedirs("results", exist_ok=True)
    
    algorithm_list = ['RL-Chaotic-HHO', 'I-NSGA-II', 'MOPSO', 'MODE', 'DQN', 'QL-ABC']
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¯¹æ¯ä¸ªè§„æ¨¡çš„ç»“æœè¿›è¡Œå½’ä¸€åŒ–å¤„ç†
    normalized_results = {}
    for scale, scale_results in results.items():
        if scale_results:  # ç¡®ä¿æœ‰ç»“æœ
            normalized_results[scale] = normalize_metrics(scale_results)
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("ç‰¹å®šè§„æ¨¡ç®—æ³•å¯¹æ¯”å®éªŒæŠ¥å‘Š\n")
        f.write("=" * 100 + "\n\n")
        
        # å®éªŒé…ç½®ä¿¡æ¯
        f.write("å®éªŒé…ç½®:\n")
        f.write(f"è§„æ¨¡: {len(configs)}ä¸ªè§„æ¨¡é…ç½®\n")
        f.write(f"æ¯ä¸ªé˜¶æ®µæœºå™¨æ•°: (2,3,4,5)èŒƒå›´å†…\n")
        f.write(f"å¹¶è¡Œæœºæ•°é‡: éšè§„æ¨¡å¢å¤§è€Œå¢å¤š\n")
        f.write(f"å¯¹æ¯”ç®—æ³•: {', '.join(algorithm_list)}\n")
        f.write(f"æ¯ä¸ªç®—æ³•è¿è¡Œæ¬¡æ•°: 3æ¬¡\n")
        f.write(f"ç§ç¾¤å¤§å°: 100\n")
        f.write(f"è¿­ä»£æ¬¡æ•°: 100\n")
        f.write(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # è§„æ¨¡è¯¦æƒ…
        f.write("è§„æ¨¡è¯¦æƒ…:\n")
        for config in configs:
            f.write(f"  {config['scale']}: {config['n_jobs']}ä½œä¸š, {config['n_factories']}å·¥å‚, {config['n_stages']}é˜¶æ®µ\n")
            f.write(f"    æœºå™¨é…ç½®: {config['machines_per_stage']}\n")
            f.write(f"    å¼‚æ„æœºå™¨é…ç½®: {config['heterogeneous_machines']}\n")
        f.write("\n")
        
        # å„é¡¹æŒ‡æ ‡å¯¹æ¯”è¡¨
        f.write("å„é¡¹æŒ‡æ ‡å¯¹æ¯”è¡¨\n")
        f.write("=" * 100 + "\n\n")
        
        # 1. å®Œå·¥æ—¶é—´å¯¹æ¯”è¡¨
        f.write("1. å®Œå·¥æ—¶é—´(Makespan)å¯¹æ¯”è¡¨\n")
        f.write("-" * 100 + "\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        f.write(f"| {'è§„æ¨¡':^13s} | {'RL-Chaotic-HHO':^13s} | {'I-NSGA-II':^11s} | {'MOPSO':^11s} | {'MODE':^11s} | {'DQN':^8s} | {'QL-ABC':^10s} |\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        
        for config in configs:
            scale = config['scale']
            if scale in results:
                scale_results = results[scale]
                
                values = []
                for alg in algorithm_list:
                    if alg in scale_results:
                        value = scale_results[alg].get('makespan_best', 'å¤±è´¥')
                        if value == float('inf') or value == 0:
                            values.append('å¤±è´¥')
                        else:
                            values.append(f"{value:.1f}")
                    else:
                        values.append('å¤±è´¥')
                
                f.write(f"| {scale:^13s} | {values[0]:^13s} | {values[1]:^11s} | {values[2]:^11s} | {values[3]:^11s} | {values[4]:^8s} | {values[5]:^10s} |\n")
        
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n\n")
        
        # 2. æ€»æ‹–æœŸå¯¹æ¯”è¡¨
        f.write("2. æ€»æ‹–æœŸ(Total Tardiness)å¯¹æ¯”è¡¨\n")
        f.write("-" * 100 + "\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        f.write(f"| {'è§„æ¨¡':^13s} | {'RL-Chaotic-HHO':^13s} | {'I-NSGA-II':^11s} | {'MOPSO':^11s} | {'MODE':^11s} | {'DQN':^8s} | {'QL-ABC':^10s} |\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        
        for config in configs:
            scale = config['scale']
            if scale in results:
                scale_results = results[scale]
                
                values = []
                for alg in algorithm_list:
                    if alg in scale_results:
                        value = scale_results[alg].get('tardiness_best', 'å¤±è´¥')
                        if value == float('inf') or (isinstance(value, (int, float)) and value < 0):
                            values.append('å¤±è´¥')
                        else:
                            values.append(f"{value:.1f}")
                    else:
                        values.append('å¤±è´¥')
                
                f.write(f"| {scale:^13s} | {values[0]:^13s} | {values[1]:^11s} | {values[2]:^11s} | {values[3]:^11s} | {values[4]:^8s} | {values[5]:^10s} |\n")
        
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n\n")
        
        # 3. åŠ æƒç›®æ ‡å¯¹æ¯”è¡¨
        f.write("3. åŠ æƒç›®æ ‡å‡½æ•°å¯¹æ¯”è¡¨\n")
        f.write("-" * 100 + "\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        f.write(f"| {'è§„æ¨¡':^13s} | {'RL-Chaotic-HHO':^13s} | {'I-NSGA-II':^11s} | {'MOPSO':^11s} | {'MODE':^11s} | {'DQN':^8s} | {'QL-ABC':^10s} |\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        
        for config in configs:
            scale = config['scale']
            if scale in results:
                scale_results = results[scale]
                
                values = []
                for alg in algorithm_list:
                    if alg in scale_results:
                        value = scale_results[alg].get('weighted_best', 'å¤±è´¥')
                        if value == float('inf') or value == 0:
                            values.append('å¤±è´¥')
                        else:
                            values.append(f"{value:.1f}")
                    else:
                        values.append('å¤±è´¥')
                
                f.write(f"| {scale:^13s} | {values[0]:^13s} | {values[1]:^11s} | {values[2]:^11s} | {values[3]:^11s} | {values[4]:^8s} | {values[5]:^10s} |\n")
        
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n\n")
        
        # 4. è¶…ä½“ç§¯æŒ‡æ ‡å¯¹æ¯”è¡¨
        f.write("4. è¶…ä½“ç§¯(HV)æŒ‡æ ‡å¯¹æ¯”è¡¨\n")
        f.write("-" * 100 + "\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        f.write(f"| {'è§„æ¨¡':^13s} | {'RL-Chaotic-HHO':^13s} | {'I-NSGA-II':^11s} | {'MOPSO':^11s} | {'MODE':^11s} | {'DQN':^8s} | {'QL-ABC':^10s} |\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        
        for config in configs:
            scale = config['scale']
            if scale in results:
                scale_results = results[scale]
                
                values = []
                for alg in algorithm_list:
                    if alg in scale_results:
                        value = scale_results[alg].get('hypervolume', 0)
                        if value == 0:
                            values.append('0')
                        else:
                            values.append(f"{value:.4f}")
                    else:
                        values.append('0')
                
                f.write(f"| {scale:^13s} | {values[0]:^13s} | {values[1]:^11s} | {values[2]:^11s} | {values[3]:^11s} | {values[4]:^8s} | {values[5]:^10s} |\n")
        
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n\n")
        
        # 5. IGDæŒ‡æ ‡å¯¹æ¯”è¡¨
        f.write("5. åä¸–ä»£è·ç¦»(IGD)æŒ‡æ ‡å¯¹æ¯”è¡¨\n")
        f.write("-" * 100 + "\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        f.write(f"| {'è§„æ¨¡':^13s} | {'RL-Chaotic-HHO':^13s} | {'I-NSGA-II':^11s} | {'MOPSO':^11s} | {'MODE':^11s} | {'DQN':^8s} | {'QL-ABC':^10s} |\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        
        for config in configs:
            scale = config['scale']
            if scale in results:
                scale_results = results[scale]
                
                values = []
                for alg in algorithm_list:
                    if alg in scale_results:
                        value = scale_results[alg].get('igd', float('inf'))
                        if value == float('inf'):
                            values.append('âˆ')
                        elif value < 1e-6:
                            # å¯¹äºæå°å€¼ï¼Œä½¿ç”¨ç§‘å­¦è®°æ•°æ³•æ˜¾ç¤º
                            values.append(f"{value:.2e}")
                        else:
                            values.append(f"{value:.2f}")
                    else:
                        values.append('âˆ')
                
                f.write(f"| {scale:^13s} | {values[0]:^13s} | {values[1]:^11s} | {values[2]:^11s} | {values[3]:^11s} | {values[4]:^8s} | {values[5]:^10s} |\n")
        
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n\n")
        
        # 6. RAæŒ‡æ ‡å¯¹æ¯”è¡¨
        f.write("6. å¸•ç´¯æ‰˜æœ€ä¼˜è§£æ¯”ç‡(RA)æŒ‡æ ‡å¯¹æ¯”è¡¨\n")
        f.write("-" * 100 + "\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        f.write(f"| {'è§„æ¨¡':^13s} | {'RL-Chaotic-HHO':^13s} | {'I-NSGA-II':^11s} | {'MOPSO':^11s} | {'MODE':^11s} | {'DQN':^8s} | {'QL-ABC':^10s} |\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        
        for config in configs:
            scale = config['scale']
            if scale in results:
                scale_results = results[scale]
                
                values = []
                for alg in algorithm_list:
                    if alg in scale_results:
                        value = scale_results[alg].get('ra', 0.0)
                        if value < 0 or np.isnan(value):
                            values.append('å¤±è´¥')
                        else:
                            values.append(f"{value:.3f}")
                    else:
                        values.append('è¾ƒå·®')
                
                f.write(f"| {scale:^13s} | {values[0]:^13s} | {values[1]:^11s} | {values[2]:^11s} | {values[3]:^11s} | {values[4]:^8s} | {values[5]:^10s} |\n")
        
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n\n")
        
        # 7. è¿è¡Œæ—¶é—´å¯¹æ¯”è¡¨
        f.write("7. è¿è¡Œæ—¶é—´(ç§’)å¯¹æ¯”è¡¨\n")
        f.write("-" * 100 + "\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        f.write(f"| {'è§„æ¨¡':^13s} | {'RL-Chaotic-HHO':^13s} | {'I-NSGA-II':^11s} | {'MOPSO':^11s} | {'MODE':^11s} | {'DQN':^8s} | {'QL-ABC':^10s} |\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        
        for config in configs:
            scale = config['scale']
            if scale in results:
                scale_results = results[scale]
                
                values = []
                for alg in algorithm_list:
                    if alg in scale_results:
                        value = scale_results[alg].get('runtime', 0)
                        values.append(f"{value:.2f}")
                    else:
                        values.append('å¤±è´¥')
                
                f.write(f"| {scale:^13s} | {values[0]:^13s} | {values[1]:^11s} | {values[2]:^11s} | {values[3]:^11s} | {values[4]:^8s} | {values[5]:^10s} |\n")
        
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n\n")
        
        # 8. å¸•ç´¯æ‰˜è§£æ•°é‡å¯¹æ¯”è¡¨
        f.write("8. å¸•ç´¯æ‰˜è§£æ•°é‡å¯¹æ¯”è¡¨\n")
        f.write("-" * 100 + "\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        f.write(f"| {'è§„æ¨¡':^13s} | {'RL-Chaotic-HHO':^13s} | {'I-NSGA-II':^11s} | {'MOPSO':^11s} | {'MODE':^11s} | {'DQN':^8s} | {'QL-ABC':^10s} |\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        
        for config in configs:
            scale = config['scale']
            if scale in results:
                scale_results = results[scale]
                
                values = []
                for alg in algorithm_list:
                    if alg in scale_results:
                        value = scale_results[alg].get('pareto_count', 0)
                        values.append(f"{value}")
                    else:
                        values.append('0')
                
                f.write(f"| {scale:^13s} | {values[0]:^13s} | {values[1]:^11s} | {values[2]:^11s} | {values[3]:^11s} | {values[4]:^8s} | {values[5]:^10s} |\n")
        
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n\n")
        
        # 9. å½’ä¸€åŒ–æŒ‡æ ‡å¯¹æ¯”è¡¨
        f.write("9. å½’ä¸€åŒ–æŒ‡æ ‡å¯¹æ¯”è¡¨\n")
        f.write("=" * 100 + "\n\n")
        
        # 9.1 å½’ä¸€åŒ–è¶…ä½“ç§¯æŒ‡æ ‡
        f.write("9.1 å½’ä¸€åŒ–è¶…ä½“ç§¯(HV)æŒ‡æ ‡å¯¹æ¯”è¡¨\n")
        f.write("-" * 100 + "\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        f.write(f"| {'è§„æ¨¡':^13s} | {'RL-Chaotic-HHO':^13s} | {'I-NSGA-II':^11s} | {'MOPSO':^11s} | {'MODE':^11s} | {'DQN':^8s} | {'QL-ABC':^10s} |\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        
        for config in configs:
            scale = config['scale']
            if scale in normalized_results:
                scale_results = normalized_results[scale]
                
                values = []
                for alg in algorithm_list:
                    if alg in scale_results:
                        value = scale_results[alg].get('norm_hypervolume', 0)
                        values.append(f"{value:.4f}")
                    else:
                        values.append('0.0000')
                
                f.write(f"| {scale:^13s} | {values[0]:^13s} | {values[1]:^11s} | {values[2]:^11s} | {values[3]:^11s} | {values[4]:^8s} | {values[5]:^10s} |\n")
        
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n\n")
        
        # 9.2 å½’ä¸€åŒ–IGDæŒ‡æ ‡
        f.write("9.2 å½’ä¸€åŒ–IGDæŒ‡æ ‡å¯¹æ¯”è¡¨\n")
        f.write("-" * 100 + "\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        f.write(f"| {'è§„æ¨¡':^13s} | {'RL-Chaotic-HHO':^13s} | {'I-NSGA-II':^11s} | {'MOPSO':^11s} | {'MODE':^11s} | {'DQN':^8s} | {'QL-ABC':^10s} |\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        
        for config in configs:
            scale = config['scale']
            if scale in normalized_results:
                scale_results = normalized_results[scale]
                
                values = []
                for alg in algorithm_list:
                    if alg in scale_results:
                        value = scale_results[alg].get('norm_igd', 0)
                        values.append(f"{value:.4f}")
                    else:
                        values.append('0.0000')
                
                f.write(f"| {scale:^13s} | {values[0]:^13s} | {values[1]:^11s} | {values[2]:^11s} | {values[3]:^11s} | {values[4]:^8s} | {values[5]:^10s} |\n")
        
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n\n")
        
        # 9.3 å½’ä¸€åŒ–GDæŒ‡æ ‡
        f.write("9.3 å½’ä¸€åŒ–GDæŒ‡æ ‡å¯¹æ¯”è¡¨\n")
        f.write("-" * 100 + "\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        f.write(f"| {'è§„æ¨¡':^13s} | {'RL-Chaotic-HHO':^13s} | {'I-NSGA-II':^11s} | {'MOPSO':^11s} | {'MODE':^11s} | {'DQN':^8s} | {'QL-ABC':^10s} |\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        
        for config in configs:
            scale = config['scale']
            if scale in normalized_results:
                scale_results = normalized_results[scale]
                
                values = []
                for alg in algorithm_list:
                    if alg in scale_results:
                        value = scale_results[alg].get('norm_gd', 0)
                        values.append(f"{value:.4f}")
                    else:
                        values.append('0.0000')
                
                f.write(f"| {scale:^13s} | {values[0]:^13s} | {values[1]:^11s} | {values[2]:^11s} | {values[3]:^11s} | {values[4]:^8s} | {values[5]:^10s} |\n")
        
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n\n")
        
        # 9.4 å½’ä¸€åŒ–SpreadæŒ‡æ ‡
        f.write("9.4 å½’ä¸€åŒ–SpreadæŒ‡æ ‡å¯¹æ¯”è¡¨\n")
        f.write("-" * 100 + "\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        f.write(f"| {'è§„æ¨¡':^13s} | {'RL-Chaotic-HHO':^13s} | {'I-NSGA-II':^11s} | {'MOPSO':^11s} | {'MODE':^11s} | {'DQN':^8s} | {'QL-ABC':^10s} |\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        
        for config in configs:
            scale = config['scale']
            if scale in normalized_results:
                scale_results = normalized_results[scale]
                
                values = []
                for alg in algorithm_list:
                    if alg in scale_results:
                        value = scale_results[alg].get('norm_spread', 0)
                        values.append(f"{value:.4f}")
                    else:
                        values.append('0.0000')
                
                f.write(f"| {scale:^13s} | {values[0]:^13s} | {values[1]:^11s} | {values[2]:^11s} | {values[3]:^11s} | {values[4]:^8s} | {values[5]:^10s} |\n")
        
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n\n")
        
        # 9.5 å½’ä¸€åŒ–RAæŒ‡æ ‡
        f.write("9.5 å½’ä¸€åŒ–RAæŒ‡æ ‡å¯¹æ¯”è¡¨\n")
        f.write("-" * 100 + "\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        f.write(f"| {'è§„æ¨¡':^13s} | {'RL-Chaotic-HHO':^13s} | {'I-NSGA-II':^11s} | {'MOPSO':^11s} | {'MODE':^11s} | {'DQN':^8s} | {'QL-ABC':^10s} |\n")
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n")
        
        for config in configs:
            scale = config['scale']
            if scale in normalized_results:
                scale_results = normalized_results[scale]
                
                values = []
                for alg in algorithm_list:
                    if alg in scale_results:
                        value = scale_results[alg].get('norm_ra', 0)
                        values.append(f"{value:.4f}")
                    else:
                        values.append('0.0000')
                
                f.write(f"| {scale:^13s} | {values[0]:^13s} | {values[1]:^11s} | {values[2]:^11s} | {values[3]:^11s} | {values[4]:^8s} | {values[5]:^10s} |\n")
        
        f.write("+" + "-"*15 + "+" + "-"*15 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*13 + "+" + "-"*10 + "+" + "-"*12 + "+\n\n")
        
        # æ€»ç»“
        f.write("å®éªŒæ€»ç»“\n")
        f.write("=" * 100 + "\n")
        f.write(f"æœ¬å®éªŒå¯¹æ¯”äº†6ç§ç®—æ³•åœ¨{len(configs)}ä¸ªç‰¹å®šè§„æ¨¡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚\n")
        f.write("è§„æ¨¡é…ç½®ï¼š\n")
        for config in configs:
            f.write(f"- {config['scale']}: {config['n_jobs']}ä¸ªä½œä¸šï¼Œ{config['n_stages']}ä¸ªé˜¶æ®µï¼Œ{config['n_factories']}ä¸ªå·¥å‚\n")
        f.write("æ¯ä¸ªé˜¶æ®µçš„æœºå™¨æ•°åœ¨(2,3,4,5)èŒƒå›´å†…ï¼Œå¹¶è¡Œæœºæ•°é‡éšè§„æ¨¡å¢å¤§è€Œå¢å¤šã€‚\n")
        f.write("æ‰€æœ‰ç®—æ³•å‡é‡‡ç”¨ç›¸åŒçš„ç§ç¾¤å¤§å°(100)å’Œè¿­ä»£æ¬¡æ•°(100)ç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚\n")
        f.write("è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ï¼šHVã€IGDã€GDã€Spreadã€RAäº”ä¸ªæ ¸å¿ƒæŒ‡æ ‡ã€‚\n")
        f.write("æŒ‰ç…§å­¦æœ¯è®ºæ–‡æ ‡å‡†è®¡ç®—æ–¹å¼ï¼š\n")
        f.write("1. æ‰€æœ‰ç®—æ³•çš„ç›®æ ‡å€¼å…ˆè¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼ˆé¿å…ä¸åŒé‡çº²å½±å“ï¼‰\n")
        f.write("2. åŸºäºå½’ä¸€åŒ–ç›®æ ‡å€¼è®¡ç®—ç»„åˆå¸•ç´¯æ‰˜å‰æ²¿ä½œä¸ºçœŸå®å‰æ²¿PF*\n")
        f.write("3. åŸºäºå½’ä¸€åŒ–ç›®æ ‡å€¼å’Œç»„åˆå‰æ²¿è®¡ç®—å„é¡¹æŒ‡æ ‡\n")
        f.write("HVï¼ˆè¶…ä½“ç§¯ï¼‰ï¼šè¶Šå¤§è¶Šå¥½ï¼Œå·²å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´ã€‚\n")
        f.write("IGDï¼ˆåå‘ä¸–ä»£è·ç¦»ï¼‰ï¼šè¶Šå°è¶Šå¥½ï¼ŒåŸºäºå½’ä¸€åŒ–ç›®æ ‡å€¼è®¡ç®—ï¼Œ0è¡¨ç¤ºæœ€ç†æƒ³ã€‚\n")
        f.write("GDï¼ˆä¸–ä»£è·ç¦»ï¼‰ï¼šè¶Šå°è¶Šå¥½ï¼ŒåŸºäºå½’ä¸€åŒ–ç›®æ ‡å€¼è®¡ç®—ï¼Œ0è¡¨ç¤ºæœ€ç†æƒ³ã€‚\n")
        f.write("MSï¼ˆæœ€å¤§åˆ†å¸ƒæ€§ï¼‰ï¼šè¶Šå¤§è¶Šå¥½ï¼ŒåŸºäºå½’ä¸€åŒ–ç›®æ ‡å€¼è®¡ç®—ï¼Œ1è¡¨ç¤ºæœ€å¤§è¦†ç›–èŒƒå›´æœ€å¥½ã€‚\n") 
        f.write("RAï¼ˆå¸•ç´¯æ‰˜æœ€ä¼˜è§£æ¯”ç‡ï¼‰ï¼šè¶Šå¤§è¶Šå¥½ï¼Œè¡¨ç¤ºç®—æ³•æ‰¾åˆ°çœŸå®å¸•ç´¯æ‰˜æœ€ä¼˜è§£çš„æ¯”ç‡ï¼Œç†æƒ³èŒƒå›´0-1ã€‚\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    print(f"\nç‰¹å®šè§„æ¨¡ç®—æ³•å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {filename}")
    
    # ç”ŸæˆExcelè¡¨æ ¼ï¼ˆåˆ†ç¦»ç‰ˆæœ¬ï¼‰
    excel_filename = f"results/ç‰¹å®šè§„æ¨¡ç®—æ³•å¯¹æ¯”æŠ¥å‘Š_{timestamp}.xlsx"
    generate_excel_report(results, normalized_results, configs, excel_filename)

def generate_excel_report(results: Dict, normalized_results: Dict, configs: List[Dict], filename: str):
    """ç”Ÿæˆåˆ†ç¦»çš„Excelæ ¼å¼æŠ¥å‘Š - å››ä¸ªæŒ‡æ ‡å’Œä¸¤ä¸ªä¼˜åŒ–ç›®æ ‡åˆ†åˆ«ä¿å­˜"""
    
    algorithm_list = ['RL-Chaotic-HHO', 'I-NSGA-II', 'MOPSO', 'MODE', 'DQN', 'QL-ABC']
    
    # è·å–åŸºç¡€æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    base_filename = filename.replace('.xlsx', '')
    
    # 1. ç”Ÿæˆä¸¤ä¸ªä¼˜åŒ–ç›®æ ‡çš„Excelæ–‡ä»¶
    objectives_filename = f"{base_filename}_ä¼˜åŒ–ç›®æ ‡.xlsx"
    with pd.ExcelWriter(objectives_filename, engine='openpyxl') as writer:
        
        # å®Œå·¥æ—¶é—´è¡¨
        makespan_data = []
        for config in configs:
            scale = config['scale']
            if scale in results:
                row = {'è§„æ¨¡': scale}
                for alg in algorithm_list:
                    if alg in results[scale]:
                        row[alg] = results[scale][alg].get('makespan_best', 0)
                    else:
                        row[alg] = 0
                makespan_data.append(row)
        
        makespan_df = pd.DataFrame(makespan_data)
        makespan_df.to_excel(writer, sheet_name='å®Œå·¥æ—¶é—´(Makespan)', index=False)
        
        # æ€»æ‹–æœŸè¡¨
        tardiness_data = []
        for config in configs:
            scale = config['scale']
            if scale in results:
                row = {'è§„æ¨¡': scale}
                for alg in algorithm_list:
                    if alg in results[scale]:
                        row[alg] = results[scale][alg].get('tardiness_best', 0)
                    else:
                        row[alg] = 0
                tardiness_data.append(row)
        
        tardiness_df = pd.DataFrame(tardiness_data)
        tardiness_df.to_excel(writer, sheet_name='æ€»æ‹–æœŸ(Total_Tardiness)', index=False)
        
        # åŠ æƒç›®æ ‡è¡¨
        weighted_data = []
        for config in configs:
            scale = config['scale']
            if scale in results:
                row = {'è§„æ¨¡': scale}
                for alg in algorithm_list:
                    if alg in results[scale]:
                        row[alg] = results[scale][alg].get('weighted_best', 0)
                    else:
                        row[alg] = 0
                weighted_data.append(row)
        
        weighted_df = pd.DataFrame(weighted_data)
        weighted_df.to_excel(writer, sheet_name='åŠ æƒç›®æ ‡', index=False)
        
        # å¸•ç´¯æ‰˜è§£æ•°é‡è¡¨
        pareto_count_data = []
        for config in configs:
            scale = config['scale']
            if scale in results:
                row = {'è§„æ¨¡': scale}
                for alg in algorithm_list:
                    if alg in results[scale]:
                        row[alg] = results[scale][alg].get('pareto_count', 0)
                    else:
                        row[alg] = 0
                pareto_count_data.append(row)
        
        pareto_count_df = pd.DataFrame(pareto_count_data)
        pareto_count_df.to_excel(writer, sheet_name='å¸•ç´¯æ‰˜è§£æ•°é‡', index=False)
    
    print(f"ä¼˜åŒ–ç›®æ ‡ExcelæŠ¥å‘Šå·²ä¿å­˜: {objectives_filename}")
    
    # 2. ç”Ÿæˆå››ä¸ªå½’ä¸€åŒ–æŒ‡æ ‡çš„Excelæ–‡ä»¶
    metrics_filename = f"{base_filename}_å½’ä¸€åŒ–æŒ‡æ ‡.xlsx"
    with pd.ExcelWriter(metrics_filename, engine='openpyxl') as writer:
        
        # å½’ä¸€åŒ–è¶…ä½“ç§¯è¡¨
        hv_data = []
        for config in configs:
            scale = config['scale']
            if scale in normalized_results:
                row = {'è§„æ¨¡': scale}
                for alg in algorithm_list:
                    if alg in normalized_results[scale]:
                        row[alg] = normalized_results[scale][alg].get('norm_hypervolume', 0)
                    else:
                        row[alg] = 0
                hv_data.append(row)
        
        hv_df = pd.DataFrame(hv_data)
        hv_df.to_excel(writer, sheet_name='å½’ä¸€åŒ–è¶…ä½“ç§¯(HV)', index=False)
        
        # å½’ä¸€åŒ–IGDè¡¨
        igd_data = []
        for config in configs:
            scale = config['scale']
            if scale in normalized_results:
                row = {'è§„æ¨¡': scale}
                for alg in algorithm_list:
                    if alg in normalized_results[scale]:
                        row[alg] = normalized_results[scale][alg].get('norm_igd', 0)
                    else:
                        row[alg] = 0
                igd_data.append(row)
        
        igd_df = pd.DataFrame(igd_data)
        igd_df.to_excel(writer, sheet_name='å½’ä¸€åŒ–IGD', index=False)
        
        # å½’ä¸€åŒ–GDè¡¨
        gd_data = []
        for config in configs:
            scale = config['scale']
            if scale in normalized_results:
                row = {'è§„æ¨¡': scale}
                for alg in algorithm_list:
                    if alg in normalized_results[scale]:
                        row[alg] = normalized_results[scale][alg].get('norm_gd', 0)
                    else:
                        row[alg] = 0
                gd_data.append(row)
        
        gd_df = pd.DataFrame(gd_data)
        gd_df.to_excel(writer, sheet_name='å½’ä¸€åŒ–GD', index=False)
        
        # å½’ä¸€åŒ–åˆ†å¸ƒæ€§è¡¨
        spread_data = []
        for config in configs:
            scale = config['scale']
            if scale in normalized_results:
                row = {'è§„æ¨¡': scale}
                for alg in algorithm_list:
                    if alg in normalized_results[scale]:
                        row[alg] = normalized_results[scale][alg].get('norm_spread', 0)
                    else:
                        row[alg] = 0
                spread_data.append(row)
        
        spread_df = pd.DataFrame(spread_data)
        spread_df.to_excel(writer, sheet_name='å½’ä¸€åŒ–åˆ†å¸ƒæ€§(Spread)', index=False)
        
        # å½’ä¸€åŒ–RAæŒ‡æ ‡è¡¨
        ra_data = []
        for config in configs:
            scale = config['scale']
            if scale in normalized_results:
                row = {'è§„æ¨¡': scale}
                for alg in algorithm_list:
                    if alg in normalized_results[scale]:
                        row[alg] = normalized_results[scale][alg].get('norm_ra', 0)
                    else:
                        row[alg] = 0
                ra_data.append(row)
        
        ra_df = pd.DataFrame(ra_data)
        ra_df.to_excel(writer, sheet_name='å½’ä¸€åŒ–RAæŒ‡æ ‡', index=False)
        
        # åŸå§‹æŒ‡æ ‡å€¼è¡¨ï¼ˆä¾›å‚è€ƒï¼‰
        original_data = []
        for config in configs:
            scale = config['scale']
            if scale in results:
                row = {'è§„æ¨¡': scale}
                for alg in algorithm_list:
                    if alg in results[scale]:
                        row[f'{alg}_HV'] = results[scale][alg].get('hypervolume', 0)
                        row[f'{alg}_IGD'] = results[scale][alg].get('igd', float('inf'))
                        row[f'{alg}_GD'] = results[scale][alg].get('gd', float('inf'))
                        row[f'{alg}_Spread'] = results[scale][alg].get('spread', 0)
                        row[f'{alg}_RA'] = results[scale][alg].get('ra', 0)
                    else:
                        row[f'{alg}_HV'] = 0
                        row[f'{alg}_IGD'] = float('inf')
                        row[f'{alg}_GD'] = float('inf')
                        row[f'{alg}_Spread'] = 0
                        row[f'{alg}_RA'] = 0
                original_data.append(row)
        
        original_df = pd.DataFrame(original_data)
        original_df.to_excel(writer, sheet_name='åŸå§‹æŒ‡æ ‡å€¼å‚è€ƒ', index=False)
    
    print(f"å½’ä¸€åŒ–æŒ‡æ ‡ExcelæŠ¥å‘Šå·²ä¿å­˜: {metrics_filename}")
    
    # 3. ç”Ÿæˆç»¼åˆç»Ÿè®¡Excelæ–‡ä»¶
    stats_filename = f"{base_filename}_ç»¼åˆç»Ÿè®¡.xlsx"
    with pd.ExcelWriter(stats_filename, engine='openpyxl') as writer:
        
        # ç®—æ³•æ€§èƒ½æ’åè¡¨
        ranking_data = []
        for alg in algorithm_list:
            alg_stats = {
                'ç®—æ³•': alg,
                'å®Œå·¥æ—¶é—´è·èƒœæ¬¡æ•°': 0,
                'æ€»æ‹–æœŸè·èƒœæ¬¡æ•°': 0,
                'HVè·èƒœæ¬¡æ•°': 0,
                'IGDè·èƒœæ¬¡æ•°': 0,
                'GDè·èƒœæ¬¡æ•°': 0,
                'Spreadè·èƒœæ¬¡æ•°': 0,
                'RAè·èƒœæ¬¡æ•°': 0,
                'å¹³å‡å¸•ç´¯æ‰˜è§£æ•°': 0
            }
            
            total_pareto_count = 0
            valid_scales = 0
            
            for config in configs:
                scale = config['scale']
                if scale in results and scale in normalized_results:
                    valid_scales += 1
                    
                    # ç»Ÿè®¡è·èƒœæ¬¡æ•°
                    if alg in results[scale]:
                        total_pareto_count += results[scale][alg].get('pareto_count', 0)
                        
                        # æ£€æŸ¥æ˜¯å¦åœ¨è¯¥è§„æ¨¡ä¸Šè·èƒœ
                        makespan_best = min(results[scale][a].get('makespan_best', float('inf')) 
                                          for a in algorithm_list if a in results[scale])
                        tardiness_best = min(results[scale][a].get('tardiness_best', float('inf'))
                                           for a in algorithm_list if a in results[scale])
                        
                        if results[scale][alg].get('makespan_best', float('inf')) == makespan_best:
                            alg_stats['å®Œå·¥æ—¶é—´è·èƒœæ¬¡æ•°'] += 1
                        if results[scale][alg].get('tardiness_best', float('inf')) == tardiness_best:
                            alg_stats['æ€»æ‹–æœŸè·èƒœæ¬¡æ•°'] += 1
                    
                    # ç»Ÿè®¡å½’ä¸€åŒ–æŒ‡æ ‡è·èƒœæ¬¡æ•°
                    if alg in normalized_results[scale]:
                        # HV: è¶Šå¤§è¶Šå¥½ï¼Œæ‰¾æœ€å¤§å€¼
                        hv_best = max(normalized_results[scale][a].get('norm_hypervolume', 0)
                                    for a in algorithm_list if a in normalized_results[scale])
                        
                        # IGD, GD: è¶Šå°è¶Šå¥½ï¼Œæ‰¾æœ€å°å€¼ï¼›MS, RA: è¶Šå¤§è¶Šå¥½ï¼Œæ‰¾æœ€å¤§å€¼
                        igd_best = min(normalized_results[scale][a].get('norm_igd', float('inf'))
                                     for a in algorithm_list if a in normalized_results[scale])
                        gd_best = min(normalized_results[scale][a].get('norm_gd', float('inf'))
                                    for a in algorithm_list if a in normalized_results[scale])
                        spread_best = max(normalized_results[scale][a].get('norm_spread', 0)
                                         for a in algorithm_list if a in normalized_results[scale])
                        ra_best = max(normalized_results[scale][a].get('norm_ra', 0.0)
                                        for a in algorithm_list if a in normalized_results[scale])
                        
                        if normalized_results[scale][alg].get('norm_hypervolume', 0) == hv_best:
                            alg_stats['HVè·èƒœæ¬¡æ•°'] += 1
                        if normalized_results[scale][alg].get('norm_igd', float('inf')) == igd_best:
                            alg_stats['IGDè·èƒœæ¬¡æ•°'] += 1
                        if normalized_results[scale][alg].get('norm_gd', float('inf')) == gd_best:
                            alg_stats['GDè·èƒœæ¬¡æ•°'] += 1
                        if normalized_results[scale][alg].get('norm_spread', float('inf')) == spread_best:
                            alg_stats['Spreadè·èƒœæ¬¡æ•°'] += 1
                        if normalized_results[scale][alg].get('norm_ra', 0.0) == ra_best:
                            alg_stats['RAè·èƒœæ¬¡æ•°'] += 1
            
            if valid_scales > 0:
                alg_stats['å¹³å‡å¸•ç´¯æ‰˜è§£æ•°'] = total_pareto_count / valid_scales
            
            ranking_data.append(alg_stats)
        
        ranking_df = pd.DataFrame(ranking_data)
        ranking_df.to_excel(writer, sheet_name='ç®—æ³•æ€§èƒ½æ’å', index=False)
        
        # è§„æ¨¡éš¾åº¦åˆ†æè¡¨
        difficulty_data = []
        for config in configs:
            scale = config['scale']
            if scale in results:
                difficulty_stats = {
                    'è§„æ¨¡': scale,
                    'å¹³å‡å®Œå·¥æ—¶é—´': 0,
                    'å¹³å‡æ€»æ‹–æœŸ': 0,
                    'å¹³å‡å¸•ç´¯æ‰˜è§£æ•°': 0,
                    'æœ€ä½³å®Œå·¥æ—¶é—´': float('inf'),
                    'æœ€ä½³æ€»æ‹–æœŸ': float('inf'),
                    'å®Œå·¥æ—¶é—´æ ‡å‡†å·®': 0,
                    'æ€»æ‹–æœŸæ ‡å‡†å·®': 0
                }
                
                makespans = []
                tardiness_vals = []
                pareto_counts = []
                
                for alg in algorithm_list:
                    if alg in results[scale]:
                        makespan = results[scale][alg].get('makespan_best', 0)
                        tardiness = results[scale][alg].get('tardiness_best', 0)
                        pareto_count = results[scale][alg].get('pareto_count', 0)
                        
                        if makespan > 0:
                            makespans.append(makespan)
                            tardiness_vals.append(tardiness)
                            pareto_counts.append(pareto_count)
                
                if makespans:
                    difficulty_stats['å¹³å‡å®Œå·¥æ—¶é—´'] = np.mean(makespans)
                    difficulty_stats['å¹³å‡æ€»æ‹–æœŸ'] = np.mean(tardiness_vals)
                    difficulty_stats['å¹³å‡å¸•ç´¯æ‰˜è§£æ•°'] = np.mean(pareto_counts)
                    difficulty_stats['æœ€ä½³å®Œå·¥æ—¶é—´'] = min(makespans)
                    difficulty_stats['æœ€ä½³æ€»æ‹–æœŸ'] = min(tardiness_vals)
                    difficulty_stats['å®Œå·¥æ—¶é—´æ ‡å‡†å·®'] = np.std(makespans)
                    difficulty_stats['æ€»æ‹–æœŸæ ‡å‡†å·®'] = np.std(tardiness_vals)
                
                difficulty_data.append(difficulty_stats)
        
        difficulty_df = pd.DataFrame(difficulty_data)
        difficulty_df.to_excel(writer, sheet_name='è§„æ¨¡éš¾åº¦åˆ†æ', index=False)
    
    print(f"ç»¼åˆç»Ÿè®¡ExcelæŠ¥å‘Šå·²ä¿å­˜: {stats_filename}")
    
    print(f"\nâœ… æ‰€æœ‰ExcelæŠ¥å‘Šç”Ÿæˆå®Œæˆ:")
    print(f"   1. ä¼˜åŒ–ç›®æ ‡: {objectives_filename}")
    print(f"   2. å½’ä¸€åŒ–æŒ‡æ ‡: {metrics_filename}")
    print(f"   3. ç»¼åˆç»Ÿè®¡: {stats_filename}")

if __name__ == "__main__":
    # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
    os.makedirs("results", exist_ok=True)
    
    print("=" * 80)
    print("ç‰¹å®šè§„æ¨¡ç®—æ³•å¯¹æ¯”å®éªŒç¨‹åº - è‡ªå®šä¹‰ç‰ˆæœ¬")
    print("=" * 80)
    print("æœ¬ç¨‹åºæ”¯æŒè‡ªå®šä¹‰ä¸‰ä¸ªè§„æ¨¡çš„é…ç½®è¿›è¡Œç®—æ³•å¯¹æ¯”å®éªŒ")
    print()
    
    # æ˜¾ç¤ºé…ç½®æŒ‡å—
    create_custom_scales_config()
    
    # è¿è¡ŒæŒ‡å®šè§„æ¨¡é…ç½®çš„å®éªŒï¼ˆå¸¦spreadæŒ‡æ ‡ï¼Œ3æ¬¡è¿è¡Œï¼‰
    print("=" * 80)
    print("è¿è¡ŒæŒ‡å®šè§„æ¨¡é…ç½®å®éªŒï¼ˆspreadæŒ‡æ ‡ï¼Œ3æ¬¡è¿è¡Œï¼‰")
    print("=" * 80)
    run_specific_scale_experiments()
    
    print("\n" + "=" * 80)
    print("å®éªŒå®Œæˆï¼")
    print("å¦‚éœ€ä½¿ç”¨å…¶ä»–è‡ªå®šä¹‰è§„æ¨¡ï¼Œè¯·ä¿®æ”¹ä»£ç ä¸­çš„ custom_scales é…ç½®")
    print("=" * 80)