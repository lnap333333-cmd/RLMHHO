#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼ºåŒ–å­¦ä¹ åè°ƒå™¨ - å®Œæ•´å®ç°
åŸºäºæ·±åº¦Qç½‘ç»œ(DQN)çš„æ™ºèƒ½ç­–ç•¥é€‰æ‹©å’Œé€‚åº”æ€§è°ƒåº¦ç³»ç»Ÿ
"""

import numpy as np
import random
import copy
from typing import List, Tuple, Dict, Optional
from collections import deque, namedtuple
import pickle
import os

from problem.mo_dhfsp import MO_DHFSP_Problem

# ç»éªŒå…ƒç»„å®šä¹‰
Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class PrioritizedReplayBuffer:
    """ä¼˜å…ˆçº§ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, capacity: int = 10000, alpha: float = 0.6):
        self.capacity = capacity
        self.alpha = alpha  # ä¼˜å…ˆçº§æŒ‡æ•°
        self.buffer = deque(maxlen=capacity)
        self.priorities = deque(maxlen=capacity)
        self.beta = 0.4  # é‡è¦æ€§é‡‡æ ·æŒ‡æ•°
        self.beta_increment = 0.001
        
    def push(self, experience: Experience, error: float = None):
        """æ·»åŠ ç»éªŒ"""
        if error is None:
            error = max([p for p in self.priorities] + [1.0])
        
        self.buffer.append(experience)
        self.priorities.append((abs(error) + 1e-5) ** self.alpha)
    
    def sample(self, batch_size: int) -> Tuple[List[Experience], List[int], List[float]]:
        """é‡‡æ ·ç»éªŒæ‰¹æ¬¡"""
        if len(self.buffer) < batch_size:
            batch_size = len(self.buffer)
        
        # è®¡ç®—é‡‡æ ·æ¦‚ç‡
        priorities = np.array(self.priorities)
        probabilities = priorities / priorities.sum()
        
        # é‡‡æ ·ç´¢å¼•
        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)
        
        # è®¡ç®—é‡è¦æ€§æƒé‡
        weights = (len(self.buffer) * probabilities[indices]) ** (-self.beta)
        weights = weights / weights.max()
        
        # è·å–ç»éªŒ
        experiences = [self.buffer[i] for i in indices]
        
        # æ›´æ–°beta
        self.beta = min(1.0, self.beta + self.beta_increment)
        
        return experiences, indices, weights
    
    def update_priorities(self, indices: List[int], errors: List[float]):
        """æ›´æ–°ä¼˜å…ˆçº§"""
        for idx, error in zip(indices, errors):
            if idx < len(self.priorities):
                self.priorities[idx] = (abs(error) + 1e-5) ** self.alpha
    
    def __len__(self):
        return len(self.buffer)

class DQNNetwork:
    """æ·±åº¦Qç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [128, 64]):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dims = hidden_dims
        
        # ç®€åŒ–çš„ç¥ç»ç½‘ç»œå‚æ•°ï¼ˆçº¿æ€§è¿‘ä¼¼ï¼‰
        self.weights = {}
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        # è¾“å…¥å±‚åˆ°ç¬¬ä¸€ä¸ªéšè—å±‚
        self.weights['W1'] = np.random.normal(0, 0.1, (self.state_dim, self.hidden_dims[0]))
        self.weights['b1'] = np.zeros(self.hidden_dims[0])
        
        # éšè—å±‚
        for i in range(len(self.hidden_dims) - 1):
            self.weights[f'W{i+2}'] = np.random.normal(0, 0.1, 
                                                      (self.hidden_dims[i], self.hidden_dims[i+1]))
            self.weights[f'b{i+2}'] = np.zeros(self.hidden_dims[i+1])
        
        # è¾“å‡ºå±‚
        last_hidden = self.hidden_dims[-1]
        self.weights['W_out'] = np.random.normal(0, 0.1, (last_hidden, self.action_dim))
        self.weights['b_out'] = np.zeros(self.action_dim)
    
    def forward(self, state: np.ndarray) -> np.ndarray:
        """å‰å‘ä¼ æ’­"""
        x = state.flatten() if len(state.shape) > 1 else state
        
        # ç¬¬ä¸€å±‚
        x = np.dot(x, self.weights['W1']) + self.weights['b1']
        x = np.maximum(0, x)  # ReLUæ¿€æ´»
        
        # éšè—å±‚
        for i in range(len(self.hidden_dims) - 1):
            x = np.dot(x, self.weights[f'W{i+2}']) + self.weights[f'b{i+2}']
            x = np.maximum(0, x)  # ReLUæ¿€æ´»
        
        # è¾“å‡ºå±‚
        q_values = np.dot(x, self.weights['W_out']) + self.weights['b_out']
        
        return q_values
    
    def update_weights(self, gradients: Dict, learning_rate: float = 0.001):
        """æ›´æ–°æƒé‡"""
        for key, grad in gradients.items():
            if key in self.weights:
                self.weights[key] -= learning_rate * grad
    
    def copy_weights_from(self, other_network):
        """ä»å¦ä¸€ä¸ªç½‘ç»œå¤åˆ¶æƒé‡"""
        self.weights = copy.deepcopy(other_network.weights)

class RLCoordinator:
    """å¼ºåŒ–å­¦ä¹ åè°ƒå™¨ - å®Œæ•´å®ç°"""
    
    def __init__(self, problem: MO_DHFSP_Problem, 
                 state_dim: int = 14,
                 action_dim: int = 7,
                 learning_rate: float = 0.001,
                 epsilon: float = 0.9,
                 epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01,
                 gamma: float = 0.98,
                 batch_size: int = 32,
                 target_update_freq: int = 100,
                 memory_size: int = 10000):
        """
        åˆå§‹åŒ–å¼ºåŒ–å­¦ä¹ åè°ƒå™¨
        
        Args:
            problem: é—®é¢˜å®ä¾‹
            state_dim: çŠ¶æ€ç»´åº¦
            action_dim: åŠ¨ä½œç»´åº¦
            learning_rate: å­¦ä¹ ç‡
            epsilon: æ¢ç´¢ç‡
            epsilon_decay: æ¢ç´¢è¡°å‡ç‡
            epsilon_min: æœ€å°æ¢ç´¢ç‡
            gamma: æŠ˜æ‰£å› å­
            batch_size: æ‰¹æ¬¡å¤§å°
            target_update_freq: ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
            memory_size: ç»éªŒç¼“å†²åŒºå¤§å°
        """
        self.problem = problem
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.gamma = gamma
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq
        
        # æ·±åº¦Qç½‘ç»œ
        self.q_network = DQNNetwork(state_dim, action_dim)
        self.target_network = DQNNetwork(state_dim, action_dim)
        self.target_network.copy_weights_from(self.q_network)
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.memory = PrioritizedReplayBuffer(memory_size)
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_step = 0
        self.episode_rewards = deque(maxlen=100)
        self.episode_lengths = deque(maxlen=100)
        self.loss_history = deque(maxlen=1000)
        
        # ç­–ç•¥ç»Ÿè®¡
        self.action_counts = np.zeros(action_dim)
        self.action_rewards = np.zeros(action_dim)
        self.action_success_rates = np.zeros(action_dim)
        
        # çŠ¶æ€-åŠ¨ä½œå€¼å†å²
        self.q_value_history = deque(maxlen=500)
        
        # åŠ¨ä½œç©ºé—´å®šä¹‰
        self.action_space = {
            0: "å¼ºåŒ–å…¨å±€æ¢ç´¢",
            1: "å¼ºåŒ–å±€éƒ¨å¼€å‘", 
            2: "å¹³è¡¡æœç´¢",
            3: "å¤šæ ·æ€§æ•‘æ´",
            4: "ç²¾è‹±å¼ºåŒ–",
            5: "å…¨å±€é‡å¯",
            6: "èµ„æºé‡åˆ†é…"
        }
        
        print(f"åˆå§‹åŒ–å¼ºåŒ–å­¦ä¹ åè°ƒå™¨:")
        print(f"  çŠ¶æ€ç»´åº¦: {state_dim}")
        print(f"  åŠ¨ä½œç©ºé—´: {action_dim}ç§ç­–ç•¥")
        print(f"  å­¦ä¹ ç‡: {learning_rate}")
        print(f"  åˆå§‹æ¢ç´¢ç‡: {epsilon}")
        print(f"  ç»éªŒç¼“å†²åŒº: {memory_size}")
    
    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """
        é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            training: æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼
            
        Returns:
            é€‰æ‹©çš„åŠ¨ä½œ
        """
        # ç¡®ä¿çŠ¶æ€ç»´åº¦æ­£ç¡®
        if len(state) != self.state_dim:
            # æˆªæ–­æˆ–å¡«å……çŠ¶æ€å‘é‡
            if len(state) > self.state_dim:
                state = state[:self.state_dim]
            else:
                state = np.pad(state, (0, self.state_dim - len(state)), 'constant')
        
        # epsilon-è´ªå©ªç­–ç•¥
        if training and random.random() < self.epsilon:
            action = random.randint(0, self.action_dim - 1)
        else:
            # ä½¿ç”¨Qç½‘ç»œé€‰æ‹©åŠ¨ä½œ
            q_values = self.q_network.forward(state)
            action = np.argmax(q_values)
            
            # è®°å½•Qå€¼å†å²
            self.q_value_history.append(q_values.copy())
        
        # æ›´æ–°åŠ¨ä½œç»Ÿè®¡
        self.action_counts[action] += 1
        
        return int(action)
    
    def store_experience(self, state: np.ndarray, action: int, reward: float, 
                        next_state: np.ndarray, done: bool = False):
        """å­˜å‚¨ç»éªŒ"""
        # ç¡®ä¿çŠ¶æ€ç»´åº¦ä¸€è‡´
        if len(state) != self.state_dim:
            if len(state) > self.state_dim:
                state = state[:self.state_dim]
            else:
                state = np.pad(state, (0, self.state_dim - len(state)), 'constant')
        
        if len(next_state) != self.state_dim:
            if len(next_state) > self.state_dim:
                next_state = next_state[:self.state_dim]
            else:
                next_state = np.pad(next_state, (0, self.state_dim - len(next_state)), 'constant')
        
        experience = Experience(state, action, reward, next_state, done)
        
        # è®¡ç®—TDè¯¯å·®ä½œä¸ºä¼˜å…ˆçº§
        current_q = self.q_network.forward(state)[action]
        if done:
            target_q = reward
        else:
            next_q_values = self.target_network.forward(next_state)
            target_q = reward + self.gamma * np.max(next_q_values)
        
        td_error = abs(target_q - current_q)
        
        self.memory.push(experience, td_error)
    
    def update(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray):
        """æ›´æ–°Qç½‘ç»œ"""
        # å­˜å‚¨ç»éªŒ
        self.store_experience(state, action, reward, next_state)
        
        # æ›´æ–°åŠ¨ä½œå¥–åŠ±ç»Ÿè®¡
        self.action_rewards[action] += reward
        if reward > 0:
            self.action_success_rates[action] += 1
        
        # è®­ç»ƒç½‘ç»œ
        if len(self.memory) >= self.batch_size:
            self._train_network()
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if self.training_step % self.target_update_freq == 0:
            self.target_network.copy_weights_from(self.q_network)
            print(f"ğŸ¯ æ›´æ–°ç›®æ ‡ç½‘ç»œ (æ­¥éª¤: {self.training_step})")
        
        # è¡°å‡æ¢ç´¢ç‡
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        self.training_step += 1
    
    def _train_network(self):
        """è®­ç»ƒç¥ç»ç½‘ç»œ"""
        # é‡‡æ ·ç»éªŒæ‰¹æ¬¡
        experiences, indices, weights = self.memory.sample(self.batch_size)
        
        states = np.array([exp.state for exp in experiences])
        actions = np.array([exp.action for exp in experiences])
        rewards = np.array([exp.reward for exp in experiences])
        next_states = np.array([exp.next_state for exp in experiences])
        dones = np.array([exp.done for exp in experiences])
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        current_q_values = np.array([self.q_network.forward(state) for state in states])
        next_q_values = np.array([self.target_network.forward(state) for state in next_states])
        
        target_q_values = current_q_values.copy()
        
        for i in range(len(experiences)):
            if dones[i]:
                target_q_values[i][actions[i]] = rewards[i]
            else:
                target_q_values[i][actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])
        
        # è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
        td_errors = []
        total_loss = 0
        
        for i in range(len(experiences)):
            state = states[i]
            action = actions[i]
            target = target_q_values[i][action]
            current = current_q_values[i][action]
            
            td_error = target - current
            td_errors.append(abs(td_error))
            total_loss += weights[i] * (td_error ** 2)
            
            # ç®€åŒ–çš„æ¢¯åº¦è®¡ç®—å’Œæ›´æ–°
            self._simple_gradient_update(state, action, td_error, weights[i])
        
        # æ›´æ–°ä¼˜å…ˆçº§
        self.memory.update_priorities(indices, td_errors)
        
        # è®°å½•æŸå¤±
        self.loss_history.append(total_loss / len(experiences))
    
    def _simple_gradient_update(self, state: np.ndarray, action: int, td_error: float, weight: float):
        """ç®€åŒ–çš„æ¢¯åº¦æ›´æ–°"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ›´æ–°æ–¹æ³•ï¼Œå®é™…åº”è¯¥ä½¿ç”¨åå‘ä¼ æ’­
        q_values = self.q_network.forward(state)
        
        # ç›´æ¥è°ƒæ•´å¯¹åº”åŠ¨ä½œçš„Qå€¼
        adjustment = self.learning_rate * weight * td_error
        
        # æ›´æ–°è¾“å‡ºå±‚æƒé‡ï¼ˆç®€åŒ–ï¼‰
        state_features = state.flatten() if len(state.shape) > 1 else state
        
        # ç®€å•çš„æƒé‡è°ƒæ•´
        if hasattr(self.q_network, 'weights'):
            for key in self.q_network.weights:
                if 'W_out' in key:
                    self.q_network.weights[key][action] += adjustment * 0.01 * np.sign(state_features).mean()
    
    def get_strategy_statistics(self) -> Dict:
        """è·å–ç­–ç•¥ç»Ÿè®¡ä¿¡æ¯"""
        total_actions = np.sum(self.action_counts)
        if total_actions == 0:
            return {}
        
        stats = {}
        for action_id in range(self.action_dim):
            action_name = self.action_space[action_id]
            count = self.action_counts[action_id]
            
            stats[action_name] = {
                'usage_count': int(count),
                'usage_rate': float(count / total_actions),
                'average_reward': float(self.action_rewards[action_id] / max(count, 1)),
                'success_rate': float(self.action_success_rates[action_id] / max(count, 1))
            }
        
        return stats
    
    def get_learning_progress(self) -> Dict:
        """è·å–å­¦ä¹ è¿›åº¦"""
        return {
            'training_steps': self.training_step,
            'epsilon': self.epsilon,
            'memory_size': len(self.memory),
            'average_loss': float(np.mean(self.loss_history)) if self.loss_history else 0.0,
            'average_q_value': float(np.mean([np.mean(q) for q in self.q_value_history])) if self.q_value_history else 0.0
        }
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'q_network_weights': self.q_network.weights,
            'target_network_weights': self.target_network.weights,
            'training_step': self.training_step,
            'epsilon': self.epsilon,
            'action_counts': self.action_counts,
            'action_rewards': self.action_rewards,
            'action_success_rates': self.action_success_rates
        }
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        if not os.path.exists(filepath):
            print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return False
        
        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            self.q_network.weights = model_data['q_network_weights']
            self.target_network.weights = model_data['target_network_weights']
            self.training_step = model_data['training_step']
            self.epsilon = model_data['epsilon']
            self.action_counts = model_data['action_counts']
            self.action_rewards = model_data['action_rewards']
            self.action_success_rates = model_data['action_success_rates']
            
            print(f"ğŸ“– æ¨¡å‹å·²ä» {filepath} åŠ è½½")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def get_action_recommendations(self, state: np.ndarray) -> List[Tuple[str, float]]:
        """è·å–åŠ¨ä½œæ¨èåŠå…¶ç½®ä¿¡åº¦"""
        if len(state) != self.state_dim:
            if len(state) > self.state_dim:
                state = state[:self.state_dim]
            else:
                state = np.pad(state, (0, self.state_dim - len(state)), 'constant')
        
        q_values = self.q_network.forward(state)
        
        # è®¡ç®—softmaxæ¦‚ç‡ä½œä¸ºç½®ä¿¡åº¦
        exp_q = np.exp(q_values - np.max(q_values))
        probabilities = exp_q / np.sum(exp_q)
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        sorted_indices = np.argsort(probabilities)[::-1]
        
        recommendations = []
        for idx in sorted_indices:
            action_name = self.action_space[idx]
            confidence = float(probabilities[idx])
            recommendations.append((action_name, confidence))
        
        return recommendations 